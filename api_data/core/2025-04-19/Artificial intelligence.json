[
    {
        "content": "The University of Maine DigitalCommons@UMaine General University of Maine Publications University of Maine Publications 10-15-2020 Applications of AI in Business, Industry, Government, Healthcare, and Environment University of Maine Artificial Intelligence Initiative Follow this and additional works at: https://digitalcommons.library.umaine.edu/univ_publications  Part of the Artificial Intelligence and Robotics Commons, Higher Education Commons, and the History Commons This Webpage is brought to you for free and open access by DigitalCommons@UMaine. It has been accepted for inclusion in General University of Maine Publications by an authorized administrator of DigitalCommons@UMaine. For more information, please contact um.library.technical.services@maine.edu.   UMaine Artificial Intelligence: Applications of AI in Business, Industry, Government, Healthcare, and Environment  Date: October 15, 2020 Run Time: 00:59:41 https://youtu.be/5889aoX9zDM  This webinar marks the first in the Fall 2021 series.  UMaine AI draws top talent and leverages a distinctive set of capabilities from the University of Maine and other collaborating institutions from across Maine and beyond, while it also recruits world-class talent from across the nation and the world. It is centered at the University of Maine, leveraging the university\u2019s strengths across disciplines, including computing and information sciences, engineering, health and life sciences, business, education, social sciences, and more.  Transcript is machine generated, unedited, in English.  00:01 Okay good morning good afternoon and good night  everyone. You're joining us from all over the   00:08 world today. We have the second series of our  UMaine Artificial Intelligence webinar series   00:16 with the topic of applications of AI in  business industry, government health care   00:20 and the environment. This webinar series has  been sponsored by IEEE Maine Communication   00:28 Society and Computer Society Joint Chapter, as  well as IEEE Region one in the northeast U.S and   00:35 promoted by IEEE USA. So we have three speakers  today. Each speaker will make the presentation and   00:42 then after all three are over, we'll have around  10-15 minutes for answering your questions. So   00:51 during the presentations, you can use the Q and A  feature to post your questions and you can do that   00:57 through the entire presentation and toward the  end we'll go through that Q & A to answer your   01:02 questions. So without further ado, I'm going to  introduce our first speaker, Dr. Mohamad Musavi,   01:09 Associate Dean of the College of Engineering and  Professor of Electrical and Computer Engineering   01:14 at the University of Maine. Prior to his current  position, he was the Chair of the ECE department.   01:21 Dr. Musavi received his MS and PhD degrees in  electrical engineering from the University of   01:26 Michigan. He has over 35 years of experience in  engineering education and research in the areas of   01:33 smart grid power systems and intelligent systems.  including neural networks, robotics and computer   01:40 vision. Dr. Musavi is the recipient of IEEE  USA engineer and educator partnership award   01:46 and I would like to invite him to the  podium to start us off with his talk. 01:53 Good afternoon everybody and thank you Ali for  the introduction. I'm going to start sharing my   02:02 screen. Okay, I hope everybody can see my screen  and again I want to thank everybody and also Ali   02:12 for the introduction and the opportunity to share  my experiences in artificial neural network,   02:19 an area that I have been involved in for the past  30 years. I have enjoyed working in this area   02:30 because of its connection to human  intelligence and its applicability to any area   02:37 where you need to learn from data. Over the past  30 years this area has grown tremendously and we   02:45 can see its applications in many aspects of  our lives and there are many more to come. 02:55 What are neural networks and why are they  important? The idea is to create a computer system   03:04 to mimic human intelligence. Human brain is a  massive network of 80 to 100 billion neurons that   03:14 are highly interconnected and act at the level of  sophistication yet unknown to human. On the other   03:22 hand, the latest computer processors are made of  8 billion processing nodes, but can work at a much   03:32 faster speed in the order of several trillion  operations per second. The question is how can   03:40 we learn from our biological neuron systems  to create artificial intelligence in machines. 03:50 So let's consider a biological neuron as  shown on the left. The main parts of this cell   03:59 as relate as relates to our work are the dendrites  on the bottom left inputting signals to the cell's   04:09 nucleus that processes the signals or information.  The axon that transmits the information from   04:18 one cell to the others and the synaptic  connections that identify how much information   04:25 is passed from one cell to another. One very  simple artificial presentation of such a neuron   04:34 is given on the right identifying the four  important functions of a biological neuron.   04:43 The input information via dendrites  are shown by X, vector X. The synaptic   04:50 strength or weights are shown by W. The nucleus  processing action is shown by the function   05:03 phi of sigma and the axon that carries the  information from one cell to the others. 05:12 Here is an artificial neural network made   05:15 from the interconnection of many nodes  the simple nodes I described in the   05:20 previous slide. This is the popular feedfort  network which has been around since the 40s   05:29 more than a decade before the invention of digital  computers and rule-based artificial intelligence.   05:37 However, the work was almost forgotten due to  lack of a learning process. It was in the mid   05:46 80s that the supervised back propagation learning  algorithm was developed to train such a network   05:55 from the functionality point of view, a neural  network can be viewed as a black box to use data   06:04 as its input and provide knowledge as output as  its output. If you are the manager of an operation   06:15 you may be interested in knowing what  you can learn from your available data,   06:21 but if you are the one who are going to  do the task, it is important that you   06:26 learn the topic before implementing any  network. I'm pleased to inform you that   06:33 we do offer an artificial artificial neural  network course here at the University of Maine.   06:41 So, we talked about being able to create knowledge  from a neural network. Human intelligence   06:48 comes in the form of recognizing things that we  have seen in the past, for example, recognizing   06:57 a person or being able to group things like  being able to distinguish a car from a truck,   07:06 or an airplane from a submarine. We are also able  to interpolate data and predict the future. For   07:15 example, if we see sufficient but incomplete  data from an object, we may be able to state   07:22 what that object is. Or if we observe the  path of a hurricane or know of the other and   07:32 know of the other factors affecting it, we can  predict where it's going to go next. So I have   07:40 categorized the knowledge provided by a neural  network into four categories -classification,   07:48 clustering, function approximation and prediction.  For every one of these categories I have prepared 07:57 one example that I will present next. 08:05 Before giving the examples let's go over  different steps in developing reliable   08:12 neural networks. First, we need to design the  network architecture or the network architecture,   08:20 which involves defining the number of inputs,  outputs, nodes and their interconnections.   08:30 Next is to train the network with a section of  the data available to us, followed by verifying   08:37 that the network is capable of doing its  job using the other section of the data,   08:45 and finally testing the network with  unseen data that has not been used   08:55 in the training or verification processes. 09:00 I should say training and  verification processes, not all.   09:06 The first example I present here  is a supervised classification   09:13 to identify chromosomes in a cell. This work  was done in collaboration with the Jackson Lab,   09:21 and to the left you will see mouse  chromosomes in a cell under the microscope.   09:28 The task is to identify all chromosomes and  arrange them in an organized karyotype on the left   09:39 for scientists to review.  The input data to the network   09:44 can be an image or features of a chromosome.  We completed this job with 88 percent accuracy   09:55 at a time where there was not any software to do  the job, and if you google my last name Musavi   10:05 m-u-s-a-v-i and mouse chromosomes, you will  find the scientific paper describing this work. 10:15 This example is also in the medical field and  the task is prediction and identification of DNA   10:26 bases which are A, C, G and T. Given the DNA  electrophoresis gel image as shown on the left   10:40 through the use of neural network, we not only  predicted where the next base should occur,   10:48 but also predicted what it should be with over  98% accuracy. To the right you will see the   10:57 output of the software that we developed for  this automated process we call the trace tool.   11:05 The green bars show the level of confidence  in the identification of the bases. It is very   11:12 important, the confidence level is very important. 11:17 So again if you google up my  name and DNA base calling,   11:26 you will see this work described in a  paper. Further description of the job. So   11:33 this application presents an  unsupervised learning. We were given   11:41 digital terrain elevation data or insured DTED  from a region and asked to identify the watershed   11:54 with the watershed areas of the data. We  designed the neural network and what you see 12:06 on the right is the output of the network showing  the watershed areas in the image. In addition to   12:18 the obvious lakes and brooks the network also  identified the possible flood areas after high   12:28 rain or melting the snow especially during the  spring, earlier spring time. The aerial image of   12:36 this same area is also shown and given here for  verification. The network did a pretty good job.   12:50 The pictures under the black box are different  stages of neural network learning process. 13:00 Here is the background for a continuous process  industry where raw materials are entered from one   13:08 end, the left, and the final product comes  out from the other end, on the bottom right.   13:18 We have historically had many pulp and paper  plants in Maine, but you can find similar   13:26 continuous processes in oil industries, flooring,  ceiling tiles and many others, where the quality   13:37 of raw material is constantly changing due to  the type and location of the raw materials used.   13:47 In a pulp and paper industry, waw materials  could come from a variety of trees and a number   13:53 of localities, but the final paper product  should always have consistent properties.   14:03 While we worked on digester refining and bleaching  as I showed to you in the previous slide.   14:13 Here I'm only using two very  important quality measures   14:18 at the end of this continuous process when the  final product which is paper here comes out.   14:27 These measures are brightness and opacity.  The top picture shows real brightness data   14:37 in red. The red again is real data that is  coming out of the plant and the blue information 14:48 overimposed on the red is the  prediction of the neural network 14:55 as what the brightness should  be one hour in advance. 15:03 Similar results have been presented for  opacity below. Why is it important to predict?   15:10 Because the operators can make corrective  actions to ensure that the paper quality   15:17 is satisfactory at all times and  there is no waste in the process,   15:22 exactly in the same way that is important to  predict the path of a tornado or hurricane. 15:33 So, in conclusion and in the last 30 years  we have worked on many other applications,   15:41 in the energy sector, smart grid utilities  and NASA satellite imaging imagery. In fact,   15:51 as I mentioned before if you Google my  last name and neural networks in general   15:58 you would see, I would say, about 60 to 70  scientific papers related to our works and   16:05 whichever is of interest to you, you can study.  Below are some other examples and that is why   16:13 these days you will notice a significant surge in  AI applications. I would end my presentation by   16:21 stating that wherever there is data neural  networks can provide insight into the process   16:31 or event. Here at the University of Maine we do  have expertise to help our state organizations,   16:39 industries, nonprofits and others to gain  knowledge from their data. So with that   16:49 I am ending my presentation. Thank you for the  time to listening to me and my contact information   16:57 is given as Musavi@maine.edu. Thank you again  Ali. Thank you very much Dr. Musavi for the great   17:06 presentation. So as I mentioned the beginning of  this talk if you're joining us a little bit late,   17:14 if you missed the first talk, we're gonna post  the recording later on the UMaine AI website   17:20 and this series is sponsored by IEEE, as well as  University of Maine. So let's go on to the next   17:27 talk as our next speaker is sharing his talks.  It's my great pleasure to introduce Owen McCarthy,   17:34 President and Co-Founder of Med Rhythms, a  digital therapeutic company that uses sensors,   17:40 music and software to build evidence-based  neurologic intervention to measure   17:47 and improve walking. Owen has become a leader  in the field of digital therapeutics, a member   17:53 of digital therapeutic alliance is developing  evidence-based FDA regulated devices to treat,   18:00 manage and diagnose neurological injuries and  diseases. Since founding Med Rhythm in 2016 Owen   18:07 has been invited to speak at digital therapeutics  conferences and events across the country   18:13 including DTX west, DTX east, neurotech  investing and partnering conference, partners   18:19 connected health conference,  HLTH evercore ISI panel,   18:23 so without further ado I would like to invite  Owen to the podium to start his time. All right,   18:31 really thrilled to be here and glad all of you  could join us from all over the world. You know,   18:37 As Ali said I'm a proud UMaine alum and  really glad that there's a lot of activity going   18:43 on in cutting edge innovation, and always thrilled  to hear from, you know, people like Dr. Musavi on   18:50 their work and and things that are going on in  our our university. So today my plan is to talk   18:56 a little bit about how AI is being used in sort  of regulated healthcare. You know there's a lot   19:02 of applications that I'm not going to talk  about where you know, AI is being used in   19:08 sort of drug development or defining care  pathways or you know even at you know better   19:14 understanding DNA and in our genetic makeup,  and so those will not be part of my talk today,   19:19 but what I'm really going to be talking about is  what I'm seeing you know both through the digital   19:24 therapeutics alliance at med rhythms and you know  with my colleagues in the field for things that   19:31 directly treat, manage and diagnose disease, that  require some sort of FDA approval in oversight.   19:39 You know because those are you know this is some  super fascinating trends going on there and so   19:44 as you know I mean you see headlines all the  time that some of them might scare you about   19:48 how the machines might be taking jobs or  these things are coming. And will   19:53 physicians be obsolete and rest assured  I don't believe that physicians   19:58 will be obsolete and these things, the  things that are coming will take all the jobs   20:04 but what's really  happening and what we're really seeing   20:08 is a couple of different things in  the field, and we're seeing physicians and doctors   20:14 and clinicians and other people in healthcare  becoming more effective because of assistive AI   20:20 tools which I'll talk about some of them and then  there's some autonomous AI that is coming out   20:26 that makes things more efficient. Though those  right now have larger challenges both in terms of   20:32 approval pathways from the FDA and in terms of  how it changes how products are are paid for and   20:40 reimbursed. And so here you'll find a chart that  Nature just published actually, and I think it was   20:46 maybe last month for all of the FDA approvals for  you know intelligence-based devices in medicine.   20:54 You'll notice actually that the majority of  them actually fall in that radiology bucket   21:01 because as we know you know the systems that Dr.  Musavi was talking about have been proven actually   21:09 pretty well at understanding imaging and doing  image recognition and in radiology there's a   21:14 lot of images to understand. And so that's where  you're seeing the majority of the the work these   21:20 days. But to break it down even further  there has been 29 products approved   21:26 by the FDA that includes AI and machine learning.  Eight of these do use a deep learning approach. 21   21:32 of them are in radiology and these products  are sort of across the board where 21:43 the AI is actually being adapted in real time um  you know to change the outputs of the algorithms   21:51 or there's a stepwise approach where the  live version of the algorithm that might detect   21:57 what's happening on the image and the radiology  is locked, but in the background, there's an   22:03 ML AI system that's making it better. They'll do a  set of quality checks and then they'll re-release   22:10 another locked version of an algorithm and sort  of a process and to date each of those sort of um   22:17 setups have been negotiated sort of on a one-off  basis with the FDA. I'm going to spend maybe a   22:23 little bit of time trying to make it more concrete  and talk about sort of use cases in dermatology   22:31 and this information and a lot of this comes from  you know one of my colleagues who's at digital   22:37 diagnostics who's educated me a lot about in  dermatology how AI can be used, and it's not just   22:45 take a picture of your skin and diagnose if you  have cancer or not. So if you think   22:49 about teledermatology and if you have some sort  of something that's happening on your skin and   22:56 you know, I actually do have something I have to  get checked out, which hopefully it's nothing bad   23:00 but if you have something on your skin, the  flow in health care is often you go to your   23:05 primary care doc who will check it out, give it a  first pass then you get put into a maybe a remote   23:12 dermatologist to take a look and covet times or  you go immediately to an in-person dermatologist.   23:18 Some of the challenges might be that to schedule a visit in some parts of the country   23:25 with a dermatologist, it might take six to twelve  months but some conditions that you detect may be   23:30 more lethal in that time frame and so a first sort  of pass for AI based systems is they're starting   23:39 to be rolled out and three durham this company  I was talking about does this in primary care   23:44 where they'll take an image when you come into the  office and then quality check if the image is good   23:50 of quality in the beginning before it gets  sent to a remote durham to triage to see   23:56 what makes the most sense. The  next level of that is if you add the the AI in   24:04 after that quality checkpoint at the primary care  office it can sort of be like an urgent expediter   24:10 where it says this is melanoma or this is a basal  cell please see a dermatologist immediately, or   24:18 it might you know, it's as Dr. Musavi said,  it's probably it's 80% likely this or 70%   24:24 likely this there's some sort of  threshold though there that it has to hit to   24:30 get over that point to jump the line and then a  real dermatologist can make the call. And you'll   24:35 see this a lot and it continues to be more of  a trend for how these sort of things roll out,   24:41 but the next case is and this is what  they're working on at digital diagnostics is   24:47 you know, the auto triage where the FDA would  approve an algorithm that says you know this can   24:56 diagnose this condition, this condition and this  condition, and if that image is taken whether it   25:03 be at primary care or in home, though there's some  challenges more challenges there, that it can jump   25:10 the line with the system that takes you know a  learning approach to drive someone directly to   25:17 an in-person dermatologist. So those are those  are some of the types of things that that you   25:23 know we're seeing you know there's there's also  applications where there's this company called IDX   25:29 that has a you know eye image software that  can automatically detect diabetes retinopathy   25:37 that really you know kind of it follows this same  path where it can take an image, and then triage   25:45 it instead and diagnose you with that and then  get you the right care at the right time. In   25:51 all of these cases you know there's still a human  in the loop, you know at some point down the line   25:57 they might be remote instead of in person, but but  there's still a human and a lot of loops because   26:03 there's it's more complicated than that like  one niche area that's being diagnosed and you   26:10 know that will likely sort of flex over time,  but it's it's you know it's important because   26:16 if you think about I mean I don't know, you know  all you're familiar with the healthcare but if   26:21 you pick a profession in healthcare - nurses or  probably radiologists or physical therapists you   26:27 know there's always a shortage you know every area  particularly enrollment particularly where it is   26:32 there's a shortage of them. They're always  booked out a long time and so tools that can   26:39 either make them better make sort of individual  diagnosis that get that can better treat patients   26:44 or better diagnose patients will not only help you  know, they're not going to replace jobs. They're   26:51 going to really alleviate sort of the tension of  we need you know 40 percent or 25 percent more   26:58 physical therapists or other things like that.  So it's a really exciting time you know as as   27:03 these come and help you know these tools become  you know the tools that have been invented   27:09 since the 40s as Dr. Musavi said and then you  know took hold in the 80s and now computing   27:15 power allows us to do them better in real world  now are coming to fruition, you know they're   27:20 really changing the landscape, you know some of  the considerations though you know if you were   27:25 to look at from you know taking it to market or  if you're building technology in the market that   27:30 that still need to be sorted out are there are   27:36 five of them that I think need to be sorted out  you know across the spectrum. And there's   27:42 some instances where where real  world clinical data is messy because the   27:51 inputs, the differences between the image  that's taken makes all the difference whether or   27:58 not something is detected in something like a hair  or something else in the the image on the skin   28:04 could really change how some of the algorithms  really detect. So you know what 3 arm found was   28:10 you know their their goal would be to get the  webcam the camera here that for the   28:16 for the picture to be used for the for  the input. However for now they're they're doing   28:24 a more controlled approach in a primary care  office because they found that the quality   28:28 wasn't as good, maybe that maybe a generation or  two before you know some of the phones that came   28:35 out and because of the data coming in and so you  know that's a real constraint on any system is the   28:40 the the quality of the inputs particularly as you  put it in the real world. And then next   28:47 is there's actually a whole class of products  that are coming out and trying to go around   28:54 the FDA regulation, which some of it might  be good and some of it might be bad, but   28:58 what they're doing is they're taking  data, providing information, giving it to   29:05 a non-specialist and allowing them to try to make  a determination based on that. Some of them could   29:10 and some of it may be okay, but the  more you do that the more likely you   29:18 maybe lose control over some of the  regulated nature that may be necessary   29:23 if you're really dealing with life or death  or someone's health and so that's something   29:27 to keep an eye out particularly as you might  find. Bad actors or people that that that don't   29:35 you know understand the quality controls on  health care. The other one is, back to inputs is,   29:41 you know there is there's a real challenge across  even clinical trials, but if you think about you   29:48 know AI-driven dermatology that there's a lack of  diversity in terms of the pictures and things   29:54 that have been training these algorithms and so  you really would you wouldn't want to leave by you   30:03 know behind ethnic groups or other groups in terms  of you know how they're being treated and so it's   30:08 something to really put some thought behind both  on the you know if you're a policy person on the   30:13 policy angle as is and how to how to be inclusive  in the development of these technologies,   30:18 or if you're a technologist and you're developing software that uses these things,   30:25 that you really think about how inputs could  be different and how treatment could be different   30:30 based on someone's, you know background so that's  something that needs more sort of more   30:37 focus. And then finally I did mention you  know nine there's you know there's been 29   30:43 approvals through the FDA. Each of those have  sort of been negotiated separately in terms of   30:48 how you set up the frameworks for regulating the  algorithms. The FDA right now is coming out with a   30:57 with what they call a document, the framework  document that they publish as a draft last year   31:03 and it should finalize soon that talks about sort  of how as a manufacturer of software a builder of   31:09 software that uses AI in treating, diagnosing or  managing diseases, you can set up frameworks to be   31:17 able to allow it to adapt and change and not have  to meet with them you know over and over again to   31:23 individually come up with a plan for that, sort  of come up with a blueprint. So that's something   31:27 to watch and that will could drive innovation.  I will mention you know the FDA has actually   31:33 been pretty forward-thinking as it relates to  software as a medical device and AI-based systems   31:39 and just created a center for digital health  excellence, I think last week. And they're   31:44 really I mean they're really thoughtful in this  regard, but also you know take keep in mind the   31:52 you know that there's an efficacy and safety  risk and benefit for patients that needs to be   31:57 evaluated you know through these processes but  they're definitely forward thinking there. And   32:03 then finally I think the last frontier one of  the last frontiers to say will these type of   32:09 systems scale in a way in healthcare and really  transform is how things get paid for. You know   32:15 the if you think about I mean we're now maybe  outside of like tech and AI, but if you   32:21 think about a system you know the back to those  scare tactics and in the articles it says are the   32:27 systems coming for your job. Well no, is my answer  because there's plenty of flex there however   32:34 you know the way health care gets paid for is  really set up by in the US by insurance companies   32:39 public and private and they have a really rigid  rules around what can get reimbursed and by who,   32:45 and if a software system is doing  the full diagnosis of melanoma,   32:50 but the treatment might go to a dermatologist,  you need to create new ways for incentives for   32:55 like you getting paid for that versus getting paid  for a clinician's time for that, and if you do   33:00 that improperly you could put the the technology  up against the clinician and make them, make turn   33:09 it into a policy lobbying war instead of a what's  better for the patient. And so you know medicare,   33:15 Medicaid is thinking about this and so are private  insurers but it's going to be - I think you   33:20 get that figured outright, you're going to see  a lot more innovation in this space. So that's   33:26 you know, that's my you know my  quick sort of view of what I'm seeing   33:30 in the real world. Look for the Q and A period  and thankful for the opportunity to talk Ali.   33:37 Thank you very much Owen for the interesting  talk. Again those who are joining us late all of   33:43 these talks will be recorded and posted on the  UMaine AI website. Please don't forget to post   33:48 your questions on the Q & A feature and we'll  answer those questions right after the next talk.   33:55 So our third and last talk for today  is delivered by Dr. Somayeh Khosroazad.   34:02 She received her BS and MS degrees in Eelectrical  Engineering from Ferdowsi University of Mashhad   34:08 in 2006 and 2009, and her PhD degree from  University of Birjand in 2017. From 2018 to   34:16 2020 she was a signal processing specialist  at Activos diagnostics and a post-doctoral   34:23 research fellow at the department of electrical  and computer engineering at the University   34:27 of Maine. She's currently liberal visiting  diversity professor at the University of Maine.   34:33 Her research interests include theory and  development of wireless communication system,   34:37 wireless sensor networks, artificial intelligence  techniques, modeling, analyzing and optimizing   34:43 new wireless systems. So without further ado I  would like to invite Dr. Khosroazad to the podium.   34:50 Thanks very much, thank you for the introduction  and let me share my screen. Good afternoon   34:58 or good morning everyone that is  depending on which times you're in.   35:02 The subject that I want to talk about that is the  applications of AI in healthcare and environment. 35:12 As you know artificial intelligence or AI  refers to a computer imitating intellectual   35:18 processes characteristic of humans to learn  from past experiences and reach goals without   35:24 being explicitly programmed for a specific result.  Today, AI is one of the most attractive subjects   35:31 that all over the science world talk about  and try to solve problems with this new tool.   35:36 There is no doubt that ai is a strong tool to  escape some of the disabilities or weaknesses   35:43 of human beings to solving and solving  complex or very time consuming problems.   35:48 In many areas, however, much work needs to be  done to bring out AI from papers or simulations   35:56 to the real world. The most basic or perhaps  the simplest perception that the general public   36:03 has of artificial intelligence is face recognition  or a little more general case, image processing   36:10 or speed or voice recognition, meaning that  there are some algorithms in which the computer   36:16 is trained to classify data after receiving  some information and prototypes, and then for   36:22 any new data it tracks the pattern which has been  trained with it to detect the classification of   36:28 the new data. However this is this is definitely  not the most impressive attraction of AI.   36:38 What makes it the most attention getter is  that it is a pervasive in all science in all   36:45 sciences and a a wide range of applications. Here,  given the limited time that I have, I just want   36:53 to focus briefly on the application of artificial  intelligence in wireless communications, wireless   36:58 sensor networks and healthcare applications.  We all know that the world is moving towards   37:05 internet of things or IOT awards in which wireless  sensor networks continuously exchange information   37:12 among a large number of devices, and  all these devices work simultaneously.   37:18 And the goal is monitoring and controlling some  specific parameters in the systems. Obviously,   37:24 for such applications AI conceptually  could be a tremendously useful tool   37:30 for increasing the speed of processing data,  as well as decreasing costs and energy.   37:35 However, one important question is how much  today and in the near future AI is powerful   37:42 and trustable to be applied for more serious  application applications in the real world.   37:51 Let me categorize AI problems from  a specific view to two groups. Some   37:56 specific problems in which a huge amount of data  is collected and AI supervised or unsupervised   38:04 tries to classify the data to multiple groups  by finding one or more thresholds or some limits   38:10 based on all statistical and probabilistic logics  that are predefined for it. In such matters, 38:20 in such matters, AI directly force fit the  data. In this case, the distinctive feature   38:25 of artificial intelligence is its accuracy  and speed to search for all possibilities   38:31 and choose the best solution considering again,  some predefined general statistical criterias. 38:41 There is another type of AI applications however  in which some kind of pre-processing needs   38:47 to be done before using AI, means that the  classification is not done on data directly,   38:54 but it is done on some other features that  should be extracted from the data first,   39:00 and for extracting these features a sort of  knowledge and expertise beyond AI is required. 39:09 I would like to clarify this ladder case with an  example from our sleep monitoring project which   39:14 is being done at activist diagnostics company in  collaboration with two engineering and psychology   39:20 groups. In this project a sleep move device is  used which is a wireless device commercialized   39:27 by activist diagnostics. This device is capable  of collecting data from various types of   39:34 piezo-resistive sensors deployed in a network to  improve the dynamic range of sensor arrays and   39:41 satisfy the requirements of feature detection  and extraction from the collected neuromotor   39:48 signals. Hardware and software integrate movement  and respiration variability using a fast normal   39:55 motor about detection algorithm. Obviously,  there are some pre-processing and screening   40:02 design stages that should be done first, like  denoising and choosing the most informative data   40:08 among the data achieved from all sensors at  each moment. But after that the question is   40:15 what we have and what we are looking for. What we  have is body movement signals during the sleep,   40:21 and what we are looking for is early  detection of alzheimer's disease. 40:30 In such a situation, the body movement signals  depend on so many different factors, that even   40:38 physicians cannot make any prospects about the  result factors like age, weight, position of the   40:46 body during a sleep which is based on the habits  of different people and a lot more. In hospital   40:54 sleep studies utilizing many non-energy  signals to characterize a sleep disorder   40:59 like eye and body movements, respiratory  frequency or volume cardiac variability and so on,   41:06 show there is a strong relation uh relationship  between sleep disorders and neurological status,   41:13 and fragmented sleep results in the  impairment of cognitive function   41:18 or disrupt autonomic nervous  system regulatory processes   41:22 during their sleep periods. However, there is no  definitive algorithm to assist in diagnostics. 41:31 In such situations, 41:38 in such a situation where pure data have a  very high dispersion compared to the amount of   41:44 available data and there is no statistically  direct relationship between the data   41:50 and the desired output. This is necessary  to first extract some more informative and   41:57 relevant features from the pure data and  then use AI tools to classify these features.  42:06 What we did for this project is the whole  two nice data are separated into 10 minutes   42:14 epochs with epochs with one minute overlaps.  This window size is chosen based on the number of   42:22 sleep-related spontaneous movements with a  periodicity of three to five minutes yielding   42:29 two to three events in each 10 minutes epoch, then  considering the properties of spontaneous movement   42:36 signals and respiratory signals which was taken  from expertise of psychologists and physicians.   42:43 Some filters were designed to separately   42:47 extract respiratory and movement signals from the  raw data. The next step was extracting important   42:56 features from these two signals. Movements induce  physiological of regulation of cardiorespiratory   43:04 rate to maintain homeostasis a process that  may be indexed by the respiratory rate change.   43:11 The first analysis tracks the similarity of the  changes in these two signals in other words,   43:17 to see how movement and respiratory  signals affect each other the correlation. 43:25 The correlation between them   43:29 is calculated. In general correlation  describes the mutual relationship   43:33 which exists between two or more signals. The  cross-correlation of the two signals is maximum   43:40 at the time that equals to time lag. This time  lag is considered as an important feature. 43:50 Some other features then are like sleep duration  and fragmentation estimates or maximum amplitude   43:57 which were perceived to be related to brain  activities, were extracted and then these features   44:05 plus some other information like age and weight  were used as the inputs of the AI tools and now   44:14 these extracted information could be used as the  input of AI tools to be processed and classified,   44:22 and as you can see now by using this  processing algorithm AI could give us, 44:29 88 percent accuracy in early  detection of Alzheimer's disease. 44:40 In general what I want to highlight is AI is a  strong tool for deep learning, but as they say   44:49 it is not half it does not have necessarily a deep  sight for solving the problems. What do we need   44:56 for make it more applicable and trustable for the  real world is to learn how make right connections   45:03 among experienced expertise and AI tools. Thank  you for hearing me and I'm looking forward   45:12 to any question you have and let me thank you  Ali. Thank you very much Dr. Khosroazad for   45:20 the great presentation. I'm glad to see we had  a very sort of wide range of applications kind   45:27 of covered today so hopefully that inspires you  to ask some relevant questions. We started from   45:34 Dr. Musavi's talk which was basically the history  of UMaine working neural network and the basic and   45:40 fundamentals of AI kind of gives you an idea of  what people need to learn to be able to retool for   45:46 future jobs. And then we had a new application  that Owen covered for us and then the final   45:53 applications that Khosroazad was talking  about is basically moving on to the direction   45:58 of personalized health care and using AI for  different kind of diagnostics. So let's get into   46:07 the Q & A session again if you have questions,  don't post them in the chat window just post them   46:12 in the Q & A session. And I will start with the  first question here and all the panelists you can   46:20 maybe turn on your video so you can answer live.  I'm directing the first question to Owen McCarthy.   46:27 The question is from Jared. You talked about AI as  a job modifier without much effect on the number   46:33 of jobs in the healthcare field. Could you talk  about what kind of maintenance these AI systems   46:38 require and would they require specialized roles  within an organization to train and maintain?   46:45 Yeah okay thanks Ali and Jared. Thank  you for your question. You know I think,   46:51 I will say I don't have a crystal ball to  say how it's all going to play out so I love   46:54 everyone else's thoughts on this  as well, but you know I think   46:58 where the specialized roles will fall will be  likely with the manufacturers and I put that in   47:03 quotes because they're building software so you  know they're software development manufacturers.   47:09 You know there's going to be a heavier need on  you know their you know if you think about the   47:16 traditional quality engineering group that you'd  have at, in a software shop. There's going to be   47:21 someone that's on the teams that are really  crossing over from quality engineering for   47:27 the software but also on the regulatory side and  has the combined knowledge and so I can see that   47:32 as a specialized role in an organization. And  then what's it mean on the health care people   47:39 and their jobs, you know. That's it's going to be  an interesting one to see how it plays out because   47:44 you know, will it mean they'll get to  spend more time with their patients sort of   47:48 on you know and and start having to you know  right now it's a very like structured you have   47:54 10 minutes, you come in, you have 10 more minutes  you come in. Would they have more time to work on   48:00 lifestyle changes or other things at the clinical  level because some of these other things will be   48:04 taken away? Maybe. Will it allow for services like  Lavongo etc to offer full digital connected care   48:12 no matter where you are? That's also a trend so  in which then the jobs might turn into some more   48:18 remote jobs or more you know, clinicians that that  understand different data streams that can make   48:25 better decisions on them coming through. So Jared,  I don't know the exact answer to your question,   48:30 but there's some there's definitely some threads  that you know that will change. But I think   48:37 related to AI i think the the specialized roles  will rely within them will be exist within the   48:43 manufacturers likely of the the products. Thank  you Owen and while we have you here on video let   48:50 me ask you another question before we go to  the other panelists. So there is also another   48:55 question from Marie. What is your company's  experience with FDA and CMS? Yeah no thank you. So   49:03 we have had I think eight interactions and maybe  nine now with the FDA related to our our initial   49:11 product which is helping people improve their  walking following stroke. It's in our pivotal   49:16 trial stage and those interactions have been in  like what they call a pre-submission meeting,   49:21 so you meet with them to get their buy-in about  the approach that you're taking, and so we in and   49:26 to do that you have a host of consultants and  other folks that you work with and it's been   49:34 very collaborative, very positive. We're fortunate  enough, earlier this year to get breakthrough   49:39 designation for our stroke product which means  that the FDA believes it could better treat an   49:48 irreversibly debilitating disease more effectively  with a full unmet need there. So that was thought   49:55 that was helpful and then on the CMS side you know  we've you know the more you know about healthcare,   50:01 you realize the less you know, but we've been  diving in to you know the is there a code,   50:08 like how do you get paid, like how does  it fit into the legal requirements, and   50:13 we've got sort of we've done some analysis sees  with people that used to work with at CMS and   50:19 then also with internal teams for how the product  could roll out. We just, we've also been able to   50:25 comment on a rule that just got released that will  automatically reimburse for breakthrough status   50:31 designation products for approval for four years  from CMS which we believe that could be really   50:36 positive for our stroke product. And we've so on  the policy side we've been working with that so   50:41 it's a policy slash you know strategy angle  with CMS but we've had some interaction there   50:49 but as I might have mentioned you know from Marie  feel free to reach out, I'm happy to talk about   50:53 all these things offline as well. Thank you very much, Owen. So next question is for Somayeh.   51:01 So what is the impact of AI with next  generation communications networks.   51:09 Sure I was just typing the answer for  this question because I saw the question   51:14 and I can say it has definitely a great impact  on the next generation because most important   51:23 most important topic of the next generation is as  I told, internet of things. When sensor networks,   51:31 wireless sensor networks has a really important  role in this subject and when different   51:39 devices are working with each other simultaneously  and they want to extend their information   51:47 and we need to control and monitor different  parameters in such a big and large network.   51:56 AI can help a lot because it is an intelligence,   52:03 artificial intelligence and it can it  should be actually designed in somehow that   52:09 can be estimate some specific changes in this  network and give us some good estimations about   52:17 future and how these devices work with each other  without controlling by human beings directly   52:30 at the AI tools need to be are designed  for such a great and large network to   52:37 do the best for controlling  ammo entering such a system.   52:42 Very good thank you very much. So the next question  is for Dr. Musavi. It's a long question. So   52:50 basically it refers that you're teaching to the  neural network to their students. Recently we   52:56 have pretty solid solutions for deep neural  network and machine learning in general over   53:01 cloud, whereas machine learning looks like  a plug-and-play of some blocks. If the goal is   53:07 to prepare our students for job market, do we  still need to teach them theories and concept   53:14 or do we need to teach them how to work with these  clouds cloud-based applications? I mean do we need   53:20 to teach them how to use python libraries and so  on or we need to focus on how to use cloud-based   53:27 systems? Thank you Ali and thank you for the  question. Yeah that's a very important question.   53:34 It depends on what you are looking  at. Of course cloud computing   53:42 and available software is important to use but  we have gone through this many times in the past,   53:48 that people use available software online and  provide their data to the system and they don't   53:55 get the result that they want, and of course I  try to avoid using this terminology, but some   54:02 people call it garbage in garbage. If you do not  have the knowledge of how the system is working,   54:10 then you the result that you may  get you just looking at it from a   54:17 dark point of view, not having inside  information. Therefore, I suggest that again,   54:24 if you have an application and if you want to give  it to cloud computing or available software go for   54:30 it. If it gives you the result that you want  you're all set you don't need that knowledge.   54:35 However, if you are encountering difficulty  honestly, you need to go ahead and   54:42 have at least a knowledge of neural network.  So that is what my experience has been.   54:50 I think I don't want people to lose their trust in  neural network because not knowing how to handle   54:57 it and at the same time, I think you want to tune  to our next presentation. I believe is next week   55:06 or next month. Next month when we have one of our  faculty members talk about deep learning and big   55:14 data and that's so we offer both courses. We  offer courses in application audience that   55:22 and we also offer courses in what's under  the engine, what's under the hood. So   55:29 hopefully I was able to answer your question  and again as I said over my 30 years I have   55:34 gone through this process many times. I had  people who said I don't need this knowledge   55:39 and I can go ahead and use this but then after  a couple of, a month came back to me and said   55:45 this doesn't make sense I'm getting garbage  out. So it depends on what you are looking at   55:51 and what your application is. Thank you very  much Dr. Musavi for answering that. There is   55:56 also another question I'm going to direct this one  to Owen McCarthy. So how do we keep the algorithms   56:03 produced via public funding free for advancing  future AI developments in a world where increased   56:09 governmental regulation intellectual property  rights etc are locking the code. Additionally   56:16 how do we keep health care processes down as  if these algorithms are adding more costs? Yeah   56:23 thank you, so the second part of that question  you know I think comes down to this really   56:28 that's more of almost a policy angle because the  the problem you often see in healthcare is people just   56:35 add things and that you know because no one wants  to lose anything. It's additive. However you know   56:43 now in COVID has highlighted this. Now that we can  treat anyone wherever they are and like treat them   56:50 per in a personalized way and maybe even prevent,  you should keep long-term health care costs down.   56:57 And to do that like the technologies will enable  that to happen and then it comes down to you know   57:04 a policy angle for how then the health care  systems adopt that and you know that's you know   57:11 that's a bit outside my pay grade for that one  you know that's and gets political real fast on   57:18 how that can happen. But but the the technologies  under themselves will make efficiencies and stop   57:25 things from happening you know like you know  or you can treat people earlier before they get   57:32 things to happen and so it should keep health care  prices down but it's gonna require some some swift   57:38 policy work and on the intellectual property  side you know I think that's a place where   57:43 you know there's two things you know the the  there's actually if you're a software developer   57:50 you've unto yourself over time said we believe in  the open source movement and that's how we build   57:58 a lot of things and we care about employers  in places that can that continue to do that   58:05 so I think that attitude will force companies  from to at least keep some things from being   58:11 locked down. And then places like University of  Maine play a huge role in that because you are   58:17 you you're an institution and we're an institution  that does research for the public good and   58:23 you disseminate that information through papers  and other ways and so there's a huge role for   58:29 higher education to lead the charge in advancing  the future in AI development. Thank you very   58:36 much Owen. That's all the time we have for today  the rest of the questions will be posted online   58:41 again. I would like to thank all our panelists for  presenting the talks today. I also want to thank   58:48 the staff at The Office of Vice President  for Research who have been running this   58:52 in the background Mindy, Tilan and Liz, as  well as search from UMaine I.T to help us   58:59 make sure there is no technical issues. Again  thanks to sponsors and promoters of this event   59:04 IEEE from all over the world our colleagues in  California in Silicon Valley, in Boston section as   59:11 well as Bangalore in India and Shenzhen in China.  I would like to invite everybody to attend our   59:17 future AI webinars first Thursday of the month  at noon time eastern time. Our next event is   59:23 happening on November 5th and the last one  for the fall will be December 3rd with more   59:28 interesting talks to come. My name is Ali  Abedi I'm Assistant of Vice President for   59:32 Research at the University of Maine. I would  like to thank everybody for joining us today   59:37 and hope to see you in future events.  Thank you and have a great day everyone.           The University of Maine in Orono is the flagship campus of the University of Maine System, where efforts toward racial equity are ongoing, as is the commitment to facing a complicated and not always just institutional history. The University recognizes that it is located on Marsh Island in the homeland of the Penobscot nation, where issues of water and its territorial rights, and encroachment upon sacred sites, are ongoing. Penobscot homeland is connected to the other Wabanaki Tribal Nations \u2014 the Passamaquoddy, Maliseet, and Micmac \u2014 through kinship, alliances, and diplomacy. The university also recognizes that the Penobscot Nation and the other Wabanaki Tribal Nations are distinct, sovereign, legal and political entities with their own powers of self-governance and self-determination. ",
        "title": "Applications of AI in Business, Industry, Government, Healthcare, and Environment",
        "citationCount": 0
    },
    {
        "content": "The University of Maine DigitalCommons@UMaine General University of Maine Publications University of Maine Publications 9-23-2021 Artificial Intelligence in Cyber Security University of Maine Artificial Intelligence Initiative Follow this and additional works at: https://digitalcommons.library.umaine.edu/univ_publications  Part of the Artificial Intelligence and Robotics Commons, Higher Education Commons, and the History Commons This Video Recording is brought to you for free and open access by DigitalCommons@UMaine. It has been accepted for inclusion in General University of Maine Publications by an authorized administrator of DigitalCommons@UMaine. For more information, please contact um.library.technical.services@maine.edu.   UMaine Artificial Intelligence: Artificial Intelligence in Cyber Security  Date: September 23, 2021 Run Time: 00:59:39 https://youtu.be/TgUiJVjbZF8  This webinar marks the first in the Fall 2021 series.  UMaine AI draws top talent and leverages a distinctive set of capabilities from the University of Maine and other collaborating institutions from across Maine and beyond, while it also recruits world-class talent from across the nation and the world. It is centered at the University of Maine, leveraging the university\u2019s strengths across disciplines, including computing and information sciences, engineering, health and life sciences, business, education, social sciences, and more.  Transcript is machine generated, unedited, in English.  00:00 good morning good a fte rnoon and good 00:02 evening to a ll our advertis e rs  from 00:04 va rious  pa rts  of the world my name is  00:06 a li abedi s e rving a is hwarpa li a s  region 00:09 1 a s s is tant a rea  cha ir and a s s ocia te  00:11 vice  pres ident for res earch a t the 00:13 univers ity of ma ine before  we s ta rt our 00:15 webina r today i would like  to thank our 00:18 ieee  colleagues  in s ilicon va lley and 00:20 bos ton s ections  i truly regen one  uh 00:24 us a  ieee  india  and actua lly china  for 00:27 promoting this  event which yie lded over 00:30 321 regis tra tions  00:33 a  s pecia l thanks  to is rae li 00:34 communica tions  s ocie ty and computer 00:36 s ocie ty's  joint chapter here  in ma ine  a s  00:39 well a s  univers ity of maine  a rtific ia l 00:41 inte lligence  initia tive  for planning and 00:44 hos ting this  event 00:45 pleas e make  s ure  to ente r your ques tions  00:48 in the  qr may box and we 'll ans wer them 00:50 a t the  end of the pres enta tions  00:53 it is  now my grea t plea s ure  to introduce 00:56 our modera tor 00:57 dr julia  upton as s ocia te  profes s or of 01:00 ma thema tics  a t aus tin univers ity voice 01:03 cha ir of i triple  e  ma in s ection and 01:05 cha ir of i triple  e  ma in communica tions  01:07 and computer s ocie ty joint chapter to 01:10 introduce  our s peakers  julia  01:13 thank you a li 01:15 uh i would like  to welcome everyone  on 01:17 beha lf of ieee ma in s ection and the  join 01:21 computer s ocie ty communica tion s ocie ty 01:23 chapter thank you for joining us  01:26 uh we 01:28 will we  have  four s peakers  for you today 01:30 um and our firs t s peaker is  um 01:34 bill layer 01:36 uh rea r admira l re tired bill layer 01:39 s erved in the  united s ta tes  navy for 33 01:42 yea rs  in inte lligence  and cryptologica l 01:44 warfa re  01:45 his  ca reer s panned the  cold war des ert 01:48 s torm  and the  globa l war on te rroris m 01:51 his  m inion navy a s s ignments  included the 01:53 deputy director for informa tion 01:55 technology and communica tions  a  01:57 commander nava l s ecurity group command 02:00 for meade  maryland 02:02 and a t the  na tiona l s ecurity agency 02:04 where  he  s erved as  a  s enior opera tions  02:06 officer in the na tiona l s ecurity 02:09 opera tions  center 02:10 he  s erved as  the commanding officer 02:13 nava l informa tion opera tions  command in 02:15 norfolk virginia  where  he  was  s e lected 02:18 to flag rank in 2008 02:21 a s  a  flag officer he  focus ed on cyber 02:24 warfa re  s e rving as  the  director of 02:26 informa tion opera tions  on the  s ta ff of 02:28 the  chief of nava l opera tions  02:31 and a s  the  deputy commander for u.s  02:33 flee t cyber command u.s  10th flee t and 02:36 the  director of warfa re  integra tion for 02:39 informa tion dominance  on the  navy s ta ff 02:41 in the  pentagon 02:43 he  re tired from the  navy in 2014 and 02:46 worked in the defens e  indus try focus ing 02:48 on deve loping cyber capabilities  for the 02:51 milita ry 02:52 ra re  admira l lehr is  a  na tive  of ma ine 02:55 and has  a  bache lor of a rts  degree  in 02:57 politica l s cience  from the  univers ity of 02:59 s outhern ma ine and a  mas te r of a rts  and 03:02 na tiona l s ecurity and s tra tegic  s tudies  03:04 from the  u.s  nava l war college 03:07 s o le t's  welcome our firs t s peaker for 03:09 our a rtific ia l inte lligence  and cyber 03:11 s ecurity webina r today 03:14 admira l 03:16 your floor 03:19 thank you very much dr upton uh it's  uh 03:22 my honor to  be  uh 03:24 to be pa rt of the  pane l today and and 03:27 the  firs t thing tha t i will do i a lways  03:28 ge t a  little  nervous  when uh 03:31 i did in engineering intens e  uh 03:33 environments  tha t i want to  emphas ize  my 03:35 my s cience  was  politica l s cience  and and 03:38 today i think you'll s ee 03:40 probably a  lot more  a long the  policy 03:42 implica tions  03:43 uh tha t we  have  in cyber s ecurity and 03:47 and uh 03:48 and 03:49 wha t tha t means  and i think it tees  up 03:51 s ome of the  other pane lis ts  s o i've  got 03:53 a  pre tty s hort pres enta tion and uh 03:58 you know it kind of goes  a long with with 04:00 s ome thinking tha t i've  been doing of 04:01 la te  and you know how a re  we going to 04:04 cope  with 04:05 uh 04:06 cyber s ecurity over the  next couple  of 04:08 decades  and 04:10 you know for a ll tha t's  been s a id and 04:12 written about the end of uh 04:15 the  war on te rroris m  uh in a fghanis tan 04:18 uh 04:19 you know we were  there  for a  rea s on we 04:21 were  there  becaus e  the united s ta tes  was  04:23 a ttacked and 04:24 and we s pent an awful lot of money in 04:26 iraq and a fghanis tan over the  la s t two 04:29 decades  and a  lot of us  will remember 04:32 exactly where  we were  uh 20 yea rs  ago uh 04:36 in remembrance ceremonies  04:38 this  uh 04:39 this  s a turday 04:41 but but 04:42 thinking ahead 04:44 you know coming out of of where  the 04:46 na tion has  been 04:48 you know i i 04:49 think i come to the  conclus ion tha t 04:51 we 're  very unlike ly to have  another 04:54 la rge  04:55 uh conflic t tha t will commit wha t we  did 04:57 over the  las t 20 yea rs  04:59 and 05:01 if tha t's  only a  gues s  05:03 prognos tica tors  a re  horribly bad a t 05:05 predicting predicting war but 05:08 wha t does  tha t mean for for cyber and 05:11 cyber s ecurity in pa rticula r and 05:14 you know we a ll know if you're  looking 05:16 in this  this  kind of a rea  tha t the  pace 05:19 and the complexity of globa l cyber 05:21 a ttacks  in the  la s t 05:23 you know 25 yea rs  has  05:25 uh changed s ignificantly 05:28 now from my bio you you'll ge t pre tty 05:30 quickly tha t i was  the pa rt of the  navy 05:32 in the  pa rt of the  navy tha t was  05:34 a s s ocia ted with na tiona l s ecurity agency 05:36 in fort me i s pent you know mos t of the 05:39 la s t uh 15 yea rs  of my ca reer in tha t 05:42 environment 05:43 and you know through tha t i s aw you know 05:45 tha t rea lly 05:47 you know how we us e cyber uh as  pa rt of 05:50 es pionage  and 05:52 and you know as  time went on and in 05:55 be ing involved in the  s tand up of you 05:58 know the  navy cyber s ervice  flee t s occer 06:00 command and 10th fleet 06:02 you know there 's  rea lly tha t tha t 06:04 s pectrum tha t es pionage  where it 06:06 probably s ta rted in a ll na tions  06:09 there 's  certa inly cyber crime tha t 06:11 a ffects  us  a ll and and then you know 06:13 wha t i focus  on wha t i think about is  is  06:16 wha t cyber means  for warfa re  06:19 and you know for warfare  it's  s een the 06:21 s ame kind of uh 06:24 evolution tha t tha t we 've  s een in 06:26 protecting bus ines s es  and protecting 06:28 everything from from kitty s cripts  to 06:30 fis hing to zero uh day exploits  and and 06:33 la s tly with the s ola r winds  06:36 uh a ttack uh you know a  very complex 06:39 s upply cha in 06:41 uh exploit and and the  cos ts  a re  06:44 a re  mind-boggling rea lly if you go back 06:46 to 06:47 you know wha t was  clea rly a  politica lly 06:49 motiva ted uh 06:51 you know a ttack in 2007 in es tonia  you 06:54 know it's  rea lly ha rd to pinpoint wha t 06:55 the  cos ts  were  and you know s ome los s  to 06:58 banking revenue  is  es tima ted a round a  07:01 million dolla rs  07:02 you go forward you know you know e ight 07:05 yea rs  uh to wha t happened in in s audi 07:07 a rabia  aga in probably a  politica lly 07:10 motiva ted a ttack and retribution for 07:13 uh for s tuxnet you know 35 000 computers  07:17 another 7 500 s ervers  des troyed 07:20 and and it put the  s audi a rabia  oil 07:23 economy a t ris k 07:24 [Mus ic] 07:25 uh 07:26 a  couple  yea rs  la te r with not pe tya  07:28 aga in you know computers  des troyed more 07:30 s ervers  des troyed billions  los t and one 07:32 of the  inte res ting things  about not 07:34 pe tya  07:35 is  tha t a  07:37 united s ta tes  ins urance 07:40 ca rrie r decla red tha t it was  an act of 07:42 war and and has  re fus ed to to  pay on 07:45 ins urance s o 07:47 thes e  kind of uh 07:49 cha llenges  a re  in the  na tiona l s ecurity 07:51 rea lm  for a ll tha t we  do with s ober for 07:53 s ola r cyber s ecurity 07:55 and 07:56 la s tly within within the yea r you know 07:59 s ola r winds  which is  08:00 an incredibly complex s upply cha in 08:03 a ttack 08:04 and 08:05 wha t i think has  08:07 caus ed me 08:08 you know thought and worry as  as  it go 08:10 is  the 08:12 you know a lmos t the  s ens e  of 08:14 he lples s nes s  and and where  do you s ta rt 08:17 to unrave l this  tha t i hea rd from cyber 08:19 s ecurity experts  and people  who a re  08:22 trying to put toge ther the  s ola r winds  08:24 a ttack 08:25 and you know the  s tudy tha t was  done  by 08:28 you know pres identia l pane l uh 2016 tha t 08:31 you know thes e  cos ts  you know amount 08:33 be tween you know s even 08:35 fifty s even and a  hundred and uh nine 08:37 billion 08:38 uh it's  incredible  amount of money 08:40 tha t's  los t to  the economy 08:43 and s o 08:44 if we  do have  this  s itua tion where  we 08:47 a re  08:48 you know looking for 08:51 you know wha t cyber s ecurity looks  like 08:53 in a  con 08:54 uh outs ide  of a  conflic t you know we 've 08:57 got to  think about where  we a re  with 08:59 de te rrence  and 09:00 you know we know tha t there 's  a  c los e 09:02 re la tions hip be tween crimina l hackers  09:04 and na tion-s ta te  09:05 a ttackers  tha t you know an example of 09:08 tha t is  in in many pieces  of 09:11 uh ma lware  you can s ee  tha t it checks  09:13 for the pres ence  of a  cyrillic  keyboa rd 09:16 uh s o it does n't land on a  rus s ian 09:19 ta rge t 09:20 the  united s ta tes  has  tried you know 09:22 crimina l indictments  but they're  09:24 incredibly difficult to  act on 09:27 uh there  a re  often conflic ting roles  09:29 be tween es pionage and cyber s ecurity uh 09:32 we s aw tha t in the  obama  adminis tra tion 09:35 with you know agreements  with pres ident 09:37 z tha t we kind of le ft tha t a ll off the 09:39 table  becaus e  we we do want to  collect 09:42 inte lligence  09:43 but with with a ll de te rrence  there 's  a  09:47 neces s ity to  back 09:48 up uh 09:50 wha t we 're  trying to 09:51 uh prevent with s ome actions  09:54 and i think we 've s een a  diffe rent look 09:56 with with pres ident biden and 09:59 in the  warning to rus s ia  10:01 with ppd 10:02 21 uh warning but the  the  problem with 10:05 ppd21 it's  incredibly broad it's  10:08 everything 10:09 and if everything is  important how a re 10:11 we going to rea lly make tha t enforceable  10:14 and you know 10:15 followed by tha t did it have  an e ffect 10:17 is  the  our evil uh kind of dis appea ring 10:20 from the  uh 10:23 uh 10:24 from the  you know environment for a  10:26 while  is  tha t connected yeah i don't i 10:28 think it's  too ea rly to  know 10:30 but but a ls o in a  more  promis ing thing 10:33 you know s hortly a fte r the  putin biden 10:36 uh s ummit 10:37 there  was  a  m icros oft uh exchange  s erver 10:40 a ttack tha t was  uh a ttributed to china  10:43 and in both na to and the  eu join the 10:46 united s ta tes  and in condemning tha t s o 10:49 there 's  got to  be  thes e  kind of of 10:52 things  uh happening in this  environment 10:54 where  we 're  10:56 us ing a ll the  tools  of na tiona l s ecurity 10:58 to be able  to  do tha t and and la s tly you 11:01 know i think it's  you know we 're  gonna  11:02 have  to re think cyber s ecurity over the 11:05 long haul and wha t 11:07 you know it could mean over the  next 11:08 couple  of decades  and 11:10 and you know s ecurity has  to  be  more  by 11:12 default it you turn it on it's  going to 11:14 be  s ecure  tha t's  two-factor 11:16 authentica tion digita l identities  11:19 for mos t things  tha t we  do online  i know 11:21 tha t's  controvers ia l there  a re  s ome 11:23 machine  lea rning a i things  with file les s  11:26 ma lware  de tection you know a  company 11:28 ca lled blue  vector 11:29 you know has  advanced threa t de tection 11:31 tha t it lea rns  pre tty quickly wha t a  11:34 norma l environment looks  like  and is  11:37 very quick to uh 11:39 to 11:42 identify thos e  things  tha t a re  abnorma l 11:44 and and like ly ma lware  in an environment 11:47 and and work with uh traditiona l 11:49 cybers ecurity s ys tems  zero trus t it 's  a  11:52 huge  thing 11:54 within the  federa l government and dod uh 11:56 i think tha t has  to  be  how we think 11:59 about 12:00 uh 12:00 s ys tems  going forward 12:02 um 12:04 i have  to make it more  difficult to  12:05 remove  informa tion from a  s ys tem and in 12:09 tha t a rea  it's  things  you know da ta  los s  12:12 prevention it looks  aga in 12:13 with machine lea rning tools  to  to  do 12:16 behaviora l ana lys is  in rea l time to s ay 12:19 this  this  is  s omething tha t you don't 12:21 have  permis s ion to do 12:23 uh and one  of the  things  we lea rned from 12:25 uh s ola r winds  is  you know policy 12:28 enforcement which we 've  genera lly ta lked 12:30 about in te rms  of 12:33 how 12:34 it applies  to  individua ls  but policy 12:36 enforcement a ls o has  to apply 12:39 to uh s oftware  authorities  and if you 12:41 thought a  little  bit about the  s ola rwind 12:43 product tha t was  be ing us ed to 12:46 dis tribute  pa tches  uh wha t uh wha t can 12:49 your s oftware  acces s  and and and wha t 12:51 s hould it not be  able  to  acces s  s o 12:54 s o uh bringing uh tha t thinking tha t 12:56 we 've  done  uh in a  human s ens e  to a  12:58 machine  s ens e a s  well 13:00 and and the  las t thought is  it you know 13:03 comes  from an a rtic le  tha t was  you know 13:05 publis hed in fore ign a ffa irs  about a  13:07 yea r ago by genera l nakas one  who's  the 13:09 commander for united s ta tes  cyber 13:11 command and 13:13 is  defending ford 13:15 in a  traditiona l m ilita ry s ens e 13:16 defending forward is  s omething tha t 13:18 we 've  a lways  thought about you if you 13:20 wa it for s omeone  to a ttack you you're  13:22 probably going to los e  100 of the  time 13:25 i've  long a rgued tha t tha t cyber is  no 13:28 diffe rent than defending an a ir bas e  or 13:30 or defending aga ins t the  s ubmarine  and 13:33 genera l nakas oni bas ica lly s ays  we have 13:35 to defend beyond the  firmament of the 13:38 na tion 13:39 and tha t leads  to you know i think 13:41 rea lly how we think about cyber defens e 13:43 how we leverage  the 13:45 practices  and the  authorities  of our 13:47 a llies  to  be able  to  do tha t s o i think 13:50 tha t's  about my time and i will turn it 13:52 back over to  julian 13:56 thank you very much admira l 14:00 if you have  any ques tions  for the 14:01 admira l plea s e type  them into the  q a  14:04 portion and we 'll addres s  them a t the 14:06 end 14:07 our s econd s peaker 14:09 is  s cott mcgann 14:12 s cott mcgonn has  been a  s pecia l agent 14:14 with the  federa l bureau of 14:15 inves tiga tions  for 25 yea rs  14:18 during his  time with the fbi he  has  14:20 inves tiga ted white -colla r crime the 14:22 rus s ian and ita lian mafia s  cyber crime 14:25 counter-te rroris m  and es pionage ma tte rs  14:29 s pecia l agent magan is  an fbi certified 14:31 firea rms  ins tructor a  member of the 14:34 fbi's  evidence  res pons e team a  certified 14:37 police  ins tructor an fbi agent faculty 14:41 member teaching fbi cours ework to police 14:43 agencies  domes tica lly and abroad 14:46 he  rece ived his  undergradua te  degree 14:48 from the  univers ity of mas s achus e tts  a t 14:50 amhers t 14:52 his  mas te r of s cience  in crim ina l 14:54 jus tice  from the  univers ity of 14:55 mas s achus e tts  a t lowell and his  mba  from 14:59 bentley univers ity 15:00 he  currently teaches  is s ues  in cyber 15:03 crime and cyber s ecurity a s  an adjunct 15:06 faculty member a t young mas s achus e tts  15:08 law 15:09 in addition s pecia l agent ghan was  15:11 nomina ted for the 2018 a ttorney 15:14 genera l's  award for fraud prevention 15:17 and the 2018 fbi director's  award for 15:20 outs tanding crimina l inves tiga tion 15:23 for his  involvement in an inte rna tiona l 15:25 corpora te  es pionage inves tiga tion 15:28 agent magang is  the  a lpha  team leader 15:30 for opera tion works peed the  government's  15:33 full-s ca le  e ffort to s ecure  the 15:35 deve lopment and de livery of the  covit 19 15:38 vaccine  it currently is  a ls o involved in 15:41 tra ining and s peaking to the priva te  15:43 s ector 15:44 in academia  about cyber threa ts  15:46 corpora te  es pionage counterinte lligence 15:48 ma tte rs  ins ider threa ts  and inte llectua l 15:51 property theft on the  on beha lf of the 15:53 fbi 15:54 well welcome s pecia l agent the  floor is  15:56 yours  15:58 thank you s o much dr upton uh i'm going 16:01 to ta lk about s omething tha t's  a  little  16:03 bit outs ide  the  norma l rea lm  for 16:05 engineers  and and i i genuine ly thank my 16:08 colleague  for jus t introducing s ome of 16:10 the  ideas  of es pionage and hacking and 16:14 a ll of thos e  things  tha t i ta lk a  grea t 16:16 dea l about but one  of the  things  tha t 16:18 i've  been involved with la te ly tha t the 16:20 american public  genera lly does n't ge t to 16:22 s ee 16:23 is  a  number of diffe rent a s pects  to  the 16:27 whole  s ubject of fore ign influence  and 16:30 wha t i'm  re ferring to is  tha t our 16:33 country has  a  number of diffe rent 16:34 advers a ries  out there  in the  world and 16:38 a s  a s  does  every country certa inly 16:41 but as  pa rt of this  16:43 there  a re  advers a ries  out there  na tion 16:46 s ta tes  tha t a re  looking to obta in our 16:49 technology when they can't deve lop it 16:51 thems elves  s o a s  uh ieee  a  s ocie ty of 16:54 engineers  uh who a re  out there  working 16:57 ha rd to deve lop thes e  things  we don't 16:59 want to  s ee tha t idea  uh tha t 17:02 inte llectua l property s tolen by fore ign 17:05 agents  and s o i want to  ta lk a  little  17:08 bit about tha t becaus e  the 17:10 how it us ed to happen in the  pas t is  not 17:12 how it happens  now when we 're  dis cus s ing 17:14 es pionage  in the  pas t we  us ed to ta lk 17:17 about uh s pies  coming into the  country 17:20 deve loping s ources  and a s s e ts  and and 17:23 they would s tea l but now it's  s o much 17:25 broader than tha t and i want to  give  you 17:27 a  little  bit of an idea  of how tha t has  17:30 deve loped 17:32 we 've  s een the  headlines  uh previous ly 17:34 a ll over the  na tion about 17:37 diffe rent entities  uh diffe rent 17:39 countries  obta ining intellectua l 17:41 property whether a t univers ities  or 17:43 res ea rch a s s ocia tions  or companies  uh 17:46 obta ining this  technology for the ir own 17:49 benefits  certa inly s omething tha t the 17:51 fbi in the ir counter inte lligence 17:54 counter es pionage  divis ions  17:56 try to work aga ins t 17:59 i i i'm  jus t amazed tha t i can even s how 18:02 you this  s lide  tha t it 's  been 18:04 uncla s s ified in yea rs  pas t we  certa inly 18:07 wouldn't ta lk about anything re la ted to 18:10 counterinte lligence  but a s  you can s ee 18:12 here  from uh this  s lide  we have  a  number 18:15 of counter inte lligence  cas es  throughout 18:18 the  government throughout the  fbi 18:20 and the ca s es  on technology trans fer 18:24 have  increas ed markedly over the  la s t 18:27 two decades  18:29 i became involved with 18:31 inte llectua l property theft and economic 18:34 es pionage  in the  middle  of my ca reer and 18:38 have  not gotten away from it becaus e 18:40 it's  become s o preva lent you can s ee 18:42 tha t ca s es  on economic es pionage  and 18:45 counter prolife ra tion of technology 18:48 has  uh increas ed to about a  third of our 18:52 tota l counterinte lligence  cas es  and 18:54 aga in from my pers pective  be ing a  an fbi 18:58 agent for 26 yea rs  i have  never s een the 19:02 fbi put out a  s lide  like  this  previous ly 19:05 to the public  um s o this  s hould be  a ll 19:07 new informa tion for you but it 19:09 highlights  the importance  of technology 19:12 trans fer and i us e  tha t te rm  in the 19:14 pe jora tive  uh of technology trans fer a t 19:17 the  fbi 19:20 uh and and wha t i a lluded to previous ly 19:22 was  tha t technology trans fer is  coming 19:26 in a  lot of nefa rious  uh from a  lot of 19:28 nefa rious  vectors  it us ed to be  jus t 19:31 s pies  coming here  trying to find 19:33 informa tion uh and bring it back to 19:35 the ir home country and certa inly we have 19:37 tha t we  uh tha t has  never gone  away but 19:40 we a ls o have  diffe rent uh entities  19:43 diffe rent na tion s ta tes  uh influencing 19:46 our government as  we 've  hea rd about in 19:48 the  2016 e lection certa inly in the 2020 19:51 e lections  this  topic  has  come to the 19:54 fore  but a ls o more importantly and i 19:57 i'm  in the  bos ton a rea  and work in 19:59 grea te r new england and i can te ll you i 20:02 have  s een non-traditiona l collectors  a t 20:04 the  12 o 'c lock o 'c lock pos ition on this  20:06 graphic  non-traditiona l collectors  have 20:09 become 20:10 uh much more  important to  fore ign 20:13 governments  and s o thes e  non-traditiona l 20:16 collectors  a re  people  who a re  not 20:17 tra ined s pies  but they s imply have 20:19 acces s  to  the  informa tion tha t other 20:22 governments  want and for va rious  rea s ons  20:24 and s ometimes  becaus e  of a  little  20:27 intim ida tion they provide  this  20:29 informa tion uh from our country to the ir 20:32 typica lly the ir country of origin or to 20:35 other fore ign governments  there  a re  a  20:38 number of diffe rent ways  tha t fore ign 20:39 governments  will obta in 20:42 inte llectua l property informa tion and 20:44 the  ideas  tha t engineers  deve lop e ither 20:47 through hacking influence  or a  lot of 20:50 times  through ta lent convers ion where 20:53 they will have  ta lent recruitment 20:55 programs  and a  number of comp uh 20:57 countries  have  this  where  they will 21:00 acquire  informa tion from an individua l 21:03 who is  a  leader in tha t pa rticula r fie ld 21:06 s o if the  fie ld is  nano technology 21:09 they will e ffective ly co-op s omeone 21:12 through money cas h or a  number of other 21:14 methodologies  uh in order to  provide 21:17 tha t country with 21:19 informa tion uh on nanotechnology in tha t 21:22 pa rticula r example 21:24 s o s ome of the  techniques  a re  lega l 21:26 certa inly joint ventures  a re  providing 21:28 money and inves tment into companies  is  21:31 lega l but oftentimes  thos e  techniques  21:34 a re  not 21:35 clea rly trans pa rent in wha t's  going on 21:37 and certa inly une thica l a t a  m inimum 21:41 i'll give  you an example  in the  ta lent 21:43 plan uh cas e tha t i jus t mentioned 21:46 rega rding technology uh s ome of you may 21:48 know he  made  headlines  la s t yea r dr 21:51 lieber of ha rva rd univers ity uh was  21:54 a rres ted by mys e lf and s ome of my 21:56 colleagues  uh for making fa ls e  21:59 s ta tements  uh was  the  initia l cha rge  but 22:02 he  was  a llegedly involved in a  ta lent 22:04 program and i'll s how you here  an 22:06 excerpt from the  a ffidavit for the 22:08 a rres t warrant where  it's  highlighted 22:11 here  tha t he  was  ge tting fifty thous and 22:13 dolla rs  per month and an extra  150 000 a  22:17 yea r for living expens es  and money to 22:19 deve lop a  lab 22:20 over a t the  wuhan ins titute  of 22:22 technology 22:23 uh 22:24 you can s ee here an extra  50 000 22:27 a  month on top of a  uh wha t i perce ive 22:31 to be a  generous  ha rva rd s tipend uh an 22:34 annua l s a la ry uh was  certa inly 22:37 motiva ting for dr lieber when i a rres ted 22:39 him  with my colleagues  22:41 um he  was  certa inly not s urpris ed to s ee 22:45 uh tha t this  was  s omething s o if you a re  22:47 approached a s  an engineer out there 22:49 deve loping s ome new technology 22:52 or s omeone  a t your uh company has  been 22:56 approached uh there  is  a  quid pro quo 22:59 expected when s omeone 's  paying you fifty 23:02 thous and dolla rs  a  month uh for the 23:04 informa tion tha t's  in your head 23:08 here  is  a  traditiona l s py ms  yay here 23:12 was  a t bos ton univers ity uh pos ing as  a  23:16 s tudent s he  was  a  member of a  top 23:19 milita ry academy and directed by a  23:22 fore ign government 23:23 la s t yea r we  looked to a rres t her but 23:26 s he a lready s kipped town 23:28 and 23:29 before  s he  could be  a rres ted 23:31 this  young man was  a  medica l s tudent as  23:34 well in the  bos ton a rea  and 23:37 his  uh activities  were  dis covered a t the 23:40 a irport when 21 via ls  of a  biologica l 23:43 s ubs tance  were  found wrapped in his  s ock 23:46 when he  was  trying to go back to his  23:48 country of origin he  was  a rres ted a t 23:50 logan a irport wha t's  more  inte res ting 23:53 about this  pa rticula r ca s e  is  tha t this  23:56 happened 23:57 30 times  within a  s ix-month period with 24:00 diffe rent individua ls  um s o this  is  uh 24:04 the  wholes a le  theft of inte llectua l 24:07 property in this  cas e  from our bio bio 24:10 uh pha rma  indus try in the  bos ton a rea  24:14 and a s  fa r a s  corpora te  es pionage  goes  24:16 here 's  a  grea t ca s e i like  this  cas e  i 24:19 ca ll it a  grea t ca s e  becaus e  it was  one 24:21 of my cas es  uh american s uperconductor 24:24 was  a  company here  in mas s achus e tts  and 24:26 the ir inte llectua l property the ir low 24:28 voltage ride  through s olution for your 24:31 e lectrica l engineers  out there  in the 24:32 audience  uh was  s tolen by a  fore ign 24:35 company um and they us ed the  traditiona l 24:39 uh money 24:41 uh ego a s s uas ion and uh s exua l favors  uh 24:46 in order to  uh ge t de ion ca rabas ovac 24:50 s een right here  who was  a  s e rbian 24:52 na tiona l uh to flip for the ir pa rticula r 24:55 company s o he  was  the ins ider a t 24:58 american s uperconductor who gave  the 25:00 crown jewels  to a  fore ign competitor uh 25:04 a  very inte res ting cas e  which i can 25:06 us ua lly ta lk about a t length um it was  25:09 jus t made  into an fbi documenta ry which 25:12 will be  coming out this  month s o very 25:14 good cas e  on economic es pionage 25:20 and 25:23 inte lligence  opera tions  will ta rge t 25:25 academics  and res ea rchers  and recruit 25:28 uh people  a t va rious  companies  in our 25:31 country and will often make contact 25:35 through 25:36 profes s iona l ne tworking s ites  i am  not 25:38 immune from this  uh here  mandy which i'm 25:41 s ure  is  her given name uh reached out to 25:44 me on linkedin a s  i ge t to  the end of my 25:46 ca reer i put up a  linkedin page  and it 25:48 was n't very long before  mandy wanted to 25:51 be  friends  uh for thos e  of you who don't 25:53 know the  us  government is n't rea lly 25:56 enamored with tick-tock but i'm  s ure 25:58 it's  okay becaus e  you note  down here 26:00 tha t the  culture  there  is  magica l s o i'm 26:02 s ure  it's  okay to accept tha t uh 26:04 linkedin connection i jus t uh 26:06 s creens hotted this  as  uh for my future 26:09 lectures  becaus e  it was  s omething i had 26:11 ta lked about in the  pas t and here  it was  26:14 uh actua lly happened to me but not only 26:17 tha t but more  inte res tingly is  my 26:20 uh my two s ons  who a re  young ma les  in 26:24 the ir ea rly twenties  were  approached by 26:27 a s ian individua ls  a ttractive  fema les  on 26:30 the ir s ocia l media  right a fte r i ignored 26:34 this  uh 26:35 this  connection reques t and certa inly um 26:38 there  they don't m ind clicking on 26:40 connections  with a ttractive  uh fema les  26:42 from other countries  but they came to me 26:45 having had the  counter inte lligence  26:47 lecture  tha t i give  my children uh be ing 26:50 s ons  of an fbi agent and i s a id yeah 26:53 tha t's  becaus e  of me thanks  and uh they 26:55 ignored thos e connections  s o this  does  26:58 happen and it's  s omething you're  27:01 probably not very familia r with or 27:03 haven't hea rd much of but it does  happen 27:06 a ll over our country every day happens  27:09 to people  in the ieee  as  well 27:13 um and quickly wha t can we do to protect 27:16 ours e lves  i te ll everybody ca ll your 27:18 loca l fbi and pa rtner with them uh 27:22 corpora tions  who a re  out there  can ge t 27:23 be tte r lectures  uh than this  brie f 27:26 introduction and can get informa tion on 27:29 ris ks  and conflic ts  of inte res t we s peak 27:32 to boa rds  we  s peak to executives  we  ta lk 27:35 to adminis tra tors  a t res ea rch 27:37 ins titutions  a ll over the  country s o ge t 27:40 with your loca l fbi and as k for the ir 27:43 priva te  s ector coordina tor there 's  one 27:46 in every fbi office  and they will be 27:49 able  to  as s is t you in protecting 27:52 yours e lves  and certa inly they can hook 27:54 you up with the  cyber uh crime s quad i 27:57 worked in computer hacking for a  dozen 27:59 yea rs  i was  on the  cyber crime s quad and 28:02 even though i le ft it to  work other 28:03 ma tte rs  i never got away from cyber 28:05 crime s o i s till go out there  and 28:07 lecture  on bus ines s  ema il compromis e  and 28:10 rans omware  and hacking and 28:12 uh da rk web and a ll of thes e  other 28:15 s ubjects  but i wanted to introduce you 28:17 to the s ubject of uh fore ign influence 28:20 and es pionage um s omething you probably 28:23 don't ge t a  lot of a t your regula r uh 28:25 meetings  and uh i thank you 28:31 thank you very much s cott um 28:34 if you have  any ques tions  pleas e pos t 28:37 them in q a  28:39 and it's  my pleas ure  to introduce  our 28:41 next s peaker 28:44 dr dan s hoemaker 28:46 dr dan s chumacher rece ived a  doctora te  28:49 from the  univers ity of michigan in 1978 28:52 he  taught a t m ichigan s ta te  univers ity 28:54 and then moved to the  directors hip of 28:56 the  informa tion s ys tems  function for the 28:58 medica l s chools  a t ms u 29:00 he  he ld a  joint teaching a t depa rtment 29:02 cha ir pos itions  a t mercy college  of 29:04 de troit 29:05 when mercy was  cons olida ted with the 29:07 univers ity of de troit in 1990 he  moved 29:09 to the bus ines s  s chool to  cha ir the ir 29:11 depa rtment of computer informa tion 29:13 s ys tems  29:14 he  a ttended the  organiza tiona l rollout 29:16 of the  dis cipline  of s oftware  29:18 engineering a t the  ca rnegie  mellon 29:20 univers ity s oftware  engineering 29:21 ins titute  29:23 in the  fa ll of 1987 and he  was  a lready 29:26 teaching an s e i bas ed s oftware 29:29 engineering curriculum which he 29:30 es tablis hed as  a  s epa ra te  degree  program 29:32 to the mba  within the 29:35 udm college  of bus ines s  adminis tra tion 29:38 dr s howmaker s pecific  a reas  of 29:40 s chola rs hip publica tion and teaching 29:42 were  the  proces s -bas ed s tages  of the  29:44 wa terfa ll s pecifica tions  s qa  and 29:47 acceptance  s us ta inment he  was  a ls o a  29:50 primary cons ultant in the  de troit a rea  29:52 on the  cmm cmmi 29:55 dr s chumacher's  trans ition into cyber 29:57 s ecurity came as  a  res ult of the  audit 30:00 and compliance  e lements  of tha t body of 30:02 knowledge  a s  well a s  the  long 30:04 es tablis hed 30:05 s qa  s cm e lements  of the ir curriculum 30:09 they were  des igna ted the  39th center of 30:11 academic exce llence  by the  ns a  a t wes t 30:14 point in 2004 and they have  tried to 30:17 s tay on the  leading edge  in the 30:18 a rchitectura l a s pects  of cyber s ecurity 30:20 s ys tems  des ign and implementa tion as  30:22 well a s  s oftware  as s urance 30:25 a s  a  res ult of dr s chumacher's  30:27 a s s ocia tions  with ns a  and his  inte res t 30:30 in s oftware  a s s urance  he  pa rtic ipa ted in 30:33 the  ea rlies t meetings  of the  s oftware 30:34 a s s urance  initia tive 30:36 he  was  one  of the  three authors  of the 30:38 common body of knowledge  to produce 30:40 acquire  and s us ta in s oftware  and he 30:42 cha ired the  workforce  educa tion and 30:44 tra ining committee  from 2007 to 2010. 30:48 he  was  cha ir of workforce  tra ining and 30:50 educa tion for the  s oftware  a s s urance 30:52 initia tive  a t dhs  30:54 and he was  s ubject ma tte r expert for uh 30:57 you know for nice 30:59 s ecurity provis ion 31:01 dr s hoemaker was  a ls o a  s ubject ma tte r 31:03 expert 31:04 for the 31:06 human s ecurity 2017. 31:09 he  a ls o publis hed frequently in the 31:11 build s ecurity and webs ite  31:13 this  expos ure  led to a  grant to  deve lop 31:16 curricula  for s oftware  as s urance  and the 31:18 founding of the  center for cyber 31:19 s ecurity where  he  currently res ides  the 31:22 center is  a  free-s tanding academic unit 31:24 in the  college  of libera l a rts  which is  31:26 the  adminis tra tive  locus  for res ea rch 31:29 centers  within udm 31:31 dr s hoemaker's  fina l s ignificant grant 31:34 was  from the  depa rtment of defens e  to 31:36 deve lop a  curriculum and teaching and 31:38 cours e  ma teria ls  for s ecure  acquis ition 31:41 in conjunction with the  ins titute  for 31:42 defens e  ana lys is  and the  na tiona l 31:44 defens e  univers ity 31:46 a  book was  s ubs equently publis hed by crc 31:49 pres s  31:52 welcome dr s hoemaker 31:58 okay where  am i 32:02 um 32:04 i can hea r me i can't s ee  me 32:07 we can s ee  you we can hea r you we can 32:09 s ee  you and hea r you okay well then i'm 32:12 here  i 32:12 am um 32:15 gree tings  everybody 32:17 i uh 32:18 you know i when i do thes e  things  i try 32:20 to think about s omething tha t the  group 32:22 would find inte res ting 32:24 s o uh wha t i came up with 32:26 uh was  pre tty well covered by the firs t 32:28 two people  and s o i gues s  i'll jus t s ay 32:31 next s peaker 32:33 um 32:34 i le t me 32:36 ge t my 32:38 s lides  up 32:51 um 32:53 when i do thes e 32:54 ieee  vis its  i 32:56 try to come up with s omething tha t is  32:59 s ort of fits  with the 33:01 the  group i'm  ta lking to 33:03 um 33:04 mos t of the time i end up ta lking about 33:06 s upply cha in ris k management which is  my 33:09 a lleged a rea  of expertis e  33:11 um and um i 33:14 thanks  to the  s ola rwinds  people  i i find 33:16 mys e lf ta lking to a  lot of a  lot of 33:18 folks  about tha t but 33:20 um s ince  this  was  a i 33:22 i kind of 33:24 uh you know s ort of 33:27 tried to come up with s omething tha t 33:28 would be  a t leas t fit within tha t kind 33:30 of context and uh wha t i came up with 33:33 was  uh 33:34 s ome work i did back in 33:36 2008 uh was  it was  publis hed bas ica lly 33:39 in a  book 33:40 um 33:41 uh uh it kind of on the  topic  of cyber 33:43 crime 33:44 um and then wha t do i end up doing is  33:46 following an fbi agent s o you know you 33:49 can take  for wha t i've  got to  s ay uh you 33:51 know for wha tever it's  worth 33:54 but it 's  a  modes t propos a l and it fits  33:56 within kind of an ia  context s o 33:59 um 34:01 wha t you've  s een s o fa r in the  firs t two 34:03 pres enters  uh is  true 34:07 we 've  got a  worldwide  problem with cyber 34:10 crime or cyber a ttacks  take  your pick 34:13 um 34:14 micros oft did a  s urvey tha t was  rea lly 34:16 eye-opening publis hed back in december 34:19 uh about the  the  kind of the  the  cos t of 34:23 of cyber a ttacks  34:25 uh globa l cos t um tha t's  not jus t in the 34:28 u.s  34:29 um 34:30 500 34:31 billion dolla rs  with a  b in 2015 34:35 um and kind of we  worked on the problem 34:38 and 34:39 by 2020 it es ca la ted to 2 trillion 34:43 dolla rs  34:46 globa lly 34:47 and um 34:49 by the  time 2024 rolls  a round the 34:52 es tima te  is  6 trillion 34:54 s o uh it looks  like  uh cyber crime is  a  34:58 growth indus try it's  s omething tha t you 35:00 know i don't recommend you buy s tock in 35:02 but 35:02 um and i gues s  it's  becaus e it's  s o ea s y 35:06 um 35:07 one  of the  things  you might want to  us e 35:08 a s  a  s ens e  of context is  35:11 tha t 6 trillion is  the  gros s  na tiona l 35:13 product of england germany and france  uh 35:16 you know 35:17 and s o you know tha t's  kind of a  pre tty 35:19 big hit 35:21 into in the globa l economy 35:23 um 35:24 now the  reas on why obvious ly and people  35:26 the  firs t two pres enters  ta lked about 35:28 this  a t grea t length uh is  the na ture  of 35:31 the  inte rne t 35:32 um 35:33 it's  anonymous  and it's  borderles s  and 35:35 s o how in the world do you 35:39 defend aga ins t or pros ecute 35:42 s ome guy who is  s itting s omewhere  you 35:44 know not where  um a ttacking you 35:48 uh maybe  from the  other s ide  of the 35:50 world 35:50 um 35:52 and um it's  pos s ible  in certa in 35:54 countries  tha t if they're  s ucces s ful in 35:56 doing tha t to you um they may end up 35:59 with a  uh you know a  meda l uh 36:02 to a s  a  reward um and you know bas ica lly 36:05 wha t you've  got to  s ay is  a  bunch of uh 36:09 cultures  um tha t um a re  not neces s a rily 36:14 um going to be 36:17 big fans  of the united s ta tes  uh and 36:20 here  we a re  s itting there  kind of like  a  36:22 big fa t uh 36:23 plum wa iting to be  picked off a  tree  and 36:26 s o 36:27 the  inte rne t its e lf makes  it a lmos t 36:29 impos s ible  to  to  um 36:34 find and ca tch the  bad guys  36:36 um 36:38 obvious ly 36:39 s ome a re  willing to lead footprints  but 36:42 the  idea  bas ica lly is  tha t 36:44 um 36:45 the  inte rne t crim ina l is  wha t's  known as  36:47 an unknown s ubject um and the  only way 36:51 to rea lly kind of addres s  an unknown 36:53 s ubject is  by 36:54 the  cla s s ica l 36:56 approach known as  profiling 37:00 which bas ica lly us es  big behaviora l 37:02 s igna ture  s igna tures  37:04 now profiling has  been a round for a  37:05 rea lly long time 37:07 firs t profile  was  done  in 37:09 for jack the  ripper i don't know 18 37:11 s omething or other 37:12 uh and it's  deve loped 37:16 a s  a  37:17 a  a s pect of crim inology for yea rs  i mean 37:20 s ince  then um 37:22 and there  a re  37:24 um you know techniques  37:26 uh tha t a re  37:28 well recognized well known and us ed in 37:31 in in in 37:33 crimina l jus tice  37:34 uh ta lking about them from a  uh 37:38 a  cyber s tandpoint uh it's  kind of a  37:41 novel thing 37:42 becaus e  the  key bas ica lly is  the 37:45 behaviora l s igna tures  37:46 um 37:47 it's  a ll ba s ed on collecting uh wha t 37:50 amounts  to  evidence  of uh you know kind 37:53 of the  na ture  of the  crime uh a ll crimes  37:56 have  motiva ted opportunity and s o you 37:58 can kind of c la s s ify wha t you s ee  and 38:01 wha t you read 38:02 in thos e  actions  as  um you know a  means  38:06 of kind of uh cha racte rizing the 38:08 individua l tha t tha t's  bas ica lly 38:09 committed to crime 38:11 um 38:13 now 38:14 s ince  it's  done  on a  digita l device  tha t 38:15 actua lly makes  it s ort of ea s ie r uh 38:19 becaus e  uh it's  pos s ible  to  build a  38:22 inductive  profile  uh us ing evidence  tha t 38:25 you ga ther 38:27 from the  actua l actions  tha t a re  taken 38:29 on the  um you know by the  individua l uh 38:33 and recorded uh or a t lea s t ava ilable  to 38:36 be 38:37 you know kind of acces s ed through s ys tem 38:39 logs  and things  like  tha t um then 38:41 es s entia lly wha t you've  got is  a  pa tte rn 38:44 of behavior tha t may or may not be  us ed 38:46 to kind of crea te  a  typology and tha t 38:48 typology is  s omething tha t you can then 38:50 us e  a s  a  bas is  for 38:52 e ither inves tiga ting or 38:55 preventing a  type  of 38:58 a  uh you know 39:01 certa in types  of a ttack crim ina l a ttacks  39:05 um 39:06 things  like s ys tem logs  and s ys tem leve l 39:08 recons tructions  of a ttack behavior uh 39:12 you know 39:13 a re  a re  firs t of a ll 39:16 they exis t uh you know in the  s ens e tha t 39:18 tha t it 's  s omething tha t's  pa rt of 39:20 s ys tem proces s ing um and a t the  s ame 39:23 time uh you know there  a re  timelines  and 39:26 things  like tha t tha t you can us e 39:28 a s  a  bas is  for 39:30 um 39:32 kind of not kind of for for following 39:34 the  text 39:37 and cha racte rizing 39:40 the  times tamp time pa tte rn ana lys is  39:43 aga in is  a  fa irly common um 39:46 method for uh incident res pons e  um 39:49 and 39:50 um 39:51 we were  us ing uh 39:54 from a  s tandpoint of looking a t 39:56 um 39:57 the  kind of coding 39:59 a ttacks  40:00 uh things  like s tylis tic  and linguis tic 40:03 cha racte ris tics  a ll tha t's  s omething 40:06 tha t the  machine keeps  jus t s imply a s  40:08 pa rt of its  proces s ing 40:10 but a t the  s ame time you have  a  40:14 opportunity to us e  tha t a s  evidence  or 40:17 a s  a  bas is  a t lea s t for building 40:19 profiles  40:21 of of crim ina l activity or if you want 40:25 to us e  the  s imple  te rm  a ttacks  40:27 uh 40:28 on uh and and thos e a ttacks  bas ica lly 40:30 can can be  uh formed into 40:33 a  um 40:35 type  of 40:36 of uh 40:38 a  proactive  res pons e 40:40 uh 40:41 now the  idea  here  bas ica lly is  tha t and 40:44 thos e  of you who a re  s itting lis tening 40:46 to this  a re  s aying well tha t tha t s ounds  40:48 like  ne twork uh intrus ion de tection 40:51 automa ted intrus ion detection s ys tems  40:54 which is  true 40:55 but a t the  s ame time you can extend tha t 40:57 into 40:58 um you know the  rea lm  of actua l um you 41:03 know any kind of progres s ive  action 41:05 taken aga ins t a  41:07 ta rge t 41:10 a  ta rge ted res ource 41:12 and tha t bas ica lly is  s omething tha t is  41:16 um 41:18 then 41:19 tha t you can es s entia lly build a  defens e 41:21 aga ins t or res pond to as  appropria te  41:25 um now s ince  this  is  an a i 41:28 uh s es s ion uh the thing tha t i wanted to 41:31 ra is e  is  the  fact tha t this  can be 41:32 managed by a rtific ia l inte lligence  41:35 now wha t you end up with is  41:37 uh you know three  genera l types  of of of 41:41 uh a i type  um 41:44 profile  41:46 management s ys tems  uh one  is  s imply to 41:48 have  a  bas e line  of profiles  41:50 uh which then ended up a s  a  pre tty much 41:54 like  a  virus  checker you know to 41:55 identify uh 41:58 its  crim ina l behavior 42:00 a t the  point of a  tag 42:02 um and then do s omething appropria te  in 42:04 te rms  of e ither s hutting off the  s ys tem 42:06 or s hutting out the  acces s  or even jus t 42:09 s ending a  s igna l tha t s ays  we 're  be ing 42:11 a ttacked um 42:13 you can a ls o us e  bas e line  anomalies  42:15 which is  bas ica lly the s ame thing you 42:17 got a  profile  but in this  pa rticula r 42:19 cas e  you ge t s omething tha t jus t s imply 42:21 does n't fit ins ide the  profile  and with 42:24 the  a s s umption tha t uh if it 's  anomalous  42:27 it's  probably enemy action 42:29 and tha t can actua lly 42:31 identify things  tha t a re  not neces s a rily 42:34 a  uh 42:35 wha t do you ca ll it captured in the 42:38 behavior pa tte rns  42:39 tha t you've  us ed to build the  profile  42:43 the  problem with tha t one  is  can 42:44 genera te  42:45 fa ls e  pos itives  a  lot of fa ls e  pos itives  42:48 and s o it's  not s omething tha t's  rea lly 42:49 very practica l right now and la s t but 42:51 not leas t you can have  anomalous  42:53 proces s ing 42:54 which is  uh 42:56 we 'll identify the  a ttack a s  it's  42:58 happening becaus e es s entia lly wha t's  43:00 going on in te rms  of the norma l s equence 43:02 of events  ins ide  the computer is  not 43:06 it's  not kos her it's  not s omething tha t 43:09 would be  norma l if tha t's  the ca s e um 43:12 you know you can ge t a  warning a t the 43:14 point where  the  a ttack's  occurring 43:17 the  problem with tha t aga in is  this  43:18 complex is  kind of ha rd to manage and 43:21 a ll this  bas ica lly is  nothing more  than 43:23 me ta lking about 43:24 um kind of s ome novel approach tha t you 43:27 might take  bas ed on wha t amounts  to 43:30 well-es tablis hed 43:33 uh proces s es  43:35 uh both uh from a  crim ina l jus tice 43:37 s tandpoint and a ls o from the s tandpoint 43:40 of computing 43:41 and um tha t from my you know is  43:44 bas ica lly a ll i have  to ta lk about here 43:46 uh any ques tions  any dis cus s ions  you 43:48 want i gues s  i'll handle  tha t a t the end 43:54 thank you very much 43:55 dan 43:56 and our la s t s peaker today is  dick 43:59 wilkins  principa l technology lia is on for 44:01 phoenix technologies  lim ited a  us -bas ed 44:04 independent pla tform firmware 44:05 deve lopment company and a ls o an 44:07 a s s ocia te  profes s or of computer s cience 44:09 and cyber s ecurity a t thomas  college  in 44:11 centra l ma ine  recently re tired 44:14 he  s its  on the  boa rd of the  unified 44:16 extens ible  firmware  inte rface  forum and 44:19 leads  our s ecurity res pons e  team he 's  a  44:22 leader in the  ieee  a t the  s ection leve l 44:24 and in the computer s ocie ty and is  44:26 active  in the  acm and pmi he  has  over 30 44:29 yea rs  indus try experience  in roles  from 44:32 s oftware engineer to  director of 44:33 engineering a t companies  like  hugh 44:35 packa rd digita l equipment corpora tion 44:37 micros oft amazon and s evera l s ma lle r 44:39 firms  44:40 profes s or wilkins  holds  a  phd in 44:42 computer s cience  from nova  s outheas te rn 44:45 univers ity a  mas te r of s cience  in 44:47 computer s cience  from the  na tiona l 44:49 technologica l univers ity 44:51 and a  bache lor of a rts  in public  44:53 adminis tra tion from s a int thomas  44:54 univers ity in miami florida  44:57 welcome dick 45:00 thank you very much dr upton i 45:02 apprecia te  the 45:04 introduction um le t me go ahead and ge t 45:07 my s lides  up here 45:09 [Mus ic] 45:16 okay 45:18 s o 45:19 i'm  going to take  this  45:21 from the  genera l to 45:23 a  little  more  s pecific  i'm going to ta lk 45:26 about a i in re la tion to s ecurity a round 45:30 pla tform  firmware 45:32 now mos t of you may s ay well gee 45:35 is n't tha t pla tform  firmware  s tuff uh 45:38 s omething tha t runs  in the  firs t couple  45:40 of m illis econds  or you know firs t few 45:42 s econds  a t mos t of the  computer s ys tem 45:45 a s  it boots  up and then kind of goes  45:47 away and 45:48 why do i ca re  and wha t does  this  45:50 inte res t me 45:51 uh wha t 45:53 why do i ca re  about the  s ecurity of tha t 45:54 pa rticula rly 45:57 in fact 45:59 it's  a  s e rious  problem in uh tha t i'm 46:02 going to be  demons tra ting to you but in 46:04 our firs t pres enta tion today a  couple  of 46:07 thos e  ea rlie r and mos t impactful a ttacks  46:11 and pa rticula rly 46:13 notorious  is  the  s audi a rabian 46:17 a ramco a ttack was  a  firmware  a ttack 46:20 where  they exfiltra ted a  bunch of 46:23 da ta  from thos e  s ys tems  and then bricked 46:26 them 46:27 and turn 46:28 over 35 000 computers  into boa t anchors  46:32 and the company had to comple te ly 46:34 replace  the ir entire  it infra s tructure 46:38 s o this  is  an example  of how s erious  46:40 firmware  a ttacks  can be  46:44 s o 46:48 there  we  go s o firmware  is  critica l 46:52 it's  the  runs  right a fte r power up 46:54 initia ls  tha t initia lizes  the  cpu and 46:57 ha rdware  protections  upda tes  the cpu 47:00 microcode 47:01 it controls  the  highly s ecure  inte r 47:04 proces s or modes  tha t even the  opera ting 47:06 s ys tem and hypervis ors  can't touch 47:09 it protects  non-vola tile  memory s ys tem 47:11 upda tes  e tc  it s ecure ly boots  the  os  and 47:15 ma inta ins  a  route  of trus t from the  cpu 47:19 ha rdware  its e lf tha t power up through 47:21 a ll of the  initia liza tion and boot 47:23 loaders  and everything e ls e  47:25 out to  an opera ting s ys tem and 47:27 theore tica lly a ll the  way out to  an 47:29 applica tion s o tha t the s ys tem can be 47:32 proven to be  s ecure  a t a t lea s t until 47:34 the  app runs  47:36 now once  it's  online  and connecting to 47:39 the  inte rne t of cours e  a ll be ts  a re  off 47:41 and it can be  a ttacked but 47:43 vendors  s oftware  vendors  and opera ting 47:45 s ys tem vendors  have  been working rea lly 47:47 ha rd over the  la s t 47:49 many yea rs  to  well ever s ince  the 47:53 inte rne t 47:54 s ys tem s ta rted ge tting connected to the 47:56 inte rne t to  protect the ir s tuff 47:59 and s o it turns  out tha t um wha t's  le ft 48:02 is  firmware 48:04 um continuing with my lis t here  it can 48:07 a ttes t to the  s ys tem s ecurity the 48:09 firmware  can and provide  evidence  to an 48:11 externa l verifie r 48:13 tha t the  s ys tem is  okay 48:15 um and it provides  critica l s e rvices  to 48:18 os  os 's  and applica tions  while  they're  48:20 running and people  don't rea lize  tha t's  48:23 going on but the  firmware  is  s till there  48:25 and s till opera tiona l and la s tly it's  48:29 pers is tent 48:31 if you can modify change  or hack a  48:34 s ys tem firmware  then 48:37 it's  there  potentia lly forever 48:40 and even wiping the  s ys tem and s ta rting 48:42 with a  new dis k drive  or s omething like 48:44 tha t can't remove  it 48:46 s o as  48:47 pa raphras ing a  google  engineer from a  48:50 few a  few yea rs  ago if you don't own 48:52 your firmware  your firmware  owns  you 48:57 s o 48:57 wha t is  pla tform  firmware  it's  you know 49:00 it's  the  thing tha t i've  been ta lking 49:01 about here  but 49:03 depending on the  implementa tion and wha t 49:05 it's  for and wha t kind of pla tform  it is  49:08 it can be  thous ands  of lines  to  millions  49:10 of lines  of code 49:13 mos t commonly nowadays  it follows  the 49:15 open 49:16 uefi s tanda rd 49:18 and a s  the  footnote  here  on the  s lides  49:21 is  the  unified extens ible  firmware  49:23 inte rface  s pecifica tion tha t defines  the 49:27 inte rfaces  be tween 49:29 the  opera ting s ys tem and applica tions  49:32 and 49:32 the  underlying firmware 49:35 during the  boot proces s  and then a fte r 49:36 the  s ys tem is  up and running 49:39 there  is  a  cus tom 49:42 mos t implementa tions  of this  a re  49:44 cus tomized from an 49:46 open s ource  tiana  core  implementa tion 49:49 tha t's  code  name from when it was  firs t 49:52 s ubmitted to the  open s ource 49:54 there  a re  a ls o older 49:57 boot firmware  ca lled u-boot and core 50:00 boot a re  the  mos t common they're  a ls o 50:02 open s ource  they're  typica lly us ed for 50:05 embedded and iot devices  and a  lot for 50:09 phones  and things  like  tha t they form 50:11 the  bas is  for s ome of the  chrome books  50:13 and things  like  tha t out there  but um 50:16 nowadays  they're  now s tanda rdizing on 50:20 the  uefi inte rfaces  s o even while  it's  a  50:22 comple te ly diffe rent implementa tion 50:24 they're  doing uefi things  50:27 la s tly there 's  linux boot 50:30 bas ica lly a  50:31 minima l linux 50:34 piece  of s oftware  tha t's  us ed to boot 50:36 full linux 50:38 this  is  more of an experimenta l thing 50:40 tha t's  going on and a  lot of people  a re  50:42 playing a round with it but it 's  rea lly 50:45 uncommon in 50:46 commercia l s ys tems  50:49 modern implementa tions  of this  of a ll of 50:52 thes e  us e  has hing and s igna tures  to  make 50:55 s ure  they're  running s ecure  and 50:56 unmodified code 50:58 they a ls o us e  s ecure  upda tes  in any 51:00 rollback to make  s ure  tha t nobody is  51:04 providing them bad code  over the 51:06 inte rne t and caus ing them to upda te  or 51:09 roll back to older uns ecure  code  e tc 51:13 they a ls o tend to meas ure  thems elves  51:15 a ls o re fe rred to a s  meas ured boot 51:18 s o tha t they can a ttes t 51:21 to the ir s ecurity and the fact they've 51:23 been unmodified to an externa l verifie r 51:28 tha t may control the ir acces s  to 51:29 ne tworks  and things  like  tha t 51:33 a s  51:34 thes e  things  tha t i've  been ta lking 51:36 about here  a re  bes t practices  51:38 they're  the  things  tha t s hould be  done 51:40 to make  s ure firmware  is  s ecure 51:44 but 51:44 many low-cos t and iot devices  embedded 51:48 s ys tems  and s urpris ingly and annoyingly 51:52 a  lot of pcs  and s ervers  out there 51:56 don't follow this  or they turn it off or 52:00 and s o they're  not a s  s ecure  a s  they 52:02 s hould be 52:03 but 52:04 this  is  not your 1970s  bios  the  thing we 52:09 ta lked about you us ua lly s ee 52:12 the  indus try continues  to us e  the  te rm 52:14 bios  a s  s horthand for pla tform  firmware 52:17 but it 52:18 rea lly is n't anything like wha t ibm 52:21 crea ted for the ir firs t pc back in the 52:24 1970s  and ea rly 80s  52:27 s o 52:28 i put up this  not 52:31 for any s pecific  piece  of informa tion 52:33 but i want to  point out jus t genera lly 52:35 the  million line  plus  52:38 bios  is  jus t the  the firs t line  in this  52:42 uh 52:43 cha rt of 52:46 the  firmware  tha t ge ts  loaded on a  52:47 machine 52:49 um but there 's  a ll kinds  of other code 52:52 tha t runs  during the boot proces s  52:55 tha t s ecures  the  s ys tem upda tes  the 52:58 microcode  this  is  for a  currently uh 53:01 wide ly ava ilable  53:03 inte l proces s or an example  of the  bill 53:06 of ma teria ls  of the  firmware  tha t 53:08 ge ts  loaded a t boot time and initia lized 53:11 and run during the  s ta rtup of an inte l 53:14 uh cpu s o the  de ta ils  don't ma tte r here 53:17 i'm  jus t pointing out there 's  a  lot of 53:18 s tuff here  and it's  rea lly important and 53:22 if it ge ts  53:23 damaged in s ome way by a  hacker bad 53:26 things  can happen 53:29 oops  53:30 we s omehow got ahead of ours e lves  here 53:35 okay 53:37 s o 53:38 this  is  an a i pres enta tion s o i want to 53:40 make s ure  tha t we  tie  this  back to how 53:43 does  a i fit into this  is s ue  of s ecurity 53:47 of pla tform  firmware s o if i'm  an i.t 53:50 manager i want to  know a ll the  devices  53:51 in my s ys tem a re  following the  bes t 53:54 practices  and a re  properly protected 53:56 they're  doing the right thing with the ir 53:57 firmware  becaus e aga in if any of them 54:01 is  compromis ed they can a ll be 54:03 compromis ed and 54:05 bad things  can happen throughout the 54:07 ne twork 54:08 i've  s een bas eboa rd management 54:11 controlle rs  in multi-m illion dolla r 54:14 s ervers  with hundreds  or hundreds  of 54:17 proces s ors  anyway uh where  the  bas eboa rd 54:20 management controlle r an old piece  of 54:23 firmware  and s oftware  tha t 54:26 manages  the  s ys tem opera tion where  tha t 54:28 thing has  been pene tra ted 54:30 and it has  s pread 54:33 an infection to every virtua l machine 54:36 running acros s  hundreds  of proces s ors  54:39 within the  s ame box 54:40 and then it can then expand out to the 54:42 entire  ne twork 54:44 s o 54:46 i want to  be  able  to  s can a ll the 54:47 devices  in my ne twork in rea l time and 54:50 identify vulnerable  damaged devices  54:53 anything bad tha t could be  going on and 54:56 i want to 54:57 identify devices  a t ris k even if they're  54:59 not currently behaving badly intrus ion 55:03 de traction s ys tems  a re  fine  monitoring 55:05 the  ne twork us ing a i to look for 55:08 pa tte rns  and bad behaviors  and identify 55:10 devices  tha t have been damaged but 55:13 gee  wouldn't it be  nice  to be  able  to 55:16 identify them before  they go rogue 55:19 before  they s ta rt 55:20 exfiltra ting da ta  from my from my 55:23 ne twork e tc 55:24 s o 55:27 but there  a re  thous ands  of devices  55:30 they're  running firmware  from many 55:31 s ources  and of many types  55:33 i've  ta lked in the  previous  s lide  about 55:36 gee  how much there  is  out there 55:38 um how can i make  s ure  tha t they're  not 55:41 vulnerable  they're  not damaged by an 55:44 a ttacker tha t they're  not 55:47 in s ome way going to come and bite  me in 55:49 the  rea r end 55:51 s o 55:52 one  option is  us e  a i machine  lea rnings  55:55 to s can them and eva lua te  the ir the ir 55:58 a s s e ts  55:59 s o here 's  a  56:01 kind of a  marke ting picture  rea lly of 56:03 wha t a  s ys tem might look like  tha t does  56:06 tha t 56:07 we have 56:09 a  us er inte rface 56:12 it's  s tep one  there  tha t you can 56:15 s chedule  an immedia te  s can of your of 56:17 your ne twork or 56:19 have  a  56:20 a  s can tha t runs  periodica lly or 56:22 wha tever 56:24 then we have  s canning s oftware the  red 56:26 ba ll in the  middle 56:28 tha t goes  out and touches  everything on 56:30 my ne twork 56:31 and 56:33 takes  a  look a t the  firmware  56:36 tha t's  running there  its  a ttributes  its  56:38 configura tion e tc 56:41 and then um 56:43 then s ends  tha t da ta  out to  a  s ecure 56:45 cloud 56:46 which runs  a i a lgorithms  to uh 56:50 identify wha t's  going on here 56:53 then 56:54 very quickly 56:55 becaus e  we 're  running out of time 56:58 we want to  extract 57:01 an image of wha t's  going on we want to 57:02 s cout it for improper configura tion 57:06 va lid code  s igna tures  e tc  known 57:08 vulnerabilities  this  can be  done without 57:10 machine  lea rning but then we can us e 57:14 machine  lea rning s imula te  the  code flow 57:17 to make  s ure a  cha in of trus t is  57:19 ma inta ined regenera te  c  code  from the 57:21 bina ry image  do s ta tic  code  ana lys is  e tc 57:25 we can identify inventabilities  obs erve 57:27 ris ky code practices  e tc 57:29 we can 57:30 identify is s ues  and we can take 57:32 automa tic  action or we  can te ll an i.t 57:35 manager tha t this  device  is  s us pect and 57:37 you want to  eva lua te  it and do manua l 57:40 ana lys is  57:42 and 57:43 la s tly before  i wrap up here  i jus t want 57:46 to s ay we can apply this  to  other 57:48 potentia l kinds  of ne tworks  how about 5g 57:50 ne tworks  with phones  and table ts  and iot 57:53 how about s mart vehicle  s ys tems  autos  57:55 trucks  e tc  and wha t about tha t 57:58 autonomous  vehicle  wouldn't you like  to 58:00 have  s omebody checking tha t the 58:01 driverles s  de livery truck firmware  58:04 tha t's  trave ling in the  lane  next to you 58:05 on the  highway is  actua lly s ecure and 58:08 s a fe  58:09 and there  a re  potentia lly many other uh 58:12 things  where we could apply this  to  s o 58:15 anyway 58:16 tha t's  it for me thank you 58:23 s o unfortuna te ly we 're  out of time um 58:26 exce llent pres enta tions  thank you very 58:28 much it's  been a  plea s ure  to modera te  58:29 this  pane l and the  pane lis ts  been typing 58:32 the ir ans wers  for s ome of the  ques tions  58:33 from the  audience  into the  q a  thank you 58:36 very much 58:37 and for c los ing remarks  here 's  dr eberty 58:40 thanks  very much uh aga in 58:43 julia  our modera tor and a ls o our 58:45 s peakers  and a ls o thos e  of you who 58:47 a ttended this  live  pres enta tion or 58:49 wa tching the  recording la te r if you put 58:51 your ques tion q a  and the  s peakers  were 58:53 not able  to  ans wer them live  will pos t 58:57 the ir ans wers  la te r on on our a i webs ite  59:00 i jus t want to  bring to your a ttention 59:02 tha t if you like  this  pres enta tion we 59:03 have  our october 7th event coming up a i 59:06 in s pace and aeros pace event and a ls o uh 59:09 we have  our november four events  a r in 59:12 hea lth ca re  and we have  nas a  and nih 59:15 s peakers  coming in s o thanks  very much 59:17 aga in everyone for joining us  today and 59:19 uh you can wa tch the recording of this  59:21 la te r 59:22 uh on our webs ite  if you would like  a  59:25 pdh or cu certifica te  you can jus t ema il 59:28 your name and ema il addres s  and your 59:30 a ffilia tion to the  ema il i s ha red in the 59:32 cha t box um.a i a t ma in.tdu i rece ived 59:36 your certifica te  s o thanks  aga in and s ee 59:38 you next time    The University of Maine in Orono is the flagship campus of the University of Maine System, where efforts toward racial equity are ongoing, as is the commitment to facing a complicated and not always just institutional history. The University recognizes that it is located on Marsh Island in the homeland of the Penobscot nation, where issues of water and its territorial rights, and encroachment upon sacred sites, are ongoing. Penobscot homeland is connected to the other Wabanaki Tribal Nations \u2014 the Passamaquoddy, Maliseet, and Micmac \u2014 through kinship, alliances, and diplomacy. The university also recognizes that the Penobscot Nation and the other Wabanaki Tribal Nations are distinct, sovereign, legal and political entities with their own powers of self-governance and self-determination. ",
        "title": "Artificial Intelligence in Cyber Security",
        "citationCount": 0
    },
    {
        "content": "The University of Maine DigitalCommons@UMaine General University of Maine Publications University of Maine Publications 3-4-2021 Manufacturing and Materials University of Maine Artificial Intelligence Initiative Follow this and additional works at: https://digitalcommons.library.umaine.edu/univ_publications  Part of the Artificial Intelligence and Robotics Commons, Higher Education Commons, and the History Commons This Video Recording is brought to you for free and open access by DigitalCommons@UMaine. It has been accepted for inclusion in General University of Maine Publications by an authorized administrator of DigitalCommons@UMaine. For more information, please contact um.library.technical.services@maine.edu.   UMaine Artificial Intelligence: Manufacturing and Materials   Date: March 4, 2021 Run Time: 01:00:09 https://youtu.be/-i_U9O_nFeI  UMaine AI draws top talent and leverages a distinctive set of capabilities from the University of Maine and other collaborating institutions from across Maine and beyond, while it also recruits world-class talent from across the nation and the world. It is centered at the University of Maine, leveraging the university\u2019s strengths across disciplines, including computing and information sciences, engineering, health and life sciences, business, education, social sciences, and more.  Transcript is machine generated, unedited, in English.  00:03 okay 00:03 um welcome to the humane artificial 00:07 intelligence webinar 00:08 on ai for manufacturing my name is ali 00:12 abedi i'm associate vice president for 00:13 research at university of maine and i'm 00:16 excited to introduce our panel of expert 00:19 speakers from academia 00:22 industry and government agencies to talk 00:24 about 00:25 what's happening on artificial 00:27 intelligence use and manufacturing and 00:29 materials 00:30 so from wherever you're joining us 00:32 either from 00:33 the west coast or east coast of the 00:35 united states or from 00:38 europe in ieee india or i typically 00:42 china colleagues i welcome everybody 00:44 here 00:44 good morning good afternoon and good 00:46 night depending on 00:47 where in the world you're tuning in um 00:50 we are going to have 00:52 hsp here talk for almost 10 minutes 00:54 about 00:56 the topic of ai for manufacturing uh 00:59 feel free to 01:00 post your questions in the q a as they 01:03 come to your mind 01:04 and after all the first speakers are 01:06 talking uh talks are over then i will 01:09 um pose the questions and then we can go 01:12 over the question and 01:13 uh answer period toward the end of the 01:16 program so this is a one hour webinar it 01:18 will be recorded and 01:20 um we'll post it basically later on 01:23 so without further ado let me 01:27 start the panel by introducing our first 01:30 speaker dr tony schmitz is a professor 01:35 in mechanical aerospace and biomedical 01:37 engineering department at university of 01:39 tennessee in knoxville 01:40 with a joint faculty appointment at oak 01:43 ridge national laboratory he's a very 01:45 distinguished and accomplished 01:48 researchers if i want to read his bio 01:50 it will take the entire hour so let's 01:52 skipped that but i will just highlight 01:55 that 01:55 he is one of the experts in the country 01:57 in terms of uh manufacturing 01:59 and also uh he has received a number of 02:02 awards and recognitions like young 02:04 investigator award and 02:06 nsf career award has lots of patents and 02:09 publications so we are very 02:11 honored and excited to listen to dr 02:14 schmitz today so tony take it away 02:20 thank you like i can only go down from 02:22 there so let me let me try to do my best 02:25 um so my interest is in 02:29 trying to understand how we can leverage 02:32 advances in machine learning 02:34 for machining so machine learning for 02:37 machining 02:37 and in particular milling operations is 02:40 what i'm interested in 02:41 um so how can we kind of bridge this gap 02:44 between 02:46 the great work that's been done in 02:47 machine learning and the manufacturing 02:49 shop 02:50 floor so i'm going to describe today one 02:52 particular 02:54 implementation of machine learning and 02:57 i'm going to use 02:58 models that we've developed in the past 03:01 for machining 03:02 as a way to guide that machine learning 03:06 process 03:06 so this physics guided machine learning 03:08 approach says 03:10 i have some physics-based models i can 03:13 use those as a low-cost way to generate 03:16 a lot of data to initially train my 03:19 machine learning model 03:20 but because i have uncertainties 03:22 associated with that physics-based model 03:25 i can improve my machine learning model 03:27 now by collecting new data 03:29 and adding that to the original data set 03:32 that was provided by my physics-based 03:34 models 03:35 so i'm going to show that application 03:37 with relation to 03:39 milling so first i'll talk just a bit 03:42 about machine learning 03:44 and then the models that we apply the 03:45 physics-based models 03:47 and then i'll demonstrate briefly a case 03:49 study that we completed to demonstrate 03:51 this approach 03:53 so machine learning as we know is a 03:55 data-driven approach 03:57 we have machine learning and statistical 04:00 techniques which can both be applied 04:02 where i want to learn from my either 04:04 continuous sensor data or 04:06 discrete measurement results during or 04:08 after the manufacturing process 04:11 so this is this is a great advantage 04:14 when i don't have a great 04:16 an understanding of the relationship 04:18 between the inputs 04:20 and outputs for my manufacturing process 04:23 in that in that way i can develop those 04:26 correlations 04:27 simply from the data that i collect 04:29 during the process 04:30 the challenge is that those correlations 04:32 don't know about my physical laws and 04:34 sometimes they can lead me to 04:36 a place i didn't want to go because 04:39 either inadequate data or 04:40 uncertainty in my data and so on and it 04:43 may be difficult to generalize beyond 04:45 that training data set 04:47 so in this work we're leveraging machine 04:50 learning 04:50 in particular classification which is a 04:53 supervised learning approach 04:55 where i'm trying to collect data and 04:56 then make decisions based on that data 04:59 by classifying the outcomes for example 05:02 if i showed you a face image 05:06 you could tell me probably whether that 05:07 was a male or a female 05:11 in the same way what i want to do here 05:13 is i want to introduce 05:15 them you to a spindle speed and 05:18 with combination for my machining 05:20 parameters and then have you tell me 05:22 is that going to be stable or unstable 05:24 in other words am i going to get good 05:26 machining performance or poor machining 05:28 performance from that combination 05:30 there's lots of choices and we've 05:32 applied some of those the one i'll show 05:34 you today is a k-nearest neighbor 05:36 very simple approach okay so i said 05:38 we're going to have physics-based models 05:40 that we're going to use to train our 05:41 algorithm 05:42 so one of the things i need to know is 05:45 the vibration behavior 05:47 of this tool holder spindle machine 05:49 combination that i selected for this 05:52 machining activity 05:53 so we're going to use an approach where 05:55 we take models 05:57 of the holder and tool and then we 05:59 couple them 06:00 in the frequency domain to a measurement 06:02 of the spindle and 06:03 machine in order to predict those 06:06 assembly dynamics or what's the 06:07 vibration response 06:09 at the at the end of my cutting tool 06:12 where i'm going to be performing the 06:13 machining test 06:15 so there's lots of equations here but 06:17 essentially what this is saying 06:19 is if i can describe the dynamics of my 06:22 components of my individual pieces 06:25 then there's a there's an analytical way 06:27 to put those dynamics together 06:29 to predict the assembly dynamics and so 06:33 ultimately by following 06:36 um the the modeling of the individual 06:39 pieces 06:41 compatibility conditions at the boundary 06:43 and then equilibrium conditions where 06:44 i'm 06:45 connecting things i end up with an 06:47 equation which says i can predict the 06:49 assembly dynamics 06:51 from the component dynamics so that's 06:54 one of the models 06:58 so have you shown there that's a milling 07:00 cut for those of you who haven't spent a 07:02 lot of time around milling machines 07:04 so what you saw was a rotating tool 07:06 removing material 07:07 and flinging these chips away as it as 07:10 it cut away that material 07:12 so one of the things we need to 07:14 understand is that the tool is not 07:16 rigid and there's forces applied to that 07:18 tool 07:19 dynamic forces in order to fling away 07:21 those chips 07:23 and so that leads to a situation where i 07:25 have vibrations during my cutting 07:27 process 07:28 and those vibrations can be good we call 07:30 forced vibrations or there can be bad 07:32 what we call chatter or self-excited 07:34 vibrations 07:36 um so in terms of that modeling i have a 07:38 mechanistic approach to describe 07:40 those vibrations which includes cutting 07:42 force 07:43 that cutting force we estimated using 07:46 finite element 07:47 simulation to determine these 07:49 coefficients that relates the force to 07:51 the chip that i'm removing 07:54 okay so if i have my structural dynamics 07:56 that i predicted in my cutting force 07:58 model that i predicted i can bring those 08:00 together 08:01 into a frequency domain solution that 08:04 separates the bad vibrations 08:06 chatter from the good vibrations the 08:08 stable or forced vibrations and so the 08:10 gray 08:11 region in that plot is the is the bad 08:14 vibrations 08:15 and the white region is where we have um 08:18 good machining behavior okay so the big 08:21 thing that i 08:22 face when modeling mechanistically when 08:25 i use physics-based models to describe 08:27 this approach is 08:29 if i make a prediction and then perform 08:31 an experiment 08:32 and that experiment doesn't agree with 08:34 my prediction 08:35 i do not have a backwards solution i 08:38 only have the forwards 08:39 solution so that's what was very 08:41 intriguing to me about machine 08:43 learning is to enable me to connect 08:46 my experimental result to the inputs 08:50 in a way that wasn't available to me 08:52 before so here's a case study that we 08:54 ran 08:54 i said fine i'm going to start with the 08:57 models but i'm going to interject 08:59 errors into those models so they're 09:01 going to be not quite right 09:03 and then i'm going to compare the the 09:06 initially trained 09:07 model the machine learning model 09:10 to the true the true 09:13 behavior by adding points so i'll add 09:17 points to the original data set 09:19 one at a time until i converge 09:23 on that true solution okay so 09:26 using this k nearest neighbor approach i 09:29 trained it 09:30 from the original data that had errors 09:32 in it 09:34 and then now i have a mapping between 09:37 stable and unstable behavior in my model 09:40 so that's the gray zone there 09:42 the the blue curve is just saying that's 09:44 the true the true response that i don't 09:46 know yet 09:48 okay so now we start performing 09:50 experiments where 09:51 i update the points by tests 09:54 in this case at a five millimeter axial 09:57 depth of cut for the machining operation 09:59 so i update 10:01 in a smart way if i get a result i say 10:04 okay everything below that result 10:06 is stable if i get a positive or a 10:09 stable result 10:10 if i get an unstable result i say okay 10:13 everything above that result is unstable 10:15 so not only am i updating at the point 10:17 that i tested 10:18 but also surrounding points based on 10:20 what i know 10:21 as a machining dynamics person so then i 10:25 did it at different 10:26 axial depths and the k nearest neighbor 10:30 improves as i add these data points and 10:33 so you can see us walking through that 10:35 procedure and indeed 10:37 converging on the true behavior and so 10:40 this convergence criteria 10:41 i showed there's the number of correct 10:43 points relative to the number 10:45 of total points and so you can see that 10:47 that ratio improves 10:49 as we as we proceed with the testing 10:53 okay so i know that was quick but i just 10:55 wanted to give you a flavor 10:57 for how we can use models for 11:00 manufacturing 11:01 processes to see the machine learning 11:03 algorithm 11:04 and then update that algorithm with new 11:06 data so thank you and i'd welcome any 11:09 questions 11:11 thank you very much dr schmitz for the 11:14 great presentation 11:16 so now that we heard about the academic 11:18 side of 11:19 um manufacturing especially 11:22 talking about the physics based modeling 11:24 now we are going to the industry side 11:26 and our next next speaker um dr andrew 11:29 henderson will 11:31 um talk about the industry experience so 11:34 it's my great pleasure to 11:36 welcome dr andrew henderson to the 11:38 podium he's the cto 11:40 for primo incorporation he has over 15 11:43 years 11:44 of experience in advanced technology 11:46 data acquisition 11:48 data analysis and process and system 11:50 modeling and same as before 11:52 if i want to go over his accomplishments 11:55 he won't have his 10 minutes to talk 11:57 so i will stop here and welcome andrew 11:59 to the podium 12:01 thank you i i um i 12:04 should be sharing my screen now um let 12:07 me make it full screen 12:09 um so uh again thanks for thanks for 12:12 having me i 12:13 i i'm happy glad to be here i thought 12:16 maybe it'd be worthwhile to take just a 12:19 moment a brief moment in the beginning 12:20 to talk about who promo is 12:22 primo is a we have a product called 12:25 razer 12:25 that's uh an advanced analytics engine 12:29 that takes data from industrial 12:31 operations 12:32 and uh analyzes it to create these 12:35 notifications these things we call 12:36 insights 12:37 and those insights are are bits of 12:40 information that 12:42 operations people can go use to improve 12:44 productivity 12:45 and uh it accomplishes razer 12:47 accomplishes what it does 12:49 because we we leverage uh a bunch of 12:52 different techniques from the field of 12:54 artificial intelligence 12:55 and this of industry for industry is a 12:58 reflection of the fact that 13:00 uh all of our leaders come from industry 13:03 in some form manufacturing mining 13:05 and so we bring our experience to how 13:08 we develop razer and apply it in the 13:12 in industry and so i 13:15 i what i have is a few different 13:17 examples of how 13:19 uh andrew sorry to interrupt i think we 13:21 can't see your screen 13:22 so maybe you share it again please oh 13:25 did i 13:26 sorry i didn't do the final click i 13:29 apologize 13:31 can you see now yes perfect thank you so 13:35 um so so i have a few examples here 13:39 of of how uh various 13:43 uh assets or aspects of artificial 13:45 intelligence are applied 13:47 are applied to solve problems in 13:48 manufacturing 13:50 and uh there's an arc to the 13:52 presentation where i start out 13:53 i talk about consumer ai uh and then i 13:56 end up 13:57 talking about you know some of the 13:59 challenges that real world 14:00 in manufacturing faces and how we might 14:03 deal with them 14:04 so the first example here this is around 14:06 product quality this is 14:08 as as dr schmitz mentioned a moment ago 14:12 taking images and recognizing cats or 14:15 features or faces in the images being 14:18 able to classify 14:19 what's in them and so we can take those 14:21 exact same 14:23 approaches and from consumer ai 14:27 and more or less directly apply them to 14:30 manufacturing where 14:31 if you have an inspection station that's 14:34 that with a 14:34 with a camera that's taking images of a 14:36 product then you can feed those images 14:39 uh you can train a neural network to 14:41 recognize 14:42 whether the product is is has a defect 14:45 or not and may and the class of defect 14:48 and so what this requires is 14:51 a large data set of images and it 14:54 requires them to all be classified 14:57 uh in order to train that neural network 14:59 uh 15:00 and typically that that requires a 15:03 person in the loop to do that labeling 15:05 of those images 15:06 so that you can train it and then the 15:08 neural network is a is a black box we 15:10 don't often 15:11 know what's going on inside of the 15:13 neural network how what it does to make 15:15 it 15:15 what it's using to make its decision and 15:18 we'll 15:18 we'll talk about each of these as we go 15:20 along but this is this is 15:22 this is good though because what the 15:25 image classification 15:26 can do is it can offload some of that 15:28 work that a human might be doing 15:30 so that the human can go uh uh take care 15:34 of other 15:35 uh use their skills in other ways inside 15:37 of manufacturing or 15:42 so so they can use their skills in other 15:43 ways inside of manufacturing and then 15:45 um but this is at the end of the process 15:48 so this is after 15:49 something has been made and there's a 15:51 lag between when the product is made and 15:53 when the inspection occurs and so 15:55 oftentimes one of the first questions 15:57 that comes up 15:57 is well can you tell me sooner i'd like 16:00 to know because i don't 16:01 want to wait until uh 16:04 i've potentially made 5 10 20 more 16:07 products before i get the feedback from 16:09 inspection 16:10 and so we can take uh an almost 16:13 identical approach 16:15 and apply it to sensor data coming from 16:18 the the machine that's doing that's 16:20 conducting the operation so in this case 16:22 a stamping press we might be collecting 16:24 pressure temperature vibration etc 16:27 it and again because this is a 16:30 neural network approach we have to train 16:32 it we need to have 16:34 uh e event data from the machine 16:38 and we have to be able to have it 16:40 classified to say 16:42 whether that was that led to a defect or 16:44 not 16:45 and then the neural network can learn uh 16:48 to to recognize patterns in that data 16:52 that 16:53 will lead to a defect and so we've moved 16:56 that further up the process we we still 16:59 haven't 16:59 necessarily prevented a defect from 17:01 occurring but we 17:03 we will have uh note identified 17:07 as soon as the first one occurs that 17:09 there that that there has been a 17:12 an issue in the process so that's so 17:14 that you can stop then 17:15 and not not make not continue to make 17:19 more 17:20 and there are ways to to further 17:23 analyze the the signal in order to 17:26 save more time to be able to perhaps 17:29 stop a long-running process before uh 17:33 you've wasted before you've spent eight 17:36 nine hours perhaps making product that 17:38 you can't use 17:40 and there's also ways of looking at how 17:42 the how the signals are trending 17:44 over time and being able to be more 17:46 predictive but those are that's a 17:48 that's you know another conversation 17:52 so one of the as i mentioned a neural 17:54 network is a black box it doesn't really 17:56 tell us what's going on inside of it how 17:58 it's making its decisions 17:59 so that's always a question that people 18:02 want is 18:02 okay so you tell me that there's a 18:04 problem can you tell me why there's a 18:05 problem 18:06 uh there are ways of doing this one of 18:09 which that's 18:10 uh that's fairly common and robust is 18:13 using a uh 18:14 decision trees or more broadly a random 18:17 forest 18:18 and similar training right so you still 18:22 have to have 18:23 that that curated data set that's all 18:26 been 18:26 labeled so that you can put it in and 18:28 train so that you can train your random 18:30 forest 18:31 to uh be able to recognize those defects 18:35 but the random forest is a little 18:38 different in how it's structured and 18:40 built 18:40 and that each node there's a decision 18:42 point at each node 18:44 and it takes it takes uh a feature of a 18:47 signal 18:48 and depending on the level of that 18:51 feature 18:52 it decides which path to go down the 18:53 tree in order to make its decision 18:55 and because of that we can come back and 18:58 uh 18:58 take a look at what it's doing during 19:01 that decision making process to come to 19:04 the conclusion at the end 19:05 so this can help us understand what are 19:08 the most important factors leading to 19:11 the decision for a particular defect 19:14 and so that helps understand the root 19:17 cause of where it's coming from 19:19 and uh that can drive decisions that 19:21 people make around how to go 19:22 correct it so all of what i've talked 19:26 about so far has been supervised 19:27 learning you have that data set you have 19:29 the labels that you use 19:31 um to to train the model 19:34 oftentimes we don't have those labels we 19:37 just we have data 19:38 and um so then we have to look at 19:41 applying 19:42 unsupervised techniques so uh 19:45 things like what um the the clustering 19:48 the k 19:48 nearest neighbors clustering approach 19:50 would be uh 19:52 considered an unsupervised technique um 19:56 as as dr schmitz was talking about a 19:57 moment ago and so 19:59 what we what this example is showing is 20:01 there's a 20:02 there's a a piston that's pumping of 20:05 pumping fluid 20:06 at a station on a line in a 20:08 manufacturing process 20:10 and there's an accelerometer that's been 20:13 mounted on 20:14 that that device and the 20:17 the spikes in vibration represent events 20:20 and so we use signal processing 20:22 techniques in order to be able to 20:24 uh divide this long continuous data 20:27 stream 20:27 into those different events and then we 20:30 can apply clustering just like dr smith 20:32 was saying 20:33 to be able to group those different 20:35 events 20:36 into categories so that we can better 20:38 understand 20:40 uh what's what the content of our signal 20:42 is so there's 20:44 what comes out of it is that there's uh 20:46 this 20:47 this curve that uh we don't really know 20:50 what it is we don't 20:52 at this point we don't really care why 20:53 or we don't really care what it is 20:55 we just label it generically event a it 20:58 happens a bunch of times 20:59 there's another thing called call it 21:01 event b it happens a bunch of times in 21:03 the data set and then there's this thing 21:05 that at first glance it gets grouped 21:08 together we call it 21:09 and it's event c but then we can run 21:11 that same clustering 21:12 again on each of these groups to see if 21:15 there are subgroups and what we find is 21:17 that there's actually two 21:18 subgroups of of uh event c 21:21 and so with this we can start to make we 21:24 can start to look for weird behavior in 21:26 the system so 21:27 so um in that event c 21:30 we can build an expectation based off of 21:33 what's the most commonly occurring 21:35 wave form for that particular event and 21:37 we'll 21:38 we'll call that our expectation and then 21:40 anything that doesn't 21:42 match to a degree with that expectation 21:45 we'll say that's 21:46 that's an anomaly that's something 21:47 different and 21:49 by tracking and and the the net result 21:52 of all this is that by tracking 21:53 those odd ones those those those 21:57 unexpected events and looking at how 22:00 frequently they're occurring and what's 22:01 the percentage that they're occurring 22:02 within a window of time 22:04 we can see this is this is showing that 22:06 so the the percentage 22:08 of those anomalous events uh within the 22:12 the subset we can see a a rise 22:15 uh at a point in time and 22:19 this this drop represents a period in 22:22 time in which the 22:23 the process stopped so uh and 22:27 we the reason we say this is 22:29 semi-supervised is because what happens 22:31 next we get the feedback that says 22:34 yeah that the line stops because there 22:36 is a 22:37 the the an incorrect fluid was put into 22:39 the system 22:41 and it happened roughly 24 hours before 22:45 the the line stops so we can see that 22:47 just by 22:48 taking this sort of naive approach of 22:50 identifying the anomalies 22:52 within those within that cluster of 22:54 signals uh 22:55 we can see a rise that gives us an 22:57 indication that something 22:59 is different about how that operation is 23:01 running and so then we can create an 23:03 alert 23:03 the alert doesn't necessarily say what 23:06 the problem is 23:08 and why but it does say hey there's 23:10 something uniquely different here that 23:11 people should be paying attention 23:13 perhaps even go take a look and we can 23:15 extend this 23:16 this semi-supervised approach even 23:19 further 23:19 to apply some more human knowledge about 23:22 the system to say 23:24 the the different features of these 23:26 curves represent different 23:27 aspects of the process and we can even 23:30 say 23:30 that you know the perhaps what's driving 23:34 the anomaly 23:35 condition is that this this piston 23:38 retracting the vibration is low during 23:40 that and that could 23:41 could uh indicate to the maintenance 23:44 people 23:45 what to go look at and it gives them a 23:47 better idea of what might be the problem 23:49 and what to fix 23:51 and so the the key takeaways of all of 23:53 this is to say 23:54 this these examples that i'm showing 23:56 we're only scratching the surface 23:57 there's so many different ways that we 23:59 can continue going and exploring and 24:01 extracting value 24:02 out of by using artificial intelligence 24:05 to analyze the data 24:06 and also there's no need to wait to get 24:09 started meaning 24:10 meaning each one of these came from data 24:13 sets that people 24:14 had within their operations and so you 24:17 can you can use those data sets and 24:19 and begin to get value so 24:22 that is that is it for me 24:25 thank you all for your time thank you 24:27 very much uh andrew for 24:29 the presentation um so now we are moving 24:32 on to the 24:34 next talk uh by kurt goodwin um kurt 24:37 is a humane mechanical engineering alum 24:39 and 24:40 he has over 40 years engineering 24:42 experience in 24:43 introducing and also developing new 24:46 technologies for jet engines gas and 24:49 wind turbines he has served as general 24:51 manager for advanced manufacturing and 24:54 now 24:54 although he is semi-retired but he's 24:56 still consulting with new manufacturing 24:58 and startups like beehive 25:00 3d so carrot take it away 25:04 okay and i think i'm sharing hopefully 25:07 you guys see some big engine blocks on a 25:11 yes perfect so i was about a far from uh 25:15 an artificial intelligence expert as 25:17 there is 25:18 a mechanical engineer i spent most of my 25:21 just as 25:22 as ali just said uh spent most of my 25:24 career 25:25 uh trying to help with the adoption of 25:26 new technology um 25:28 so i'm going to address sort of a people 25:31 aspect a little bit of 25:33 of how that does and and some ideas that 25:36 hopefully will help 25:38 help those of you that have something to 25:40 offer to to work with the 25:42 people more successfully a big piece of 25:44 my job has been 25:47 trying to help not just ai but different 25:51 digital 25:52 folks to understand manufacturing shops 25:55 and what drives them 25:57 early on we noticed that you know the 25:59 most successful groups in this area 26:02 had grown out of manufacturing 26:04 backgrounds or at least the teams 26:06 included large 26:07 numbers of people that had manufacturing 26:09 experience 26:11 because they understood their customers 26:14 um 26:14 and what their needs and the language 26:16 and drivers in a way that 26:18 you know somebody who's mostly done 26:21 software might not 26:24 i think it's interesting you notice both 26:25 tony and andy have that 26:27 experience themselves most factories if 26:31 you don't know this 26:32 are driven by fulfillment first 26:35 and second and to some extent uh 26:38 driven by cost it's a very tough 26:41 environment they're basically driven to 26:43 deliver 26:44 a product whether it's cars or medical 26:47 devices 26:48 or turbines or engines or whatever 26:51 every they're measured every week every 26:53 month every quarter 26:54 it's it's it's it's a tough 26:58 it's a tough uh business to be in 27:01 tech companies um come in and they might 27:05 be selling machine 27:06 monitoring parts flow better controllers 27:09 people who come in they do the 27:12 installations and then they fly home 27:13 friday morning sometimes 27:16 almost inevitably something goes wrong 27:19 the engineers and the workers in the 27:21 cell try and fix it 27:23 and if the tech company is there or 27:26 representative 27:27 things go well if if they're not there 27:29 they start trying to figure out how to 27:31 work around the glitch 27:33 um sometimes the outside helpers don't 27:36 even make it back the next week 27:38 and that's that that's the end of 27:40 cooperation 27:42 at the point where you're not able to 27:45 make product 27:46 and the people that are trying to help 27:48 you aren't there to help you 27:50 you've lost them forever that they're 27:52 not going to want to 27:53 work with you again 27:56 those companies that are successful 27:59 they know how to become part of the team 28:01 they understand that there are time 28:03 pressures 28:04 value being there when it's needed to 28:06 preserve shipment they've been stuck 28:08 doing 100 hour weeks themselves 28:11 um and so they they understand their 28:14 customers somewhat 28:16 the thing that you see over and over 28:18 from the most 28:20 successful people at doing this 28:22 regardless of the background 28:24 is they start out by talking to the guys 28:26 on the floor 28:28 and and working a shift with them they 28:30 don't try and hook everything up at once 28:32 if something does go wrong they ride 28:34 through it with them 28:36 um and basically 28:39 they they become they do everything they 28:42 can to put themselves in the 28:44 in the shoes of the people that are 28:47 working in the factory 28:49 so now whenever we work with startup 28:51 manufacturers like 28:53 beehive 3 additive that is mentioned 28:56 here 28:56 we try and start with a mix of people 29:00 that have those different backgrounds 29:02 manufacturing people that that have had 29:06 a lot of technology and and 29:10 digital experience and digital and tech 29:13 people that have worked on 29:14 on the factory doesn't have to be 29:16 anybody but if you can't 29:18 communicate between yourselves you can't 29:21 appreciate what's uh what's being 29:23 offered 29:24 it sounds simple right it doesn't sound 29:27 like this is any particular 29:29 revelation um but i've seen this 29:32 absolutely make and break a tech startup 29:35 or 29:36 a digital offering or a company that's 29:39 that's trying to get out there 29:41 um so what else um 29:45 if you can get past that startup 29:47 challenge if you can get the 29:48 relationship started 29:50 i think the next thing that's very 29:51 helpful is to try and 29:53 learn to think how to think about ai 29:57 um ginny rowdy who used to run ibm 30:01 had a comment that ai should stand for 30:03 augmented intelligence 30:05 the idea should not be that you're just 30:08 handing control over to an autopilot 30:10 which is kind of the way 30:12 some people describe some of this stuff 30:15 but rather that you have another set of 30:17 eyes and brains 30:19 on the floor to try and help you 30:20 understand what's going on 30:22 you know i think i think andy's point 30:25 is very similar to this 30:30 you don't necessarily know what you're 30:32 looking for to begin with 30:34 you try and collect the data that you 30:36 can 30:37 and and then think of it as more getting 30:40 help 30:41 noticing clues that might get missed 30:43 otherwise 30:44 so in in my experience many projects 30:47 start out with a very specific set of 30:50 instructions or goals to attack a very 30:53 specific perceived problem 30:55 you know one example is we're trying to 30:57 catch machine downtime 30:59 so we can get more throughput and up up 31:02 front there's an assumption or an 31:04 accepted idea that productivity is lost 31:07 because the machines are down 31:09 uh being fixed too much i had a great 31:12 example of this once we had a shop that 31:15 had 31:15 had to handle a big opportunity for 31:17 growth 31:18 in sales if they could only ship more 31:20 engines um 31:22 you put monitoring on a lot of machines 31:24 the data collections on times and starts 31:27 watch for 31:28 vibrations and events and oil 31:31 temperatures and so forth the first 31:33 the first breakthrough that came was 31:37 once we started mapping everything and 31:40 started understanding it 31:42 we discovered that a number of the basic 31:44 assumptions were 31:45 wrong you know so for example there was 31:48 a piece of equipment that gated 31:49 production 31:50 with very large engine block washer long 31:53 operation every 31:56 it had to go through a single machine 31:57 and every product had to go through it 32:00 on the good side of it um it didn't 32:04 break down very often 32:07 so you know the things like oil 32:09 temperatures and vibrations that we were 32:10 working 32:11 watching for didn't didn't turn to be 32:14 immediately useful but collecting the 32:18 information 32:19 um mapping times and and 32:22 flow through the shop um 32:26 you it still didn't deliver as much as 32:29 we 32:30 wanted from it so the data analysis 32:33 that that flagged that specific machine 32:36 it it one of the things that it was 32:38 noted was 32:39 that the it was often late starting 32:42 specifically during time periods around 32:45 the end of the morning or early 32:47 afternoon 32:48 so not not that it was breaking or 32:50 running right but for 32:52 some reason it would not be running 32:55 and consistently a certain set of times 32:59 it turned out to be a very simple 33:01 non-technical thing 33:03 that basically when the daily parts 33:05 delivery 33:06 truck came from the main factory the 33:09 operator 33:10 for that big machine that was needed 33:12 would 33:13 try and be a good guy and jump in and 33:14 help the crew unload it 33:16 so if the machine was already running it 33:18 was great it was no problem everything 33:20 kept 33:20 going we got flow beautifully 33:24 if not the start had to wait until the 33:26 operator finished unloading the truck 33:28 came back and got things started 33:30 the corrective action was about as 33:32 simple as it gets it was basically 33:34 hey george make sure the washer is 33:36 running before you do anything 33:38 or leave your station now you can argue 33:42 do you need ai to find that it's kind of 33:46 an irrelevant argument to me 33:49 because maybe you don't need it 33:52 but it had gone unnoticed before and the 33:55 data call 33:57 called attention to a specific time and 33:59 period and machine to investigate 34:01 and that's where i get the extra set of 34:04 eyes 34:04 helps speed up their the realization of 34:07 what you're looking at it 34:09 the other key thing to that is that's a 34:12 win 34:13 you need to celebrate it the first 34:15 reaction 34:16 when we found that basically we were 34:18 losing time because somebody was 34:20 unloading a truck 34:22 everybody wants to chastise somebody 34:24 else you know 34:25 you know the cell leader you shouldn't 34:27 have known this was a problem 34:29 the shop leader gets very defensive that 34:32 you know it seems like his uh 34:34 um his shop is out of control the 34:37 operators why did you let this happen 34:39 you should have needed a problem you 34:41 have to not let that 34:42 happen because if you don't recognize 34:47 that you've found an opportunity to make 34:49 things better 34:50 and encourage people to to 34:53 fix it work through it um 34:56 you know then you're you're not going to 34:58 get the feedback you need and people 35:00 will 35:02 actually not fudge the data but they 35:04 will 35:05 they will try and interfere with the 35:06 collection of data to be successful 35:10 so you know i think 35:13 if you can find the right ways to 35:15 encourage those kind of learnings and 35:17 applications 35:18 and and for everybody to be flexible 35:23 don't look at aei as a competitor or 35:25 something that's trying to take my job 35:27 but it's somebody else on the floor to 35:29 help me understand what's going on 35:31 and every time we get an improvement 35:34 it's a win 35:35 all that stuff will help 35:38 anybody on either side be more 35:40 successful 35:42 which is always the goal so that's it 35:45 thank you thank you very much kurt um 35:48 so now that you heard about academic 35:51 side and industry side 35:54 our last presentation is updating you on 35:57 what's going on in 35:58 national labs so it's my great pleasure 36:00 to introduce dr vincent parker a senior 36:02 research scientist in 36:04 electrical and electronic system 36:06 research division at oak ridge national 36:08 lab 36:09 uh his research is focused on computer 36:12 vision and 36:12 image processing with the periodic 36:15 election for 36:16 high performance image processing 36:18 algorithm development so 36:21 dr vincent please thank you thank you 36:24 very much so in the in this presentation 36:26 i'm going to cover a lot of the work we 36:27 do at the manufacturing demonstration 36:29 facility 36:30 i'm going to discuss the data we are 36:33 collecting 36:33 in the within the facility and the use 36:36 of ai 36:37 to process such data in order to answer 36:39 some of the scientific problems that 36:40 that we have in 36:41 in front of you of us so uh just uh on 36:44 my first slide 36:45 uh i will highlight the the link at the 36:47 bottom i don't know how i'm gonna be 36:48 able to share that with 36:49 with you all but there is a possibility 36:52 to have a virtual tour of the facility 36:53 where you'll be able to see 36:55 over a hundred thousand square feet uh 36:57 building 36:58 about 200 type of system that we that we 37:01 are working with 37:02 so for me as a data scientist this is a 37:04 fantastic playground because i get to 37:06 use to 37:07 play with machines of different types 37:10 and sizes and see how data-driven 37:13 methodologies can be used 37:15 in order to improve the systems or 37:18 assess the quality of the component 37:20 coming out of those 37:22 of the systems so that was 37:25 slide is not going to the next okay here 37:26 we go um so when when you work with this 37:29 type of of machine in the facility 37:31 you have example on the left-hand side 37:33 of the great thing that you can produce 37:35 with them 37:36 uh they look great this component this 37:38 car looks great 37:40 the the chevy cover looks great but at 37:41 the end of the day it's not necessarily 37:43 functional 37:44 and the problem with that is uh when 37:46 you're looking at critical components 37:49 you don't necessarily have a way right 37:51 now 37:52 to tell what's coming out of the machine 37:54 is actually of great quality without 37:56 doing 37:57 any kind of non-destructive evaluation 37:59 of really expensive 38:01 testing in order to validate the 38:03 component which 38:04 in the at the end of the day uh kills 38:06 the business case for 38:08 for additive altogether and and so our 38:11 interest here is to see 38:13 if there is any mechanism using data 38:16 to get a better understanding of the 38:18 process so we can develop 38:20 certification methodologies for those 38:22 components but ultimately come up with a 38:24 way to accelerate production of those 38:26 components and improve the manufacturing 38:28 technologies all together 38:29 so we are taking a a traditional smart 38:32 manufacturing approach where you try to 38:33 understand the process optimize it 38:35 eventually implement feedback loop 38:37 control mechanism if you can correct 38:38 your process on the fly 38:39 and ultimately that will lead to a 38:41 scenario where you'll know so much about 38:43 your process and you control your 38:45 process 38:46 so well that you will be able to tell 38:48 this component coming out of my machine 38:50 i don't need to test it because i know 38:52 so much about it that i can say 38:53 it's it's actually a good component so 38:56 in order to do that 38:58 you need to uh have a lot of data and 39:00 and that's really where 39:02 our our wheel hours here uh is so we 39:05 want to make sure that we can collect 39:07 information at any given step of the 39:09 manufacturing process 39:10 this slide is going to get extremely 39:11 busy in a second i don't want you to 39:14 try to dissect everything but just 39:17 it's here to give you an idea of the 39:19 type of information we are interested in 39:21 collecting so if you have a goal for 39:22 example to produce an n95 mass which is 39:25 something that we did 39:27 you're going to be looking at a 39:29 different type of design 39:33 excuse me modeling and simulation for 39:35 past planning 39:37 before you send this to the printer as 39:39 with the same time as the as the 39:40 feedstock 39:41 and every time you're going through this 39:43 this chain you're going to be collecting 39:44 information about the the printer itself 39:47 instrumenting the printer to look at 39:48 what's happening inside it doing data 39:50 registration and anomaly detection 39:52 in order to analyze this data and then 39:55 you have your 39:56 first component printed you're going to 39:57 chop it into pieces 39:59 and go through subsequent steps of 40:00 post-processing 40:02 testing and so on in order to create 40:04 what we call a digital clone of the 40:06 physical component 40:07 so you're going to have at this point 40:08 the entire history of your component 40:10 contained in a data package 40:12 and you will be able to use this data 40:14 package for visualization purposes or to 40:16 feed a larger database 40:18 that as it grows will help you 40:20 understand better what's going on 40:21 in in your manufacturing process when 40:23 you do inter builder intra build uh 40:26 machine learning you're gonna be able to 40:28 then go back 40:29 and say okay now i can i can start 40:32 predicting the performance of my 40:33 component because i've learned so much 40:34 about my process 40:36 but also act on the design itself and 40:39 help uh some of the the the cad software 40:42 to produce 40:43 uh design that are also optimized for um 40:46 with material science knowledge in mind 40:49 so 40:51 in order to get there you don't want to 40:53 reproduce this for every single system 40:55 so you need to come up with a unified 40:57 data architecture that will help you 40:58 collect such information 41:00 and the way we see this is to look at a 41:02 component 41:03 as a massive building block set and what 41:05 you're doing really with the machine 41:07 is to tell the the system grab this 41:10 block and 41:11 of this particular color and put it at 41:13 this particular location in space 41:15 when you do this you have data that 41:17 tells the machine 41:18 or you have you have processes that tell 41:20 the machine how to do this 41:21 but you can also collect the data on the 41:23 system to know how the machine actually 41:25 perform 41:25 and so that's coming from the different 41:27 data producers let me get a laser 41:29 pointer here 41:32 for some reason i can't that's 41:35 interesting 41:37 looks like they've changed the system i 41:39 don't know um so 41:41 um you you're going to have different uh 41:43 uh data producer you're going to be 41:44 collecting this data and each 41:46 uh each data producer will provide you 41:48 one value or multiple values that can 41:50 then store for each xyz location 41:52 so now you have a feature vector of 41:54 information that describes each element 41:55 in space 41:56 which is a fantastic scenario for any 41:59 kind of machine learning type of 42:01 of applications so when you have all 42:03 these data packaged together 42:05 you can do anything that's listed on the 42:06 right-hand side of this line 42:08 and so with that i'm going to go through 42:10 some of the examples on how you can use 42:12 this data 42:13 so first and that was touched on by uh 42:16 dr anderson 42:17 um uh you can uh observe what's 42:20 happening inside your powderbed system 42:22 so if you have for example 42:23 an image like this you're gonna be able 42:25 to see what's what's 42:26 uh what's happening you can see certain 42:28 type of features and what you want to do 42:30 is classify those voxels 42:32 or pixels in this particular case 42:35 for to identify the type of of classes 42:39 they belong to 42:39 so that's kind of the first phrase that 42:41 we had we moved on to 42:42 something a lot more advanced where we 42:45 train a unit in this particular case 42:47 to take a stack of of of images of from 42:51 multiple modalities train the model and 42:54 then the model spits out a 42:56 map of all the the the defects 42:59 that you are defects or features that 43:01 you are interested in detecting 43:02 so when you scale that up to the size of 43:04 the component you can render in 43:06 in 3d an entire map of all the features 43:09 that are present in 43:10 in this particular component and you can 43:13 then help 43:14 operators of the machine uh see things 43:17 that are 43:18 happening when they are printing their 43:20 their parts and see if they can 43:22 modify the process in order to get 43:24 better results 43:26 the thing that's interesting with this 43:27 and that goes along the the comment that 43:28 we 43:29 made before uh this gentleman in front 43:32 of the computer here is 43:33 he's an operator of a machine he has no 43:35 computer science background 43:37 but you can provide them too that they 43:39 can help them 43:42 become better operator of the system 43:43 it's not again 43:45 to replace the operator of the system 43:46 it's having a computer helping you 43:49 being better at you at your job and so 43:51 he's training his own models with the 43:53 platform we put in place so that's a 43:54 that's a nice way to 43:56 use ai in this in this particular case 43:59 a direct example or a direct use oops 44:02 two slides a direct use of this 44:04 particular 44:06 type of of models is you can start 44:09 looking at 44:10 automating correction on the machine so 44:12 for example on the binder jet system 44:13 like this one 44:14 we use exactly the same techniques 44:16 putting cameras to 44:17 get different modalities of the sensor 44:20 of the 44:20 of the process uh classified the data to 44:23 get 44:23 in green the part and in in purple it's 44:26 incomplete spreading 44:27 that's a defect that's fairly easy to 44:29 engineer uh on the on the machine 44:31 so in this particular example here what 44:34 you have is 44:34 in the x-axis the number of the layer 44:37 number 44:38 you're going up as you're going from 44:41 left to right 44:41 and here you have the percentage of 44:43 pixels that we are uh 44:44 of a given color so in this particular 44:47 case what we did we we forced the 44:48 printer to create an 44:50 incomplete spreading so you have a 44:52 percentage of of pixel that increases 44:55 roughly from two percent at the 44:56 beginning to a quarter of the image was 44:58 covered with 44:59 with purple pixels at this point we turn 45:01 on the 45:02 switch and say okay now it's ends off 45:04 and we're going to let the ai takes over 45:06 and change the process parameters in 45:08 order to go down 45:09 and remove this particular defect and 45:11 and you can see the curve is going down 45:13 to a level that is actually lower than 45:15 where it started so you can use ai 45:18 for some of those uh particular defects 45:21 and make sure that you don't have an 45:22 operator in front of the machine 45:24 at all time in order to correct for for 45:26 some of the 45:27 of those problems that are actually 45:29 fairly straightforward and easy to 45:32 uh to correct if you cannot implement ai 45:34 for this type of correction you can 45:36 however send messages to operator of the 45:39 system make sure that they 45:40 they see this another place where we use 45:43 ai is on ct reconstruction 45:45 so the advantage of of additive is that 45:48 you know 45:49 the the the overall shape of your 45:51 component and you can use that at your 45:53 advantage in order to help with ctrl 45:55 construction of these samples 45:57 so if you use a traditional ctr 45:59 construction algorithm this is an 46:01 example of what you're going to get 46:03 but we've developed a technique that's 46:05 that's mixing 46:06 prior knowledge or design knowledge of 46:09 the component 46:10 and some data that we've collected 46:14 across multiple builds in order to train 46:16 a model that will uh 46:18 just give you a overall better 46:20 reconstruction of your component with 46:22 less noise air and more defined 46:24 uh defects detected within the the 46:27 geometries 46:28 those are actual two exact uh um 46:32 reconstruction example this is a 46:34 traditional reconstruction and this is 46:36 what we're getting out of our of our 46:39 models um one of the thing that we are 46:43 also interested in doing is 46:45 is pushing the machine to do things that 46:46 are not supposed to do 46:49 so if you look at a design like this and 46:51 you print it in a particular 46:53 system in this particular case it's an 46:55 electron beam machine from arkham 46:57 if you print pencil bar at the bottom 46:58 and at the top and you use the black box 47:00 of the machine that's provided by the 47:02 manufacturer 47:03 it will print but it's not going to 47:04 produce the same component they're going 47:06 to look the same 47:07 they are not going to perform the same 47:08 if you take micrographs out of those 47:11 cylinders they are circled with the same 47:13 color here you see two different type of 47:15 texture which is well known 47:17 is directly uh uh gonna be correlated to 47:20 the type of mechanical test you're gonna 47:21 get so you're you're seeing two 47:23 different type of clusters 47:24 not same results however you've 47:26 collected enough data to 47:28 learn different type of patterns that 47:30 will lead to the production of certain 47:32 type of microstructure growth or other 47:33 type of microstructure groups 47:35 and so you can use this in order to 47:37 fine-tune the 47:39 process parameters and apply a 47:41 particular type of 47:42 a manufacturing process depending on the 47:45 cross-section of your geometry that you 47:46 are 47:47 so that's something that that oops sorry 47:50 i'm going to come back okay so that's 47:53 something that that we did here 47:54 and those examples here is printing 47:57 again the same geometry we pulled again 47:59 micrographs from from tensorboard the 48:01 bottom and the top 48:02 and you can see the microstructure are a 48:04 lot more similar 48:05 and the uh mechanical tests are 48:08 actually clustered together so this 48:11 um this is how you take control of your 48:14 manufacturing process so now it's not 48:16 random anymore 48:17 it's not you're not at the mercy of the 48:19 decision of the engineer of the 48:21 that that put together the machine or 48:23 the programmer that put together a 48:25 software that runs the machine 48:26 you're already in control of what you 48:27 want to get out of the system 48:29 and when you have this level of 48:31 understanding you i don't even need to 48:33 to test those samples anymore because i 48:34 know what i'm going to get 48:36 direct application of this we've used 48:39 this type of approach and it's been 48:41 accepted 48:42 by by industry as ai has been accepted 48:45 by industry in this particular case 48:47 to validate some of the components that 48:49 were produced so we have two examples 48:50 here one with solar turbines 48:52 where we printed over 200 uh turbine 48:55 blades 48:56 and use the the the the tools that i've 49:00 highlighted before 49:01 in order to identify which blades were 49:03 of the highest quality 49:05 80 of them went to a stress test and 49:08 then on the hot fire tests 49:09 on august 25th and they perform as as 49:13 expected as well as traditionally uh 49:15 manufacturing of component 49:16 another case is something related to a 49:19 large program that we have at the 49:21 at the lab which is the transformational 49:24 challenge reactor where we are working 49:25 on 49:29 we are working on on printing uh 49:31 components 49:32 uh for nuclear type of applications we 49:35 had 49:35 uh as part of this program a 49:37 collaboration with 49:38 framatum and the tennessee valley 49:41 authority 49:42 to print component that will go into a 49:44 commercial reactor and you have a 49:45 picture of them 49:47 and they were they went through the same 49:49 type of of 49:51 tests i or evaluation i mentioned uh 49:53 before 49:54 they went to however traditional um 49:58 nd testing in order to make sure that 50:00 what we said was actually correct 50:02 and they were approved and they went 50:03 into the the 50:05 the commercial reactor at the end of 50:07 last year 50:09 so what's next for a manufacturing data 50:12 science and probably 50:13 more in particular for uh in terms of of 50:16 ai 50:16 they i mean kind of mentioned that uh 50:19 earlier 50:20 on on the material inform generative 50:23 design so 50:24 we we do have generative design type of 50:27 algorithm right now 50:28 that are great to simplify or change the 50:31 way we we design uh components 50:33 but they are not necessarily including 50:35 enough of the material information that 50:36 we can we can collect 50:38 and so that's something that we're 50:39 interested in in pushing the augmented 50:41 intelligence portion again uh the 50:44 the next generation of the of 50:46 manufacturing uh 50:48 uh operators uh will leave with a 50:51 computer 50:52 alongside them and so we need to have a 50:55 system that can help them do 50:57 uh what they what they do best i'm not 50:59 going to go in detail through all of 51:01 this the one i will 51:02 highlight that is more related to the 51:05 control of a microstructure 51:07 is the full optimization of what you are 51:09 actually doing 51:10 and making sure that you you engineer 51:13 your manufacturing material properties 51:15 in space and not solely manufacturing 51:17 components 51:17 and with that i'm at the end of my 51:19 presentation and i will welcome 51:21 questions thank you very much 51:25 vincent um so i would like to thank all 51:27 the speakers again 51:28 for the great presentations and now we 51:31 are 51:32 moving into the question and answer 51:34 period so um 51:36 thank you everybody for posting the 51:38 questions uh there so i will start with 51:40 the first question 51:42 for tony so what is the biggest 51:44 challenge that you see for implementing 51:47 ai into manufacturing domain 51:50 well if we are using a classification 51:53 approach 51:54 you'd like to have an automated 51:56 technique 51:57 for classifying that data right 51:59 otherwise you've sort of defeated the 52:00 purpose if i have to look at every 52:02 signal 52:03 and decide what happened so i think 52:06 that would be a an obstacle 52:11 for widespread implementation and that 52:14 you know 52:15 i tell you what's interesting is that 52:16 you bring in the domain 52:18 experts with the machine learning 52:20 experts and i think that collaboration 52:22 is essential 52:23 great thank you very much and next 52:26 question 52:27 is um for vincent 52:30 um what would be a possible approach 52:33 when the system 52:34 don't have a well-defined physics-based 52:37 model so if there is too many unknowns 52:39 for example in the case of 3d printing 52:42 we've seen in the in in the past that a 52:45 lot of the 52:46 physics based model for some of the the 52:48 technologies are not 52:50 um are overly complex and not 52:53 necessarily correct 52:54 uh at the end of the day and so the way 52:56 we approach this so for example for the 52:58 microstructure control i mentioned 53:00 we we've used high physics based models 53:03 uh to get there but really realized that 53:07 it was better to go 53:08 through a in-situ monitoring approach to 53:12 better understand what was happening for 53:14 a variety of combinations 53:17 of the um of the 53:20 of the manufacturing process and work 53:22 with lower order models in order to 53:25 get a an answer 53:29 they are like a lot of those models 53:32 that that seem to be right uh but 53:36 when you apply them at large scale first 53:39 sometimes you can't 53:40 because you can you cannot compute uh uh 53:43 the the the result for for a large 53:45 component 53:46 uh and and sometimes they are overly 53:48 complex it's not necessary 53:50 so good do finding a good balance 53:53 between 53:55 what sensor will provide you and what 53:57 models 53:59 rightly uh selected and applied to 54:02 sub region within your within your 54:04 geometry is probably a better approach 54:07 for for most systems thank you very much 54:11 and next question i'm going to ask 54:13 andrew um 54:15 so we talked about neural network you 54:17 know and different approaches so 54:19 in terms of like being in industry what 54:22 kind of ai or machine learning tools 54:25 um is mostly used in industry and how do 54:28 you decide which one of those 54:30 is appropriate for your application so 54:32 the uh 54:34 in terms of what's most common i mean i 54:36 i i'd probably have people arguing with 54:38 me about linear regression being an ai 54:40 tool but i 54:41 i would uh it's a way of of defining a 54:44 model of something so 54:45 i i mean that one's been there for a 54:47 long time uh but in terms of like what 54:49 we consider 54:50 advanced ai i think we're seeing a lot 54:52 more neural networks come up there are a 54:54 lot of people 54:55 asking for that use case at the very 54:57 beginning where i've got images of 54:58 products i want to classify if they're 55:00 defects or not 55:01 um beyond that i mean 55:05 they they all have their different um 55:08 their different 55:09 use cases unsupervised techniques 55:11 because you often 55:12 don't know what you don't know so let's 55:15 go 55:16 do some signal processing and then let's 55:19 group them together 55:20 and then review those results and though 55:23 then you have aha moments where you say 55:25 oh well i yeah of course that makes 55:27 sense to me that 55:28 that those things would be grouped 55:30 together or 55:31 um or the there's a 55:34 there's some press they're using like a 55:37 markov chain or some sort of thing you 55:39 might be able to determine precedence of 55:41 events or 55:42 the the sequence of events and says of 55:43 course now it all makes sense that that 55:46 those things happen 55:47 in that order so i don't i don't know 55:49 that that's a 55:50 great answer to what's the most 55:51 prevalent but it's to say that there's a 55:54 lot 55:54 of different techniques that people are 55:56 applying 55:57 that's great thank you very much andy 55:59 and the next question i'm going to ask 56:01 kurt um 56:02 you mentioned about um the other way of 56:05 looking at ai 56:06 as instead of saying artificial 56:08 intelligence we talked about it as 56:10 augmented intelligence right 56:12 and i got a comment from one of our 56:14 attendees dr terry you 56:16 mentioning that in 1994 acm 56:19 and newell award acceptance speech 56:21 frederick brooks also mentioned 56:23 something very similar 56:24 and called ai as ia or 56:27 intelligence amplification so 56:30 the question is that since you have a 56:32 lot of experience in industry 56:35 where do you think the industry will 56:37 benefit most from incorporating ai 56:41 and not just from technological point of 56:42 view from the acceptance point of view 56:45 from the engineers who are on the floor 56:47 so they don't feel 56:48 they're losing their jaws but right they 56:50 see that there's somebody helping them 56:52 right i think that's one of the reasons 56:54 that i like 56:56 the augmented uh um intelligence idea i 57:00 saw that comma i thought that was great 57:02 i 57:02 i stole it for another day um i 57:05 i think anything 57:09 that overcomes the initial 57:12 doubt is helpful there's there's a 57:15 and and i think it's been unfortunately 57:20 provoked to some extent by a lot of 57:22 discussion in the in print and in the 57:23 media 57:24 where people seem to want to get people 57:28 afraid of robots 57:29 i i think once people 57:32 [Music] 57:33 working in any shop run into a success 57:37 and start to see the opportunities for 57:41 it 57:43 as being help and not competition 57:48 it that that overcomes any 57:51 doubt or sales pitch better than than 57:55 you know anything you can say so we 57:58 you know i have a colleague who likes to 57:59 talk about getting base hits 58:01 um you don't have to solve world hunger 58:04 the first time out the first time 58:06 you uh you know they work with somebody 58:08 like tony and he helps them 58:10 not break tools off anymore because 58:12 they're driving the machine too hard 58:15 or uh i know andy's had some great use 58:19 cases on 58:20 on recognizing unrecognized limits 58:24 those breakthroughs do more to win 58:27 people over 58:28 than all the talk you can imagine 58:31 um but i think starting out by even the 58:34 best way to start out is to just 58:37 just to say well look these are not here 58:39 to replace you 58:41 these are here to help us find problems 58:44 and fix them 58:45 and get through it and then look for 58:48 that chance to show everybody 58:50 that's the best thing i know thank you 58:53 very much kurt um 58:54 we are at time so i just want to mention 58:56 that um i would like to thank again all 58:58 the speakers all of you attend this for 59:00 attending this event if you liked 59:02 this event ai for manufacturing you can 59:05 join us 59:06 on the first thursday of every month in 59:08 april we have ai for agriculture 59:10 in may we have ai for health care and in 59:13 june we are going to present multiple 59:15 projects which are funded by the umen 59:17 aic grant 59:18 i also want to thank our sponsor office 59:20 of vice president for research 59:22 sponsoring university of maine ai 59:24 initiative 59:25 and also my colleagues at the ais 59:28 student committee 59:28 doctors susan mckay terry you roy turner 59:32 sharmila mohapadi charlene jen saul 59:34 allen and jason sharland 59:36 and also in the background i would like 59:39 to thank 59:40 office of research help that we got 59:44 melinda pelletier who is actually 59:46 running the background zoom here for us 59:49 um i know we have a few more questions 59:51 but we are out of time so we will 59:53 answer those offline and again don't 59:57 forget to 59:59 respond to the survey requests we'll 60:01 send out later so 60:02 hopefully we'll make these events better 60:04 so thanks again all the speakers and 60:06 attendees and enjoy the rest of your day 60:08 have a great day    The University of Maine in Orono is the flagship campus of the University of Maine System, where efforts toward racial equity are ongoing, as is the commitment to facing a complicated and not always just institutional history. The University recognizes that it is located on Marsh Island in the homeland of the Penobscot nation, where issues of water and its territorial rights, and encroachment upon sacred sites, are ongoing. Penobscot homeland is connected to the other Wabanaki Tribal Nations \u2014 the Passamaquoddy, Maliseet, and Micmac \u2014 through kinship, alliances, and diplomacy. The university also recognizes that the Penobscot Nation and the other Wabanaki Tribal Nations are distinct, sovereign, legal and political entities with their own powers of self-governance and self-determination. ",
        "title": "Manufacturing and Materials",
        "citationCount": 0
    },
    {
        "content": "The University of Maine DigitalCommons@UMaine General University of Maine Publications University of Maine Publications 4-12-2021 Artificial Intelligence for Agriculture University of Maine Artificial Intelligence Initiative Follow this and additional works at: https://digitalcommons.library.umaine.edu/univ_publications  Part of the Artificial Intelligence and Robotics Commons, Higher Education Commons, and the History Commons This Video Recording is brought to you for free and open access by DigitalCommons@UMaine. It has been accepted for inclusion in General University of Maine Publications by an authorized administrator of DigitalCommons@UMaine. For more information, please contact um.library.technical.services@maine.edu.   UMaine Artificial Intelligence: AI for Agriculture  Date: April 12, 2021 Run Time: 00:57:20 https://youtu.be/XXoL2zwnRoU  UMaine AI draws top talent and leverages a distinctive set of capabilities from the University of Maine and other collaborating institutions from across Maine and beyond, while it also recruits world-class talent from across the nation and the world. It is centered at the University of Maine, leveraging the university\u2019s strengths across disciplines, including computing and information sciences, engineering, health and life sciences, business, education, social sciences, and more.  Transcript is machine generated, unedited, in English.  00:00 so welcome to the university of maine's 00:03 artificial intelligence webinar series 00:05 today's panel is focused on ai for 00:08 agriculture 00:09 um before we start i'd like to thank the 00:11 university of maine's office of the vice 00:12 president for research for planning this 00:14 event 00:15 and ieee main communication and computer 00:18 societies 00:19 joint chapter for sponsoring this 00:20 webinar 00:22 before we begin i want to remind those 00:24 of you attending to use the q 00:26 a feature to post your questions all 00:28 these questions will be asked 00:30 after each of the speakers or after all 00:32 of the speakers have completed their 00:33 presentations 00:35 so again thank you for joining us and it 00:37 is now my great pleasure to introduce 00:38 our first panelists 00:41 today we have joining us dr stephen j 00:44 thompson 00:44 he is the national program leader with 00:47 the usda national institute for food and 00:49 agriculture 00:50 he engages universities other federal 00:52 agencies and industry 00:54 to provide national leadership in 00:56 capacity and competitive grant programs 00:59 so with that i'd like to introduce steve 01:01 thank you 01:02 yes i'm stephen thompson um with uh 01:06 usca nifa i've been with nifa about 01:09 a little over five years now and 01:13 the national program leader in the uh 01:15 agriculture systems division i'm an 01:17 engineer by 01:18 training had 01:22 my background i was on faculty in two 01:25 different departments at virginia tech 01:28 and i was with usdar at usda 01:31 ars before i came here 01:34 so um it was associate professor 01:37 at virginia tech in the 90s so anyway 01:41 so let me give you a quick update first 01:44 on 01:44 uh some changes that have been made uh 01:47 secretary of the new secretary of 01:48 agriculture 01:49 tom vilsack he's been here before he 01:52 served under president obama 01:54 and we expect continuing interest in 01:57 some of the initiatives that are already 01:59 there 02:00 rural broadband precision ag he was a 02:03 big 02:04 proponent of robotics um in agriculture 02:08 and uh climate smart agriculture will be 02:12 more pronounced in this administration 02:15 dr 02:15 carey castille she was a science advisor 02:20 to the commissioner 02:21 for the louisiana department of 02:23 agriculture and forestry 02:25 she has an extensive extension 02:27 background 02:28 so i'll give you a short outline this 02:30 first part of the presentation 02:32 will not take very long i'm just going 02:34 to 02:36 emphasize a couple of aspects in plant 02:38 agriculture and 02:40 an example for a precision livestock 02:43 farming 02:44 and then i will get into the stuff you 02:47 really want to see 02:50 programs that involve some of most of 02:52 the research programs and the technical 02:54 side of things 02:55 that involve ai and usda nifa i'm not 02:58 going to talk about the educational 03:00 programs 03:01 but there are some uh you can 03:04 uh you can actually see those in the um 03:08 online on the nifa website 03:11 time won't allow me to get into the 03:13 weeds on every program 03:15 and then some future future outlook so 03:19 i want to center on one aspect of ai 03:22 cyber physical systems which 03:26 basically is pretty broad terms where 03:29 where physical and software components 03:31 are deeply intertwined 03:32 and they operate at different spatial 03:35 and temporal scales 03:36 and so you know many times you'll you'll 03:39 be 03:40 you'll have a system with sensing um 03:44 interpretive um should i say adaptive 03:47 control 03:48 data analytics in the middle part and 03:50 then an output control 03:52 or decision and there are feedback 03:55 loops involved you may have modeling 03:59 in working in concert with sensing these 04:02 kinds of things 04:05 um you know some of the obstacles 04:08 for i would say tech adoption and 04:11 cps the reason i say this is because 04:13 that middle part 04:14 is so important you know you have the 04:16 sensing but then the interpretation 04:19 how how ultimately how the 04:23 farm manager is going to be able to use 04:25 this is extremely important 04:28 there's some issues on the perceived 04:30 relevance of the technologies 04:32 actually just awareness of the available 04:34 technologies has been a 04:36 problem farmers like to fix things so 04:39 simplicity users guides this is not only 04:43 good for the farmer but for any 04:44 consultant that 04:45 is working with the farmer and and 04:48 there's been an issue with trust in tech 04:50 and dss 04:51 decision support recommendations now one 04:53 of our speakers um 04:56 later is going to talk about the 04:57 adoption side 04:59 and you know in a lot yeah i think 05:03 he has a very creative um he's going to 05:06 talk about a very 05:07 creative solution to this kind of shall 05:10 i say towards a solution to this kind of 05:11 thing so i will not 05:13 talk too much about the adoption area 05:16 now when you're talking about 05:18 uh plant-based management 05:24 you know there there are a lot of 05:26 sensors out there 05:28 they could be ground sensors aerial 05:31 sensors various things 05:32 and there are varying opinions on which 05:34 sensing systems work better 05:36 to detect certain anomalies in the field 05:39 okay but that research 05:40 is progressing and has progressed well 05:44 the like i say said before the middle 05:47 part of it the interpretive 05:50 kicker right part uh let me give you a 05:54 very short history on the precision ag 05:57 side of things back in the 05:59 mid 80s people were starting to realize 06:04 that 06:06 know you could you could have a cis 06:09 a setup where you're applying 06:13 inputs only to where they are needed 06:15 okay 06:16 and an article from 1985 that you won't 06:19 you won't find um i have a printed copy 06:23 but 06:23 it's i could not find it anywhere else 06:26 um farming by the foot proposed use 06:31 of lauren c technologies 06:34 uh which you know the uh accuracy is not 06:38 nearly as good as gps 06:41 but this was a concept in the early 80s 06:43 john schuler at the university of 06:45 florida 06:47 developed a combine yield map in 1986 06:51 i think this was this was a manually 06:54 obtained data 06:56 so this gives you an idea where the uh 06:59 when these concepts were first 07:01 thought about some work from my own 07:05 um background this was about a little 07:07 over 20 years ago 07:09 we thought hey you know let's just take 07:11 a high-res 07:13 video camera and put it in a 07:16 crop duster so anyway 07:20 we got an image like this and we were 07:23 able to 07:24 separate out we 07:28 grass weeds from broadleaf weeds and 07:31 cotton just as a conceptual idea 07:34 for future use in targeted spraying 07:39 for example this is 07:42 graduated to much more sophisticated 07:45 systems 07:46 up on the top left is a push broom 07:49 hyperspectral scanner 07:50 on uas these scanners have 07:54 decreased in size one on the top right 07:58 is a hyperspectral scanner that is a 08:01 frame based system of these companies in 08:05 finland 08:07 there are some companies in finland that 08:09 have developed these frame based systems 08:12 which are uh 08:13 would be easier to use but i don't have 08:15 any experience with them so 08:17 i'd be very interested to know one on 08:20 the bottom 08:21 is called a red edge um a 08:26 multi-spectral imaging system by a 08:28 company called micro 08:29 mica sense and um 08:34 so they're just they're basically 08:36 filters in 08:37 certain ranges of the optical spectrum 08:41 including near-infrared 08:44 also and 08:48 i alluded to this before cyber fiscal 08:50 systems can be the bridge in my opinion 08:53 i think 08:54 these will work better if you have sensi 08:58 if you have sensing and modeling or 09:01 sensor fusion sensing and modeling and 09:02 synergy 09:04 um i mentioned that the data analytics 09:06 part 09:07 it's very important to get that right 09:10 dss tools decision support tools are 09:14 readily 09:14 or becoming readily available for some 09:16 applications but not 09:18 others i've actually uh 09:22 funded a couple of grants for irrigation 09:24 for example where one is coming out 09:26 trying to reduce all these decisions 09:29 into a usable app 09:30 for the farmer so we'll see how that 09:32 goes 09:35 now i just want to point out 09:39 the bottom part here we're talking about 09:42 um 09:42 data interpretation and adoption 09:46 i would say now this is a slide dealing 09:49 with 09:50 variable rate technologies but this can 09:52 apply to many different 09:54 aspects say the low adoption on inputs 09:58 that require more data analysis of 10:00 prescription 10:02 algorithm development and this has been 10:04 the case this is from 10:05 a this is part of a slide from a study 10:09 on adoption from purdue university 10:13 so um i want to give one example 10:19 irrigation example look at the 10:22 yellow no not the blue and 10:26 green bars there 10:29 adoption of technologies for for 10:32 irrigation has been 10:34 really really poor very poor so 10:38 let's conceptualize something you may 10:40 have 10:41 some this is not really a flow chart but 10:43 there is flow here 10:44 you may have sensors in the field you 10:46 may have a 10:49 model of water balance that also depends 10:52 on 10:53 root distribution and root water uptake 10:56 in different zones 10:58 so in a nutshell what this one is doing 11:01 is using sensing and modeling and 11:04 synergy to make better estimates 11:08 okay so this is kind of this is a cyber 11:10 physical system in a way 11:15 let's not forget technology for small 11:18 farms 11:19 now this is something i was involved 11:21 with you can't get any simpler than this 11:24 this is a wash 11:25 tub with a toilet float and two marks on 11:28 the 11:28 back plate the the black mark would say 11:32 ear uh field capacity 11:35 uh that means well watered conditions 11:38 the red line would say irrigate 11:42 the trick is calibrating this to the 11:44 crop 11:45 soil combination and in my study i also 11:49 used soil water sensors in synergy with 11:53 this 11:54 so so we did publish something 11:57 on on use of this evaporation pan 12:01 and then in the future what you would do 12:03 is try to translate this into something 12:06 uh you you might actually take a signal 12:09 from this wash tub and send it remotely 12:11 or something 12:12 if you wanted to do that this is a very 12:15 simple concept 12:16 and not just for small farms it can be 12:18 used for big larger farms also 12:20 farmers like simple okay but there's a 12:23 lot more to 12:24 calibrating this than meets the eye 12:27 microsoft has actually gotten into the 12:29 game too 12:31 where they are essentially replacing 12:34 expensive ground 12:35 place sensors with cheap wi-fi receivers 12:39 at several locations and the principle 12:42 here is to measure the relative 12:44 time of flight of a signal to infer uh 12:47 soil properties and this is very good 12:50 paper at the bottom i would uh 12:52 suggest you get hold of this is just a 12:55 proceedings article 12:57 but uh for a proceedings 13:00 article the the results look very very 13:04 promising 13:06 livestock farming what you're doing here 13:09 is you have 13:12 to make immediate decisions in real time 13:16 whereas plant-based for plant-based um 13:19 agriculture you can 13:22 uh make decision in near real time and 13:25 depending on what you're looking at 13:27 and many times you're saving the data 13:29 from year to year say 13:31 if you have yield maps 13:34 you want to know what the yield trends 13:36 are from year to year 13:38 okay in this case 13:41 you have to take uh for animal comfort 13:43 and these kinds of things 13:45 many sensing inputs like biosensing 13:48 environmental 13:49 uh diagnostics of the systems is a is a 13:53 big 13:53 part of precision livestock farming 13:56 that's what plf 13:57 stands for and you have some of the 14:00 same concepts as you do for plant 14:04 plant-based you'll have sensing you may 14:06 have modeling 14:07 adaptive learning and i would also 14:10 submit that visual monitoring is a big 14:12 part of this too 14:13 you know i can envision having a large 14:15 screen with many many bar graphs so 14:18 the farm manager not only you know 14:22 this model based analysis uh could be 14:25 black boxed but if the farm 14:27 farm manager sees uh 14:30 sees the data up on the screen they may 14:33 actually look at some hey this doesn't 14:35 look right you know and 14:36 and uh check check out what's going on 14:39 and this could also be used to 14:41 uh update the algorithms you know in a 14:44 system like this so this is just a 14:46 concept 14:47 okay uh if i'm not short on time 14:51 i'm going to get through this 14:54 i want to mention how nifa funds are 14:56 provided let's get to the grant side 14:58 about 50 percent of our funding goes to 15:01 capacity grants 15:02 50 to competitive grants the capacity 15:06 would be 15:07 the hatch uh evans allen mcintyre 15:10 sentence and these type and i would 15:12 implore people to 15:14 you know when you're sending a um 15:16 project initiation for a capacity grant 15:19 really pay attention to the detail on 15:20 this 15:21 as uh you know if you see if i see a 15:25 three sentence methods i'll send it back 15:26 so make sure 15:27 these are fleshed out very nicely all 15:30 right 15:31 i'm going to go down the list here and 15:33 uh some with more detail than others 15:36 the first one is an engineering for 15:38 agricultural production systems now this 15:40 is in the afri 15:42 um portfolio 15:45 the funding the funding amount has gone 15:48 up to 650k 15:50 per grant 15:55 the funding rate's been rather low 15:56 though i had a 15:58 40 increase in the number of proposals 16:02 from 2019 to 2020. 16:06 funding rates down near 9 with the same 16:08 amount of money 16:10 data science for ag systems funding 16:12 rates a little bit higher 16:14 the 1 million level is for uh 16:18 cins which is uh 16:21 coordinated innovation networks okay 16:25 national robotics initiative is 16:28 now for these you need to look at the 16:30 nsf website to see the solicitations 16:34 however national robotics initiative 16:37 cyber physical systems these are both 16:40 funded by nifa and the panels are run by 16:43 nsf 16:44 uh nifa has a say in helping select the 16:47 panelists on this also 16:50 most of the all of the nifa submissions 16:52 have been 16:53 within the near within the real-time 16:56 agricultural data analytics and control 17:00 section smart and connected communities 17:03 is also a part of this 17:04 and you can see what our memphis aspect 17:08 is on that 17:10 signals in the soil another one uh 17:13 jim dombrowalski has 17:16 five million dollars my contribution is 17:19 variable 17:20 according to the budget a few more 17:24 pertinent programs 17:25 ai institutes of course these are the 17:28 big ones 17:28 we are in the process of reviewing 17:32 proposals now for the second year of 17:34 funding we funded two grants for 2020 17:37 one from uh pi from davis california the 17:41 other 17:41 university of illinois and several 17:43 several collaborators 17:45 collaborative universities and companies 17:47 too 17:48 um we're working on the solicitation for 17:52 2022 17:54 we will fund at least one grant out of 17:57 that one 17:58 sbir of course plant production and 18:01 protection engineering program 18:02 is one out of 10 programs from our 18:06 sbir portfolio there's a 15 18:09 funding rate across the board if you are 18:12 registered in sam you have to be 18:14 registered in sam's and have a duns 18:16 number or you won't get through the 18:17 front door 18:18 small and medium farms there are some 18:21 pretty interesting technology-based 18:24 uh technology-based grants funded out of 18:27 that program also 18:30 now if you're involved in the adoption 18:32 or diffusion of agricultural 18:34 technologies 18:35 one of these three programs should be 18:37 considered 18:38 especially economics markets and trade 18:42 okay so that one we're actually 18:43 modifying to enhance 18:45 uh these aspects 18:48 yes i think we're over a bit if you want 18:51 to just 18:52 put up your final yeah this is my final 18:55 slide 18:56 thank you a couple of observations we 18:59 need to integrate 19:00 social sciences even more in our grant 19:02 programs 19:03 um and emphasize extension and education 19:07 more 19:08 in aai institutes anyway you can read 19:12 all this 19:12 i'm over time so we'll talk about it 19:15 later 19:16 thank you very much 19:19 and uh if you have uh if you want to 19:22 serve on a grant panel send me a cv 19:25 to find grant summaries look uh google 19:27 usd 19:28 cris and use the assisted search thank 19:30 you very much 19:32 thank you so much very very well done 19:36 so for our next speaker i'd like to 19:39 introduce 19:39 dr nuresh davanini he is an associate 19:43 professor in the department of civil 19:44 engineering 19:45 at the city university of new york city 19:46 college he holds an ms and phd in civil 19:49 engineering from north carolina state 19:51 and he did his post-doctoral studies at 19:53 columbia so with that 19:54 uh thank you dr davanini great 19:58 thanks susan i'm gonna share my screen 20:01 so today the title for this talk is 20:04 the role of big data analytics in 20:06 securing the future of water 20:08 energy and food really what i'm trying 20:10 to present here is 20:12 the applications of the classical ai 20:15 techniques like process optimization 20:17 predictive models and how we can use big 20:20 data 20:20 to how we can leverage big data and use 20:23 these classical ai techniques 20:25 to find solutions for water food energy 20:28 problems that we are 20:29 seeing a lot these days so on that i'm 20:32 going to present two stories 20:34 one is an application in india and one 20:36 is an application in the us 20:38 both problems are national at scope for 20:41 the case of india the problem 20:42 identification is that 20:44 there is an unsustainable practice 20:46 agricultural practice currently going on 20:48 in india 20:49 that will lead to an acute water stress 20:51 it is already water stress but it will 20:52 really 20:53 lead to much more acute stress if the 20:55 practices are continuing 20:56 so for identifying this problem much 20:59 more closely what we did is to 21:01 use big data from disparate sources we 21:03 assimilated all of those things 21:05 and we tried to learn about the current 21:07 water stress using the big data so we're 21:08 learning about water stress from the 21:10 data 21:10 and then the solution we propose is then 21:13 to use data-driven predictive techniques 21:15 and process optimization these are like 21:17 the classical systems methods 21:19 to remove the stress while still 21:20 achieving food security in india 21:22 so in a way we're trying to find can we 21:24 really solve this problem using ai 21:26 techniques 21:27 in the second application which is the 21:29 planning for united states agriculture 21:31 for climate change 21:32 the problem identification is that 21:33 extreme temperatures under climate 21:35 change 21:36 will reduce yields for several of the 21:38 major u.s crops 21:39 so for that we try to use a bayesian 21:41 learning method to understand and 21:43 predict 21:44 crop yields based on technology based on 21:46 temperature and water stresses 21:48 and the solution we're trying to come up 21:50 with is use again systems optimization 21:52 methods 21:53 to predict crop switching strategies to 21:55 mitigate yield losses 21:57 so i'll start with the first story which 21:58 is the story of india and here i'd like 22:00 to acknowledge my 22:02 colleagues here umanullah from columbia 22:04 university and sharma parveen 22:06 here the question is based on the 22:08 current crisis in india 22:10 can can we solve this issue and make 22:12 india one of the 22:13 leading agricultural sustainable 22:15 producers 22:16 in the market with an end times boost in 22:18 the rural income 22:20 but really let's look at the problem 22:22 first if you look at this 22:24 these two images one is an image on the 22:26 left which is 22:27 uh provided by gray satellite which was 22:29 detecting largest groundwater mining 22:31 operations in the world 22:33 and that region that you see here which 22:35 is in red color 22:36 is also the place like the football of 22:39 india it is heavily populated and what 22:41 you're seeing on the right is 22:43 uh the actual groundwater extractions 22:45 the number of wells 22:46 that are installed in those places as 22:48 you can see like heavily populated 22:50 region 22:50 uh heavy groundwater extractions mainly 22:53 for the purpose of 22:54 agriculture but this problem has 22:57 actually started in the 1950s and 60s 23:00 when the country went through a big 23:01 famine 23:01 and food insecurity because of low 23:03 yields and high fluctuation 23:05 poor storage distribution system there 23:07 were market fluctuations 23:09 there was stock manipulation to arrest 23:11 the trend what the government has done 23:13 at that time is to float this idea 23:15 called the national grain 23:16 procurement stream where there was 23:18 guaranteed price on 23:20 select crops and all these crops used to 23:22 be procured for 23:23 the purpose of national food security so 23:26 this really initiated the green 23:27 revolution 23:28 where you have improved cultivators 23:30 better fertilizers and practices 23:32 there were research and extension 23:34 programs created by the government 23:36 while at the same time it was also 23:37 giving irrigation provisions to buffer 23:39 climate variability 23:40 and one of the biggest uh additions at 23:43 that time was to give electricity and 23:45 water for free 23:46 which means that we have heavy 23:48 groundwater installations and as a 23:50 result we have huge groundwater pumping 23:52 to sustain the climate variability for 23:54 agriculture the figures on the right 23:56 that you see here is that 23:58 just to outline that india is one of the 24:01 highest grown water extractors 24:03 compared to a lot of countries in the 24:04 world and the figure on the bottom you 24:06 see here is that 24:07 although india has other sources of 24:09 irrigation like canals and tanks 24:11 because of their unreliability and 24:13 because of 24:14 guaranteed prices and free electricity 24:17 the trend of usage of groundwater 24:18 irrigation has gone up 24:20 significantly so it's now one of the 24:21 predominant agriculture irrigation 24:23 technologies india is just extracting 24:25 groundwater 24:27 and as a result what you can see here is 24:29 this is the current cropping pattern 24:31 a lot of the cropping patterns have 24:33 shifted from their traditional practices 24:35 which used to be their done before the 24:37 1950s now because 24:39 their farmers are reacting to these 24:40 incentives what you see here is 24:42 rice has moved to the northwest 24:45 countries 24:46 of punjab states of punjab and haryana 24:48 and uttar pradesh 24:50 and what used to be pulses and cereals 24:53 are now completely not grown in that 24:55 place 24:55 but when you look at the climate side of 24:57 the story you find that that place where 24:59 you are actually growing rice currently 25:01 is uh is a dry land and this is a place 25:04 where you only get a third of what is 25:06 required by rice 25:08 but nevertheless this place contributes 25:10 to the highest grain production in the 25:11 country 25:12 because most of that is coming from the 25:14 irrigation 25:15 which is coming from groundwater and as 25:17 a result you see that 25:18 while you have food security now you are 25:21 coming it's coming at a risk of water 25:23 security for the future of the country 25:26 so the first thing we did was to 25:28 identify what this stress is so we're 25:30 trying to use this big data methods to 25:32 identify the stress predict the water 25:34 stress in the country and look at what 25:36 are these 25:36 stressy zones so for this what we did 25:39 was assimilate a large database 25:41 a hundred years worth of daily climate 25:43 rainfall temperature data 25:45 we assimilated all the current cropping 25:47 data sets from india at the district 25:49 level 25:49 and you can see the dimensionality is 25:51 really like multi multiple-fold 25:53 the data sources can come from various 25:55 sources a lot of them from government 25:57 websites 25:58 a good number of them from research 26:00 articles and just talking to locals and 26:02 getting those surveys we used all the 26:05 data to first predict 26:06 water stress measured which is really a 26:08 function of rainfall temperature 26:10 evaporation water demands and crop 26:12 biophysical characteristics 26:14 and what you see here is just an 26:15 illustration of that water stress 26:17 without going into much detail what you 26:20 can see is that if it is in the red zone 26:22 it is really bad 26:24 both at the annual water stress measure 26:26 and 26:27 when you look at multi-annual it is also 26:28 very bad meaning if this place is 26:30 susceptible to multiple years of drought 26:32 the stress really accumulates and 26:34 becomes a chronic problem in stock just 26:36 an annual problem 26:37 and when we integrate these two annual 26:38 and multi-annual stressors what you'll 26:40 see here are 26:41 a development of uh an agroecological 26:44 zone sort of what we are seeing here is 26:46 that 26:46 some places are sustainable which are 26:48 the blue zones some places are prone to 26:50 multiple years of stress multiple years 26:52 of drought 26:52 which are the red zones and some places 26:54 are really unsustainable for agriculture 26:57 at least based on the current cropping 26:58 pattern so the green zones that you see 27:00 here 27:00 which means that you really need 27:01 shifting of crops from those places 27:04 so then if this is a problem what the 27:06 solution we're trying to propose is 27:08 can we actually use all this big data in 27:10 a systems method 27:12 uh process optimization is what it's 27:14 called in ai language 27:15 to predict what the sustainable zones of 27:17 agricultural enhancement would be 27:19 so that way we're trying to address 27:20 climate energy water 27:22 food income nexus all conjunctively in 27:24 one solution 27:26 and the question we started asking was 27:28 when we looked at this figure which is 27:30 the acreage of rice and pulses in punjab 27:34 the northwest part of india 27:36 you can see since the 1970s there's a 27:38 huge increase in the cultivation of rice 27:40 and a huge drop in the cultivation of 27:42 pulses which used to be the traditional 27:44 crops in those places 27:45 and much of this rice as i said it's not 27:47 it's not raining so much there 27:49 but all that irrigation is coming from 27:51 groundwater and you know in fact 27:52 up to 70 or 80 percent of that 27:54 irrigation is really for groundwater 27:56 so we started asking this question like 27:58 can we use some 27:59 data analytic techniques to solve this 28:01 so the question is did the green 28:03 revolution really stick in the wrong 28:04 places 28:05 could india's water stress be reduced 28:07 just by changing where 28:09 what is grown so then can these analytic 28:12 methods help us identify what those 28:14 zones are 28:15 and how much to grow in those places 28:17 like a decision tool and 28:18 what is the role of climate in these 28:20 things so we created this large scale 28:23 process optimization model 28:24 which is trying to predict sustainable 28:26 zones for agricultural enhancement 28:28 the inputs of this model are the climate 28:30 scenarios more than 100 years of climate 28:33 precip and temperature at the district 28:35 level and the crop eligibility whether 28:37 the crop can be grown there or not 28:39 we are using all those inputs to predict 28:41 water stress 28:42 which will predict crop yields which 28:44 will in turn predict crop production 28:46 then the other inputs are current prices 28:48 of the crops 28:49 cost of cultivation cost of irrigation 28:51 so all those go into this process 28:53 optimization and the objective is to 28:55 maximize the national revenue that is 28:57 coming from agriculture 28:58 while we have a bunch of constraints 29:00 which are trying to keep the things 29:02 sustainable 29:02 we don't want to increase the land which 29:04 is currently under 29:06 agriculture meaning we are already maxed 29:08 out on land so we are not using more 29:10 land 29:10 and we are trying to keep irrigation to 29:12 a sustainable limit 29:13 and we also played around with a 29:15 scenario where there is no irrigation at 29:16 all across the country 29:18 and you'd be surprised that even without 29:19 irrigation you can still get national 29:21 food security 29:22 on an average year so the model runs as 29:25 a 29:26 simulation optimization model uh and you 29:29 can develop multiple scenarios out of 29:31 these things and identify what are these 29:32 best zones 29:33 so before looking at the results i'll 29:35 show you this plot again 29:36 that this is the current cropping 29:39 practice where you have 29:40 a lot of crop of rice in that northwest 29:43 part 29:44 and the other cereals which used to be 29:45 the traditional crops were not 29:47 are not there currently now when you run 29:49 this model and identify what are the 29:51 solutions that come from this model 29:53 you'll find that in the northwest part 29:55 of india the rice is completely removed 29:58 and it's been replaced by other cereals 30:00 both under the green water scenario in 30:02 the blue water scenario which is 30:04 you know assuming there is no irrigation 30:05 in the entire country completely green 30:07 and assuming there is some sustainable 30:09 irrigation which is 30:10 renewed on an annual basis that is the 30:12 blue water scenario 30:14 you can see uh there's a 75 reduction in 30:16 how much crop goes into those places 30:18 and it's been replaced by traditional 30:20 crops that used to be grown there 30:22 now when you look at at the national 30:24 level how much revenue you can generate 30:26 uh it is much greater at least thirty 30:28 percent greater than the current revenue 30:30 that you can generate 30:31 this is the sustainable irrigation 30:33 scenario even the rain fed scenario is 30:35 five percent 30:36 greater than what you're currently 30:37 generating all the other production 30:39 limits 30:40 are met or even we are producing more 30:41 than what we need for now 30:43 uh this also calls for the story that 30:45 currently the country imports a lot of 30:47 oil seeds and pulses but with this 30:49 optimized scenario 30:50 you can look at reducing those 30:52 dependencies on external trades also so 30:55 if the country becomes completely 30:56 self-sustained 30:57 for all the types of crops that you need 30:59 and an even bigger opportunity here 31:01 is that if the corporations and 31:03 government participate together 31:05 to address market risks and price risks 31:08 and also provide help in terms of 31:09 technology innovations in the 31:11 agriculture supply chain 31:12 the story is even bigger because you're 31:14 really solving a national problem 31:16 using a well-established ai technique 31:20 uh in the next story what i'm going to 31:22 present is how do we plan 31:25 united states agriculture for climate 31:26 change and i'd like to acknowledge my 31:28 colleague here from london school of 31:30 economics james rising 31:33 so here the problem was extreme 31:35 temperatures in the u.s under climate 31:37 change will reduce yields for several of 31:39 the major crops 31:40 so the first thing we try to identify 31:42 here is a learning technique a bayesian 31:44 learning technique to understand and 31:45 predict 31:46 crop yields based on technology 31:48 temperature and water stresses and the 31:49 solution we proposed are 31:51 using systems methods to predict props 31:54 switching strategies 31:56 very quickly without taking much time 31:58 i'll the it has two 31:59 aspects to it the first part is a 32:02 complete 32:02 uh yield prediction model that is trying 32:05 to predict yields for every county for 32:07 the 32:08 past 50 60 years using climate inputs 32:10 and water stress inputs 32:12 then it's a learning model so it right 32:14 tries to understand what is the 32:16 influence of temperature on each of 32:17 these crop yield 32:18 parameters then the second level is use 32:21 all these things all the predicted 32:22 yields 32:23 with an economic optimization for 32:25 decision analytics for future climate 32:27 this is akin to the optimization 32:29 technique so a 32:31 quick example of how a yield prediction 32:33 model looks like 32:34 the red color line is there two two 32:35 examples from two counties here 32:37 the model predicted yields are presented 32:39 in the black box plots the distributions 32:42 and the red color line is the observed 32:44 yield so you can clearly pick up the 32:45 technology trend 32:46 you can pick up the variability also 32:48 across the years based on the climate 32:50 stressors that it's being applied on 32:52 and finally we did the optimization 32:56 model we looked at what are the best 32:57 locations for 2050 and 2070 under future 33:00 climate scenarios 33:01 and you find that corn slowly becomes 33:03 less concentrated in the midwest 33:05 soybeans show gradual movement to the 33:07 north winter wheat moves up from the 33:09 south 33:10 along the mississippi cotton is grown at 33:12 higher latitudes compared to now 33:14 and it becomes it become dominant in the 33:16 future in the southern california area 33:18 when you look at the overall revenue 33:20 that the country can generate purely 33:22 from agriculture from these crops 33:24 you can see that the optimized scenarios 33:25 are always better than the current 33:27 observed scenarios 33:28 under future climate 2050 2070 scenarios 33:32 you do have reduction in the revenue but 33:34 then the optimized scenarios are much 33:36 better than 33:37 if you actually continue to grow where 33:39 you're growing today because 33:40 the future climate is going to reduce 33:42 the yields that means some locations are 33:43 not going to be 33:44 optimal anymore so we try to find the 33:46 optimal solutions using decision 33:48 analytics 33:50 finally i would like to close with 33:51 saying that there are multiple 33:53 possibilities with ai 33:55 to solve the multi-scale agricultural 33:57 issues so the solutions 33:58 can be applied at multiple scales and at 34:00 every scale you have possibilities with 34:02 ai 34:03 starting with at the national level if 34:05 you want to optimize where what is grown 34:07 you have analytics type of models to 34:08 develop those things 34:10 again at the national level the it's 34:12 mainly driven by economic signals 34:14 and storage and management signals those 34:17 also 34:17 can be left we can use big data to 34:19 leverage those ideas and try to give 34:21 better pricing forecasts 34:23 better climate forecasts better market 34:25 forecasts so that we 34:26 adapt to that future conditions and on 34:29 the farm technologies which is like at 34:31 the local scale 34:32 we can have efficiency in irrigation 34:34 monitoring and practices 34:36 uh rainfall crop productivity can be 34:38 increased by on-form technologies 34:40 you can have better soil sensing also 34:43 using all these like 34:44 robotic technologies and things for the 34:46 long term you can also develop 34:48 uh his traditional risk management 34:51 models 34:51 which are rooted in predictive analytics 34:53 techniques for that we can look at food 34:55 water financial storage 34:57 development and trade between various 34:59 countries and 35:00 try to increase the yield of dry land 35:02 crops using the 35:03 local sources but also regional 35:05 solutions across 35:07 i'll stop there thank you i'd like to 35:10 move on to our final speaker um abdi 35:14 tamurthy 35:14 he is the founder and ceo of consor 35:17 water technologies which 35:18 uses satellites and ai to measure soil 35:21 moisture 35:22 nutrient levels and other actionable 35:24 information about farmland and soils 35:26 to help farmers grow more with less 35:28 without using any hardware or soil 35:30 sampling 35:31 thousands of farmers worldwide currently 35:32 use concert water to save on water and 35:34 nutrients and increase the fertility of 35:36 their farms 35:38 audit is a stanford knight hennessey 35:40 scholar and holds a master's degree from 35:41 stanford along with two bachelor's 35:43 degrees from caltech he has also been 35:46 named a top founder in ai by 35:48 techcrunch and without with that i'd 35:50 like to welcome our final speaker thank 35:52 you 35:52 thank you for the kind introduction um 35:55 as just mentioned i am ultimately i'm 35:57 the founder and ceo 35:58 of constant water technologies uh where 36:01 we help farmers worldwide to grow more 36:03 with 36:03 less and i founded concert water 36:08 about four years ago and i kind of want 36:10 to narrate some of 36:12 our learnings from doing that and around 36:15 agricultural ai 36:16 where one of the key missions of 36:18 agricultural ai 36:20 at least in my opinion is to enable 36:23 farmers 36:24 to grow more with less and 36:27 while doing it in a sustainable manner 36:30 and 36:31 at scale so as you might be familiar 36:34 with ai 36:35 from other industries it's commonly 36:38 deployed 36:38 to enable scale to be unlocked 36:42 like for example when google or facebook 36:44 use ai 36:45 to target ads it enables them to scale 36:47 to billions of people 36:49 why can't that same thing be true in 36:51 agriculture as well can we design 36:52 solutions that 36:54 can scale dramatically to hundreds of 36:57 millions of farmers can we design those 36:59 kinds of solutions 37:00 it has been kind of the guiding 37:02 motivation here at least behind 37:04 how we have been approaching this area 37:07 and to give you a general idea of how 37:09 i like to think about ai at least in the 37:12 in its most 37:13 useful form in agriculture it's sort of 37:16 like this picture here 37:17 this picture might seem kind of a bit 37:19 abstract to you 37:21 if you take a second you'll notice that 37:24 there's these 37:25 purple stripes that are going 37:27 horizontally across the stream 37:28 at the screen but if you look a bit 37:31 deeper 37:32 if you look behind the stripes you start 37:33 to see something is present 37:35 and if you were kind of aware of 37:38 this location where this picture was 37:39 taken you might actually recognize 37:41 that's actually the university of maine 37:43 campus 37:44 in the background so this is essentially 37:47 one of the key purposes 37:49 of ai uh at least what it can 37:52 one of the big potentials of ai in 37:54 agriculture is that it can 37:56 unlock these hidden patterns that 37:59 human beings are not necessarily able to 38:01 unlock on their own 38:03 and then once you've unlocked that 38:05 you're able to derive 38:06 all sorts of value ai is able to 38:08 identify these hidden patterns that are 38:10 very hard for humans to do 38:13 of course there's also ai that can just 38:15 essentially 38:16 replace human beings but that's not the 38:18 kind that is 38:19 most sustainable for humanity in the 38:21 long term the best kind of ai at least 38:24 in 38:24 my opinion is ai that can unlock these 38:27 insights that you never 38:28 knew about before and thus gives you 38:31 completely original insights that can 38:33 change your operation change what you do 38:36 in a dramatic way and 38:39 today when we think about ai we usually 38:42 think about it 38:43 in terms of machine learning or 38:45 sometimes deep learning which is a 38:47 subset of machine learning this 38:49 maya is a very very broad term there's 38:50 many many different 38:52 ai techniques there's expert systems 38:54 there's uh all sorts of 38:56 techniques that are developed in the 38:57 past then when people generally talk 39:00 about 39:00 ai today they're usually referring to 39:04 machine learning when people generally 39:06 talk about ai and you might be familiar 39:08 with 39:08 some of the common methodologies used in 39:11 machine learning 39:12 and i'm going to illustrate in a moment 39:14 here 39:15 how uh how a uh how 39:18 all of these can actually find a role in 39:21 agriculture 39:22 to do very unique tasks that enable us 39:24 to unlock 39:25 all sorts of value so but before i go 39:28 into that let me just start out with 39:30 just a brief overview of each of these 39:32 types that everyone is clear 39:34 on what they mean so uh so the most 39:37 common kind of machine learning that you 39:38 might 39:39 normally think of is actually supervised 39:41 learning 39:42 the bottom right here and supervised 39:44 learning is where you have 39:45 a set of inputs and a set of outputs and 39:48 then 39:48 you train an ai model or a machine 39:51 learning model rather to be specific 39:53 to build a function that goes from the 39:56 inputs to the outputs if you gave it the 39:58 inputs 39:58 it can predict the outputs the computer 40:01 learns that function 40:03 that relates the inputs to the outputs 40:05 that is supervised learning when there's 40:07 inputs and outputs and you're building a 40:08 mathematical function 40:10 between them on the slightly other end 40:13 of the spectrum 40:14 there's unsupervised learning where 40:16 instead of having 40:17 inputs and outputs you just have inputs 40:20 and then you're trying to figure out 40:22 some relationship between the inputs 40:24 data without actually knowing 40:26 any reasonable outputs it's used to find 40:28 relationships and 40:29 between the data within the data that is 40:32 not otherwise 40:32 visible and unsupervised learning 40:36 also has interesting roles to play in 40:38 agriculture 40:39 but commonly you often think mostly 40:41 about supervised learning and not as 40:42 much about unsupervised learning 40:44 but still it's useful to have all these 40:46 frameworks in your mind 40:47 and thirdly there's reinforcement 40:49 learning 40:50 this uh this is what you might think of 40:53 like uh 40:54 like uh google's 40:58 go program playing the game go it became 41:00 so good at go 41:01 by through reinforcement learning 41:04 uh where reinforcement learning is 41:08 a slightly different paradigm of machine 41:09 learning where you're trying to 41:10 understand 41:12 how to optimize a certain task how can i 41:15 optimize to for example win a game 41:18 for example and all of these kinds of 41:20 scenarios find 41:21 interesting uses in agriculture so 41:23 without further ado when you think about 41:25 unsupervised learning 41:27 this is actually a a set of fields from 41:30 one of our clients actually at concert 41:32 water technologies here you can see that 41:34 this client has 41:36 several forms like dozens of individual 41:38 fields 41:40 however the reality is that you don't 41:42 often when you're on the farm you don't 41:44 often have 41:45 unlimited resources to plow into these 41:48 unlimited fields 41:50 you only have a limited set of resources 41:52 how do you allocate those resources in a 41:55 good manner so that you optimize various 41:58 conditions on the farm 41:59 one way that you can find out actually 42:02 is 42:02 by doing unsupervised learning on these 42:06 fields themselves to find 42:07 patterns and groupings within these 42:09 fields uh 42:11 natural groupings which then to those 42:13 groupings you can 42:14 then decide to apply certain 42:16 interventions like for example this is 42:18 just a naive guess from a 42:20 human eye perspective ai might pick up 42:23 further things 42:24 uh if you feed it with large amounts of 42:26 satellite data for example here you see 42:28 that these 42:28 fields seem to be slightly greener these 42:30 fields seem to be 42:31 slightly redders these seats are 42:33 slightly yellower 42:34 those might be natural ways to break up 42:37 the these fields in order to manage them 42:39 in a smaller chunks but there's a 42:41 variety of things that can be done here 42:43 depending on what is the final objective 42:45 for the grower is it yield maximization 42:47 reduction of disease 42:48 whatever is the objective uh the 42:51 unsupervised learning can give get you 42:53 a first step at understanding what are 42:55 these various kinds of groupings 42:57 but other than that unsupervised 42:58 learning is probably still quite in its 43:01 infancy from its applications in 43:03 agriculture because 43:05 of uh we just need to find the right 43:07 problems to apply to 43:08 so without further ado we come to 43:10 supervised learning 43:12 is probably as i mentioned the most 43:14 common thing that most people think 43:15 about when it comes to machine learning 43:18 there's two flavors of supervised 43:19 learning there's classification problems 43:22 where you're simply deciding whether 43:25 whether it's a cat or a dog is what 43:29 google's video ai would do what we are 43:32 doing 43:33 here is deciding between various classes 43:35 in agriculture 43:36 about what whether uh whether something 43:40 falls into one bucket or another bucket 43:42 as i'll show you in a bit 43:43 and regression is when and regression 43:45 actually quite common in agriculture as 43:47 well 43:48 uh much more common i'd say than in the 43:50 general 43:51 uh general ai in general uh when 43:54 regression you're trying to predict a 43:57 quantitative variable 43:58 an actual number as opposed to a class 44:00 whether it's a or b 44:02 you're trying to predict is it zero 44:04 percent fifty percent or 100 44:05 of some property whatever property is so 44:08 in a 44:09 classification one of the very very very 44:11 common use cases 44:13 of ai in agriculture which has actually 44:15 gotten quite a reasonable level of 44:17 adoption actually currently 44:19 this is not something that we do at 44:20 concert water but we find it very 44:21 interesting 44:23 to see the levels of adoption that have 44:24 been reached is actually in 44:26 using your phones to take pictures of 44:29 diseased leaves or disease plants 44:32 and then an ai can simply classify and 44:34 say 44:35 what is the disease or with some level 44:38 of 44:39 accuracy it can say what is the disease 44:41 that's afflicting your plant 44:42 you don't need to be an expert in 44:44 agronomy and understand 44:45 all of the various diseases that afflict 44:47 your plant 44:48 and as long as the system is trained 44:52 this uh the apps that are out there are 44:54 trained for those particular conditions 44:55 they can identify directly you don't 44:57 need to call up your agronomist to get 44:58 this information 45:00 uh it can find this patterns in the leaf 45:02 directly just by taking 45:04 pictures and then once you have that 45:05 picture it can identify 45:07 uh it can identify with some accuracy 45:09 what are the conditions and then you can 45:11 take the appropriate interventions 45:13 to reduce that the risk that that 45:16 disease poses to 45:17 the farming operation so that's a very 45:19 very interesting application 45:21 and and there there have been data sets 45:23 of various pictures and also 45:26 disease classifications that have 45:27 already been collected by various 45:29 labs around the world agricultural labs 45:31 and using these 45:32 people have been able to build deep 45:34 learning models in this case 45:35 to directly identify what are the 45:37 diseases uh 45:38 one other kind of application that we 45:40 have found actually more directly at 45:42 constant water is 45:43 by using satellites along with ai we're 45:46 able to identify 45:48 basically classify on a pixel level 45:50 where there might be anomalies 45:52 and potential diseased areas uh so 45:55 and what we're able to do beyond that 45:56 which is very important in agriculture 45:58 is you don't just want to have this 46:00 analysis you want to be able to present 46:01 the analysis in a way 46:03 that's understandable easy to use fast 46:06 to use for the farmer 46:07 who might otherwise find it hard to use 46:10 your technology if you can't actually 46:12 present it in a useful 46:14 manner this is not just true in 46:15 agriculture it's probably true in every 46:16 field 46:17 here to present the results in a manner 46:18 that are understandable uh here what 46:20 we're able to do is we have to identify 46:22 on a pixel level 46:24 where there might be potential diseased 46:26 areas or uh 46:28 are unhealthy areas and then what the 46:30 next layer of processing on top of that 46:32 is you 46:32 go to google maps and you place a marker 46:35 on those areas 46:36 automatically with code so then the 46:38 farmer can then just 46:40 walk to these markers and see what's 46:41 going on at that particular area 46:43 so what's basically going on here is 46:46 we're able to use a variety of satellite 46:48 bands 46:49 to pinpoint anomalies by using 46:52 supervised learning with classification 46:54 to classify pixels and decide whether 46:56 is this this pixel potentially the 46:58 location of an anomaly 47:00 and then direct farmers to those 47:02 anomalies 47:03 so that they can manage things more 47:05 effectively 47:06 and then moving on to into a regression 47:08 i think this is already mentioned 47:10 uh by uh by dr dave nani just before me 47:14 is one of the common things about 47:16 regression at least today common use 47:17 cases is actually in 47:19 crop yield prediction on a usually on a 47:22 county level or it could be sometimes 47:24 more granular than that 47:25 you can do crop yield predictions just 47:27 based on very basic 47:29 levels of satellite data you can get a 47:31 very good prediction of crop yields 47:33 which can then inform not only farmers 47:35 in that area but also investors in 47:36 general 47:37 when they try to decide whether 47:40 purchasing 47:41 corn contracts for this particular year 47:43 is a good investment decision 47:44 or not so all those kinds of efficiency 47:47 are being unlocked 47:48 by using satellites and ai kinds of 47:50 efficiency that were not present before 47:52 in the system 47:53 we're now coming more to uh where at 47:56 concert water we significantly use 47:58 supervised learning is in this uh 48:01 by using over 100 satellites throughout 48:04 the entire electromagnetic spectrum 48:06 from uv to radio waves which is the 48:09 input data and the supervised learning 48:11 if you make 48:12 we're able to combine that with the ai 48:14 models that we train using 48:17 large amounts of ground truth data in 48:19 the form of soil tests for example 48:21 actual lab soil tests 48:23 or sensor arrays around the world soil 48:25 moisture sensor rays 48:26 and convert that basic convert what the 48:29 satellite sees into actionable numbers 48:31 about 48:33 plants and soil information such as the 48:35 soil moisture levels 48:37 soil nutrient levels soil carbon levels 48:40 a variety of different things can be 48:42 understood by using 48:43 a large amount of satellite data coupled 48:46 with a large amount of ground truth data 48:48 upon which ai models are trained and 48:51 thus we are able to convert what the 48:53 satellite sees into these actionable 48:54 numbers 48:55 and why that's important is that 48:58 satellites actually take a huge huge 49:01 huge amount of data 49:02 like terabytes of data every single day 49:05 uh several terabytes of data 49:07 and overall like over the course of 49:08 several years we have many many 49:10 petabytes 49:11 of data huge amount of data human mind 49:14 cannot really comprehend that level of 49:16 data 49:17 and especially the trends shown in the 49:19 satellite data it's very very hard to 49:21 comprehend 49:22 very easily because what the satellite 49:24 sees is simply 49:26 raw numbers it's like your camera on 49:28 your phone 49:29 it just sees the counts of photons that 49:31 come off of the earth and everything 49:32 else is a derived product on top of that 49:34 just the count of photons what we are 49:36 able to do with ai is basically 49:38 convert those raw numbers into 49:41 actionable numbers 49:43 which the computer system the ai the 49:45 machine learning model 49:46 trains but trains to understand by 49:49 looking at the satellite data so very 49:51 classic 49:52 supervised learning going from inputs to 49:54 certain outputs they are 49:56 very relevant for farmers to manage 49:58 their operations 49:59 more efficiently and the best part is 50:02 that 50:02 it's not it's actually not like having a 50:04 soil sensor or a lab test because we're 50:06 able to do 50:07 uh we are able to actually use satellite 50:09 data to get things at an even granular 50:12 resolution you could even get like 10 50:14 meter less than 10 meter even three 50:16 meter 50:17 and uh in uh in a comma in a customer 50:20 units three meters would be 10 feet 50:22 so even at 10 feet resolution you can 50:25 get this information 50:26 uh using satellites coupled with ai you 50:30 can get information at that level of 50:31 resolution 50:32 so all sorts of new things are unlocked 50:34 a new possibilities 50:36 ai is unlocking all sorts of new 50:38 possibilities as opposed to 50:40 replacing existing possibilities and 50:42 just before we conclude here a quick 50:43 note about reinforcement learning 50:46 i personally feel uh reinforcement 50:48 learning is one of the 50:49 most important up and coming ways do you 50:52 place ai here 50:53 in agriculture because it's very 50:54 important from the adoption standpoint 50:57 which uh 50:58 uh which was mentioned the very first 51:00 talk in our sequence here 51:02 uh so uh one of the most basic kinds of 51:04 uh 51:05 process optimization leveraging 51:06 reinforcement learning has already found 51:08 adoption 51:09 is actually in indoor forms because all 51:12 of the variables are completely 51:13 controlled 51:14 in indoor forms and hence it's a it's a 51:16 slightly easier problem than outdoor 51:18 forms 51:18 when all the variables are controlled 51:20 but when you actually go to outdoor 51:22 farms there are very very interesting 51:23 applications 51:24 like for example in this case we are 51:26 showing i guess some of 51:28 our or some of our partners at constant 51:30 water for example tow gill 51:31 uh irrigation controller units these 51:34 controller units sit on the farm 51:36 and what they're able to do is they're 51:38 able to 51:39 use our apis on the cloud console waters 51:42 apis 51:43 to implement ai on the farm and 51:45 essentially automate the farm 51:46 but that's not the interesting part 51:48 where reinforcement learning comes in 51:50 the way the place where reinforcement 51:51 learning comes in is once you have a 51:53 unit like this 51:54 we are able to use reinforcement 51:56 learning 51:57 to optimize an objective such as uh 52:00 such as maximizing your yield that might 52:03 be the most important 52:04 thing for the farmer maximize your yield 52:06 or it could be sometimes 52:07 reduced water use uh you want to 52:09 minimize the water use there's various 52:11 objectives depending on the farmer 52:12 that's interesting for optimization a 52:15 reinforcement learning scenario 52:16 but once you have this kind of setup you 52:18 can then start to think about 52:20 reinforcement learning 52:21 in just optimizing the daily decisions 52:24 uh 52:24 water applied nutrient applied using 52:27 satellites in ai 52:29 and crucially on the behavior change 52:30 perspective which behavior change is a 52:32 very very hard thing to achieve in 52:34 in agriculture what reinforcement 52:36 learning can do for you is it can adapt 52:38 to the 52:39 to the farmers existing practices it can 52:42 essentially learn 52:43 how the farmer manages their farm on a 52:45 daily basis based on all of these inputs 52:48 and then help the farmer to slowly ease 52:51 them 52:52 into the efficient scenario provided by 52:56 the supervised learning models like for 52:58 example i'll give you a real example 52:59 here 53:00 we had some farmers who are using uh 1 53:03 000 53:04 or yeah so we 10 000 cubic feet of water 53:08 per hectare 53:09 per year uh is what they were commonly 53:12 using 53:13 and then we had them what our models 53:15 were initially suggesting to them is you 53:16 don't need ten thousand you only need 53:18 six thousand 53:19 uh when you do all these calculations 53:21 but then there's 53:22 often going there's often some push back 53:24 uh because you're going suddenly a forty 53:26 percent decrease i'm so worried that 53:28 that's something that's going to happen 53:29 to my crops 53:30 the solution to that is actually to go 53:32 from 10 000 53:33 to 1900 9800 53:38 9700 slowly ramp it down and slowly ramp 53:41 it down 53:41 as per the farmer's existing practices 53:44 that he slowly eases 53:45 into an optimal scenario uh where 53:49 you eventually reach six thousand you 53:50 don't drop to six thousand overnight 53:52 this is a gradual behavior change and 53:54 this is probably the most effective way 53:55 to achieve behavior change in 53:57 agriculture at least that's what we 53:58 suspect today 53:59 uh by enabling that gradual behavior 54:01 change which can also be unlocked by 54:03 this 54:03 these kinds of ais you're able to ease 54:06 them ease the farmers into new 54:07 technologies without it being an 54:09 aggressive 54:10 uh change that they're hesitant to adopt 54:14 so before i conclude here i just want to 54:15 leave you with one last parting thought 54:17 is all this is only possible if you have 54:20 some kind of training data 54:21 on which you can build machine learning 54:23 models so where does this data come from 54:26 uh there's a thought that i want to 54:28 leave you with uh here 54:30 because it's the most important part as 54:31 part as with any ai you need data to 54:33 make things happen 54:35 there's many creative data sets out 54:37 there though there's many 54:38 hacky data sets if you made like you can 54:40 put together a data set from multiple 54:41 different sources you have a very 54:42 interesting data set 54:43 that can then unlock new value but where 54:46 does data come from that's a crucial 54:47 question 54:49 and thank you for having me here today 54:50 and happy to take any questions if 54:52 there's any time remaining 54:53 here thank you audit and thank you to 54:57 um nourish and stephen as well i think 55:00 we're getting short on time 55:02 um so i just wanted to thank each of our 55:05 speakers 55:07 for their excellent overview i think we 55:09 covered quite a bit of ground 55:10 between the three of you i guess um one 55:13 question i might have is 55:14 as climate change worsens would you 55:16 anticipate that things become 55:18 more unpredictable and if so what is the 55:22 is there time to respond in something 55:24 like a agricultural time scale 55:27 to correct issues if you will and to 55:29 actually 55:30 will those predictions become less 55:32 accurate as the climate worsens 55:34 i guess i could take a quick stab with 55:36 it i guess is uh 55:38 yeah so so the models will get outdated 55:41 over time 55:42 so that that is where you have to 55:43 retrain the models uh 55:45 uh so uh there's many strategies for 55:48 that you could 55:49 completely retrain the models entirely 55:52 all the time like every few years 55:53 or it could be a kind of like uh online 55:56 machine learning as they like to call it 55:58 kind of method if you if you set up the 55:59 models that way to begin with 56:01 is uh from using further inputs from the 56:03 ground it could automatically be getting 56:05 better 56:06 uh with time so that's crucial though 56:08 the models have to 56:09 keep getting updated you can't even 56:11 today like uh 56:13 i wouldn't say it's reasonable to use 56:14 models developed in the 1980s 56:16 for doing work today already so 56:19 things change does happen but sort of 56:22 the good thing is that 56:23 climate change is slightly longer time 56:25 scale event it's not something that 56:26 happens like overnight 56:28 so the model that we developed today can 56:31 probably be used definitely for this 56:32 season 56:33 for the next season maybe even or even 56:35 for the next season after that 56:36 then of course there's probably already 56:38 better data sources out there so you're 56:40 anyway going to retrain the model but 56:41 it's very crucial to 56:43 a retrain every several years you can't 56:45 use the same thing for 56:47 100 years very much i wish we had more 56:50 time 56:51 the talks were very interesting and the 56:54 luckily 56:55 a video recording of this will be 56:57 available on the umaine ai webinar 56:59 website 57:00 and with that i'd like to speak all and 57:02 thank again all of the speakers and also 57:04 to the attendees for joining us 57:06 and as a reminder the next ai webinar in 57:09 our series aim healthcare 57:10 is scheduled for thursday may 6 from 12 57:14 to 1 pm eastern time 57:16 and with that thank you all for your 57:17 attendance and i hope everyone has a 57:18 good day      The University of Maine in Orono is the flagship campus of the University of Maine System, where efforts toward racial equity are ongoing, as is the commitment to facing a complicated and not always just institutional history. The University recognizes that it is located on Marsh Island in the homeland of the Penobscot nation, where issues of water and its territorial rights, and encroachment upon sacred sites, are ongoing. Penobscot homeland is connected to the other Wabanaki Tribal Nations \u2014 the Passamaquoddy, Maliseet, and Micmac \u2014 through kinship, alliances, and diplomacy. The university also recognizes that the Penobscot Nation and the other Wabanaki Tribal Nations are distinct, sovereign, legal and political entities with their own powers of self-governance and self-determination. ",
        "title": "Artificial Intelligence for Agriculture",
        "citationCount": 0
    },
    {
        "content": "Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 201 APPLICATION OF ARTIFICIAL INTELLIGENCE IN TRANSPORTATION DEMAND MANAGEMENT: DEVELOPMENT AND IMPLEMENTATION OF E-SUTRA   Resdiansyah Department of Civil Engineering  SekolahTinggi Teknologi Nasional Yogyakarta and University of Malaysia  Kota Samarahan, 94300 Sarawak, East Malaysia Telp. (+60) 82-583222 mresdiansyah@feng.unimas.my Ircham Department of Civil Engineering Sekolah Tinggi Teknologi Nasional Yogyakarta Jln. Babarsari  Daerah Istimewa Yogyakarta irc276@yahoo.co.id   Otto Sandjoko Department of Civil Engineering  Sekolah Tinggi Teknologi Nasional Yogyakarta Jln. Babarsari Daerah Istimewa Yogyakarta otto@yahoo.com  Abstract  Allowing traffic to grow to a level at which there is extensive and regular congestion is economically inefficient. Although the construction of additional roads can alleviate some of the effects of congestion, the benefits may be counterbalanced unless the growth in traffic volumes can be restrained. Therefore, another alternative is by implementing Transportation Demand Management (TDM), which means people still travel but at the same time the private car usage is reduced. This paper presents the development of an expert system for sustainable transportation (E-SUTRA) through implementation of TDM. The overall result of 69% accuracy indicates the high possibility of the E-SUTRA system to be used as an advisory tool for sustainable transportation through TDM.   Key Words: expert system, transportation demand management, sustainable transportation.   Abstrak  Lalulintas yang dibiarkan tumbuh dengan bebas sehingga menyebabkan kemacetan yang luas dan terus menerus secara ekonomi tidak efisien. Meskipun menambah jaringan jalan dapat mengurangi beberapa akibat kemacetan, manfaat itu dapat dirasakan bila pertumbuhan volume lalulintas dapat dikendalikan. Oleh karena itu, alternatif lain adalah dengan menerapkan manajemen permintaan transportasi, yang berarti orang masih dapat melakukan perjalanan tetapi pada saat yang sama penggunaan mobil pribadi dibatasi. Makalah ini menyajikan pengembangan sistem pakar untuk transportasi yang berkelanjutan, yaitu E-SUTRA, melalui penerapan prinsip manajemen permintaan transportasi. Hasil dengan tingkat akurasi 69% menunjukkan peluang sistem E-SUTRA untuk digunakan sebagai alat bantu dalam perencanaan transportasi berkelanjutan melalui manajemen permintaan transportasi cukup tinggi.  Kata-kata kunci: sistem pakar, manajemen permintaan transportasi, transportasi berkelanjutan.  INTRODUCTION  The demand of travelling increases basing on the assumption that private cars are the best or the only solution to mobility. The supply of infrastructure increases proportionally to traffic volume. At certain point, the supply cannot meet the demand due to certain factors such as budget constraints, land, or space issues (Litman, 2009). Today congestion and traffic-related pollution have increasingly become the major problems in towns and cities worldwide. According to the Kuala Lumpur Structure Plan 2020, released by \u201cDewan Bandaraya Kuala Lumpur\u201d, the number of private cars is rising dramatically. Between 1985 and 1997 the modal share of public transport decreased from 34.3 percent to 202 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 19.7 percent. This represents a major shift away from public transport and in particular bus transport; this phenomenon is partly attributable to higher personal affluence leading to an increase in car ownership and to deficiencies in the bus services. Kuala Lumpur City Council (DBKL) stated in its report on Kuala Lumpur Structure Plan 2020 that if the current trends continue, motorised trips by car are expected to almost double in 2020 compared with those of 1997. Increasing road capacity by constructing new roads and widening existing roads do not, in the long run, resolve the traffic situation but simply postpone the problem until more roads need to be built. Most areas in the city, especially the central business districts, are now densely developed with commercial buildings; land acquisitions for road construction are increasingly cumbersome and expensive. Forecasts suggest that, without any action taken, the number of private cars will continue to grow substantially over the next 25 years, adding more vehicles to our roads. As traffic grows road congestions worsen and are more widespread; and they occur for longer periods in a day. Once the traffic passes the capacity limit of a particular road or network, incidences of congestion will increase, and small volumes of additional traffic will have a disproportionate adverse effect on the network. If traffic is allowed to grow unchecked, road congestion will worsen, and in 25 years\u2019 time it will incur much higher social, economic and environmental costs than now. Provision of more road space to keep pace with traffic growth is not the answer. It is far too expensive and socially disruptive, and will exacerbate the long-term problems the relevant authorities try to tackle. Some people argue that traffic should find its own level, thus forcing drivers to find alternative ways of travel, or not to travel at all. However, it is believed that this would mean abdicating the economic, social, and environmental responsibilities. In the meantime, the countryside and urban areas would both suffer significant deterioration. The only realistic option is to seek ways of first controlling, then reversing the growth in traffic. This procedure needs to be carried out in the most equitable way, taking into consideration that certain segments of the society have limited choices in the mode of transportation. In the last few decades, traffic engineers and practitioners had come to the conclusion that traffic problems can be overcome only through the introduction of appropriate TDM measures rather than through the provision of new highway infrastructure (Papaioannou and Georgiou, 2003). The implementation of TDM requires experts or professional advisers because with their specialised knowledge, they can give advice on the most suitable TDM strategies based on the purposes and locational details.  The downside is that the number of these traffic experts is very small; they are not available most of the time for consultation and to give advice on traffic data. This shortage of traffic professionals causes time wastage in rescheduling appointments and interviews, leading to delay of the whole project.    METHODOLOGY  Expert systems provide the advices and assistance for solving problems that are normally undertaken by human experts. According to Aronson and Turban (2001), expert Application of Artificial Intelligence (Resdiansyah, Ircham, dan Otto Sandjoko) 203 systems are problem-solving packages that mimic a human expert in a special area. These systems are constructed by eliciting knowledge from human experts and coding it into a form that can be used by a computer in the evaluation of alternative solutions to problems within that domain of expertise as shown in Figure 1.     Figure 1 Artificial Intelligence in Expert System Application  The computer tool used to develop the expert system in this research is by using open source software called Kappa-PC version 2.4. The first step is to create a hierarchy from the TDM strategies towards specific objectives, main purpose, choice of area and the final three advices respectively. Kappa-PC provides a developer with the flexibility and power required in supporting complex applications (Intellicorp. Inc., 1997). It allows a knowledge engineer to develop a prototype using the hybrid knowledge representation technique.   RESULT AND ANALYSIS  System Architecture of E-SUTRA E-SUTRA is developed for young and inexperienced transport engineers and planners, who are involved in the strategy planning and implementation of transportation demand management (TDM). Such expert system is capable of providing them with reliable advices in order to effectively determine TDM strategies during the planning and implementation of project phases. Figure 2 represents the top-level hierarchical architecture of the object model developed for the E-SUTRA domain. The class of E-SUTRA represents the overall prototype that comprises nine major subclasses. The module names in Figure 2 are boxed to indicate that their subclasses are not shown. There are a total of 27 hierarchies in subclasses according to the main objective of TDM Strategies.  Illustration Example of the module in E-SUTRA  As an illustration of a typical object representation in the E-SUTRA domain, the hierarchy of Crimproved transportoption class, which is a descendant of the Congestion Reduction class shown in Figure 1. CRITO Large Urban class, is a geographical area entity that represents appropriate condition of the area where implementation of TDM 204 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 strategies is needed. It contains the slots: (i) PublicTransportImprovement1, which indicates the strategy on Public Transit (also called Mass-Transit), including various services using shared vehicles to provide mobility to the public; (ii) NonMotorTransportPlan1, which indicates Non-Motorised Transportation (also known as Active Transportation and Human Powered Transportation), including walking and bicycling, and variants such as Small-Wheeled Transport (skates, skateboards, push scooters, and hand carts) and Wheelchair travel; (iii) Ridesharing1, which refers to carpooling and vanpooling (the term is sometimes applied to public transit, particularly commuter express bus), in which vehicles carry additional passengers. Carpooling uses participants\u2019 own automobiles. Vanpooling usually uses rented vans (often supplied by employers, non-profit organisations or government agencies). Most vanpools are self-supporting, in which the operating costs are divided among members. Vanpooling is particularly suitable for longer-distance commutes. These slots are inherited from the parent class Crimproved transport option, which is a descendant of the Congestion Reduction class.    Figure2 Hierarchical Architecture of E-SUTRA  Knowledge Based Model of E-SUTRA The E-SUTRA knowledge base is made up of nine class modules as mentioned in the previous section, each of which deals with a specific strategy of the TDM domain. These are the steps necessary to launch the E-SUTRA expert system; first, open the Kappa-PC software, then open the E-SUTRA (.kal), and continue by opening the session to start the system. After the program is loaded, the general information interface appears (Figure 3) in the centre of the display area if the computer has resolution of 1366 x 768 pixels (32 bits).  Application of Artificial Intelligence (Resdiansyah, Ircham, dan Otto Sandjoko) 205   Figure 3 General Information Interface  Clicking on the OK button closes the general information interface and invokes the main E-SUTRA window, as shown in Figure 4. Two-way communications between user and E-SUTRA have been started in this interface. Figure 3 shows the E-SUTRA main menu from which the expert system for TDM can be accessed with the help of appropriate buttons. The main menu screen has e-library and ABOUT buttons to give information on all TDM strategies used in this system.    Figure 4 Graphical User Interface-Main Window  Expert System Module Since the E-SUTRA system used in this research is based on the \u201cobjective to implement of TDM\u201d as the input, clicking on the ADVISORY SYSTEM  button leads the user to nine main objectives of implementing TDM, as shown in Figure 5.  206 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212   Figure 5 TDM Main Objective Interface  There are nine main objectives associated with this interface as defined inputs. By selecting one that matches the problem encountered in the current situation, the user can see the full description about the objective. In this example, Congestion reduction has been selected. As a result, the description of this TDM strategy appears on the specific box allocated. If the user is certain of his intention and confirms the input (correct main objective), clicking on the NEXT button will call up the viewing window that displays the \u201cSpecific Aim to Achieve the Main Objective Interface\u201d, as shown in Figure 6.     Figure 6 TDM Specific Aim Interface   Application of Artificial Intelligence (Resdiansyah, Ircham, dan Otto Sandjoko) 207  Figure 7 Geographical Area Interface  This dialogue window (Figure 6) is designed to ensure the user chooses appropriate input to avoid wrong advice in appropriate strategy. In this example, the user chooses Improved Transport Options as the specific aim to achieve the main objective, which refers to the quantity and quality of accessibility options available to an individual or group, taking into account their specific needs and abilities. If the user is certain of his intention and he or she confirms the selection, clicking on the NEXT button will call up the viewing window that displays the Geographical Area interface, as shown in Figure 7.  After selecting the specific objective, the size of study area or the areas that need implementation of TDM are listed on the left-hand side of interface window. In this example, the user has selected \u201cUrban\u201d as the size of study area. By clicking on the NEXT button, the user calls up the viewing window that displays Advised TDM Strategies (Figure 8).     Figure 8 Advised TDM Strategies Result Interface 208 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 It displays three appropriate strategies which are the most suitable TDM approaches to be implemented based on the input selection. The system suggests three strategies for the user to consider and make a decision to choose a suitable TDM strategy. Descriptions of all three strategies are provided in the dialogue box to help and guide the user. The user can click the VIEW IMPACT button and Advised Strategy Impact Summary will be displayed on the interface, as shown in Figure 9.  As an example, Figure 9 shows the advised strategy for the \u201cnon-motorised\u201d travel. The result shows that the travel impact in terms of total traffic reduction is significant, as rating \u201c2\u201d is shown (beneficial). On the benefit summary, most of the objectives are given rating \u201c3\u201d (very beneficial), which is significant in reducing automobile use, providing affordable mobility and increasing travel choices. The advice strategy summary shows the travel impact, benefit and equity if the strategy chosen is implemented. Based on the travel impact summary, the user can easily identify the effectiveness of a strategy in reducing the number of trips or total traffic. The degree of effectiveness is stated in the summary using a rating scale; the rating ranges from 3 (very beneficial) to -3 (very harmful); 0 indicates no impact or mixed impact on the objective if the user implements this strategy.   Figure 9 Impact Summary Interfaces  Validation and Evaluation (V and E) Based on the judgment, the overall performance of E-SUTRA is comprehensive and satisfactory; the evaluator who is an expert in transportation field concurs with the author\u2019s view. Evaluators are independent people (not domain expert) who have the same qualification as the domain expert.   Validation Typically, according to O\u2019Keefe (1993), the performance of an expert system can be validated by running a number of test cases through the system; the results of the system Application of Artificial Intelligence (Resdiansyah, Ircham, dan Otto Sandjoko) 209 are then compared with either known results or expert opinions. Therefore, the validation process for the E-SUTRA system is carried out by testing the system and comparing the results with the views of a human expert. The performance of E-SUTRA was validated through experimental design using scenario cases of enquirers, which covered all sub-modules in the prototype system. Opinions of more than ten transportation experts were involved in this validation exercise. The purpose of this validation exercise was to gauge the accuracy of the system\u2019s knowledge base. In the validation process, the success rate of the system is calculated in terms of percentage. The final validation results demonstrate the performance of E-SUTRA is quite close to that of the experts; 72% of decisions and advices provided by the human experts match the output of the E-SUTRA system, as shown in the example given in Table 1. This is a positive indication of the accuracy of the knowledge base of the E-SUTRA system. Table 1 shows the matrix approach to measure the accuracy of the E-ASSIST system. As mentioned earlier, the results of this example are significant as 72% of the opinions of expert evaluators match the output of the E-ASSIST module in terms of advices generated.  Table 1 Example of TDM Strategy Advice: Comparison Between E-ASSIST Results and Human Expert Decision (Matrix System)\u2013Submodule: Congestion Reduction-Improved Transport Option   210 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 Table 2 Summary of Percentage Matched between E-SUTRA and Expert Evaluators   ITO UALT LUM Congestion reduction 72% 65% 69% Energy conservation 67% 75% 70% Health and fitness 67% 66% 61% Improving equity 78% 67% 63% Livability strategy 65% 65% 66% Parking solutions 76% 77% 68% Rural community 75% 64% 61% Safety strategy 65% 73% 77% Transport affordability 78% 80% 66%   Average: 69% ITO : Improved Transport Option   UALT: Use Alternative Modes & Reduce Driving  LU: Land Use Management     The system generated a total of 648 (9x3x8x3) strategies based on the nine objective modules, three specific aims, eight geographical areas and three TDM advice strategies; the percentages of accuracy validated based on all the outputs from the system are as shown in Table 2. Based on all the aspects discussed in this paper, it is clear that E-SUTRA has met the conditions required for its application as an expert system for sustainable transportation through transportation demand management. It can be used as a tool for advising junior engineers in order to make decisions on appropriate TDM strategies to be implemented in Malaysia. The validation results show that more than 60% or exactly 69% of decisions and advices outputted by the E-SUTRA system match those of expert evaluators who are transport specialists. Since the E-SUTRA system is a representation or an abstraction of human expertise, perfect performance cannot be expected (O\u2019Keefe 1993). The result of 69% accuracy has exceeded the expected level of performance of the system.  From the perspective of users, the acceptable percentage accuracy of performance for a system is called the acceptable range of performance, and this should be specified at some stage of development (O\u2019Keefe 1993). It can be certified by a third party, a government agency or the project sponsor. In this case, the acceptable range of performance has been specified by a Malaysian government authority (DBKL) with a range of 50-60% during the knowledge acquisition process. The overall result of 69% accuracy indicates the high possibility of the E-SUTRA system to be used as an advisory tool for sustainable transportation through transport demand management. Thus, we are proud and happy to say that the E-SUTRA system has accomplished the purpose for which it was designed.  Evaluation As mentioned by Wentworth (1995), evaluation of an expert system is reflected by the acceptance of the system by its end users as well as the performance of the system in its application. Generally, the flow of the consultation sessions or advice sessions between the user and the E-SUTRA system was satisfactory. The user was able to learn every single description in the TDM strategy and could obtain recommendations given by the system. Application of Artificial Intelligence (Resdiansyah, Ircham, dan Otto Sandjoko) 211 The system allowed the user to view the impact of results before making decisions on the appropriate TDM strategies to be implemented in particular situations based on the input selected. The user-friendliness of E-SUTRA was assessed to be satisfactory as a working prototype. Furthermore, the main screen of E-SUTRA, which consists of several buttons with text images, has been evaluated as user-friendly by most of the evaluators. The evaluation form recorded \u201cgood\u201d in the following items assessed: \u201creadable\u201d, \u201cvisually pleasing\u201d and \u201cattention getting\u201d. Only one parameter, \u201cconsistency of format\u201d was evaluated as \u201cfair\u201d since every sub-module interface has a different pattern and layout. In addition, the evaluators concluded that the system has adequately met its objectives, it is free from bugs and the program runs properly. The most significant discovery was that the evaluators noted the system uses appropriate strategy in order to build authentic scenarios based on the input selection. Finally, the results of the evaluation form show that the features and interface of the E-SUTRA system are acceptable to the transportation experts and the end users.   CONCLUSION  The E-SUTRA system is an example of successful development of an expert system for sustainable transportation, particularly in the area of implementations of TDM. This research project demonstrates that the application of knowledge-based expert system in this domain holds a lot of potential and promises. The method of frame-based knowledge representation coupled with the power of object-oriented programming enables the knowledge to be abstracted and represented as codes; the computer can manipulate the information efficiently and the knowledge engineers can update the knowledge base with great flexibility. Since the knowledge, techniques and strategies in the transportation arena are dynamically growing, the E-SUTRA system is by no means complete at this stage. There is certainly room for future studies.   REFERENCES   Dewan Bandaraya Kuala Lumpur. 2012. Kuala Lumpur Structure Plan 2020 Report. Kuala Lumpur. Frame, J. 2010. Traffic Management and Transport Demand Management. Distance Learning Course in Urban Planning, Chapter 4, pp. 1-19. IntelliCorp, Inc. 1997. Kappa-PC Ver. 2.4 User Manual. CA: IntelliCorp Inc. Litman. T. 2009. Are Vehicle Travel Reduction Targets Justified? Evaluating Mobility Management Policy Objectives Such as Targets to Reduce VMT and Increase Use of Alternative Modes. Victoria Transport Policy Institute.  212 Jurnal Transportasi Vol. 13 No. 3 Desember 2013: 201-212 O\u2019Keefe. R.M. and O\u2019Leary, D.E. 1993. Performing and Managing Expert System Validation. Journal of Advances in Expert Systems for Management, 1 (1): 141-176.  Papaioannou, P. and Georgiou, J. 2003. Are Traffic Demand Management (TDM) Measures A Solution for Traffic Problems in Urban Areas? The Greek Experience. Transport and Management Section, Department of Civil Engineering, Aristotle University of Thessaloniki. Thessaloniki. Turban, E. and Aronson, J. 2001. Decision Support Systems and Intelligent Systems. 5th Edition. NJ: Prentice-Hall, Inc. Inglewood Cliffs. Wentworth, J.A. 1995. Verification, Validation & Evaluation of Expert Systems. Volume 1, First Edition, A FHWA Handbook. Washington, DC. ",
        "title": "Application of Artificial Intelligence in Transportation Demand Management: Development and Implementation of E-sutra",
        "citationCount": 0
    },
    {
        "content": "\u9078\u9271\u88fd\u932c\u7814\u7a76\u4f1a\u8a18\u4e8b\n\u96d1\u8a8c\u540d \u6771\u5317\u5927\u5b78\u9078\u945b\u88fd\u932c\u7814\u7a76\u6240\u5f59\u5831\n\u5dfb 30\n\u53f7 2\n\u30da\u30fc\u30b8 126-136\n\u767a\u884c\u5e74 1975-03-25\nURL http://hdl.handle.net/10097/00114211\n\n\n\n\n\n\n\n\n\n\n\n",
        "title": "\u9078\u9271\u88fd\u932c\u7814\u7a76\u4f1a\u8a18\u4e8b",
        "citationCount": 0
    },
    {
        "content": "\u2018Journeys in the Dark\u2019 - Towards Game Master AI in Complex Board GamesToby Best1, Raluca Gaina1, Simon Lucas11Game AI Group, Queen Mary University of London, UKt.j.best@qmul.ac.uk, r.d.gaina@qmul.ac.uk, simon.lucas@qmul.ac.ukAbstractThe Game Master is a player role synonymous with manytabletop games. The asymmetric gameplay of the role pro-vides different opportunities compared to other players, andcan be both cooperative and competitive with the other play-ers in the same game. Though complex environments for ex-ploring human and Artificial Intelligence collaboration ex-ist, few focus on the Game Master role\u2019s semi-cooperativeplay. Here, we propose a new complex environment basedon the board game \u2018Descent: Journeys in the Dark (SecondEdition)\u2019, as part of the Tabletop Games Framework, show-casing one-versus-many play, tactical combat, and large, dy-namic action and state spaces. We include baseline AI playerperformance of Monte Carlo Tree Search agents in this game,finding them to be well-adept at considering multiple possi-ble end-game conditions compared to the greedy One StepLook Ahead agents. In-depth analysis reveals interesting be-haviours and Hero synergies, with the aim of informing thedesign of games and AI models to enhance human experiencein semi-cooperative environments.1 IntroductionWithin many board games and video games, participants areexpected to play equal roles. In these symmetrical environ-ments, players have the same opportunities as one another,whether that be in competition by eliminating opponentsfrom play, or collaborating to achieve a common goal.A subset of tabletop games, known as tabletop role-playing games (TTRPGs), are by contrast one-versus-manyasymmetric environments, driven by the power of collectivestorytelling and interactive narrative formed by the players(Riedl and Bulitko 2013). One player, in the role of GameMaster (GM), acts as both narrator and rules referee to guidethe other players throughout the game (Tychsen et al. 2005).A GM may have to balance playing both sides of the gamesimultaneously, taking control of the antagonistic forces aswell as any helpful allies who may assist the other players\u2019characters during play. There is a level of cooperation ex-pected between players, particularly those on the same side,whilst competition still remains between the opposing sides.Though there have been prior explorations of the use ofArtificial Intelligence (AI) agents to play TTRPGs (Martin,Copyright \u00a9 2024, Association for the Advancement of ArtificialIntelligence (www.aaai.org). All rights reserved.Sood, and Riedl 2018), the research on virtual environmentspurpose-built for such agents appears rather limited. Addi-tionally, though many TTRPGs may appear to be easy tolearn, there is a detailed level of forward planning required tosucceed. The complexity and interconnectivity of the rulesand the player\u2019s available actions (Shyne 2023) makes it lessstraightforward for agents to plan their turns when comparedto other games, which may lead to unpredictable game out-comes and higher computational costs as a result.From this, we propose a new TTRPG-like environmentfor Game AI methods, based on the game \u2018Descent: Jour-neys in the Dark (Second Edition)\u2019 (D2e) (Fantasy FlightPublishing, Inc. 2012). Although \u2018Descent\u2019 is closer to acompetitive board game than a traditional TTRPG such as\u2018Dungeons & Dragons\u2019 (D&D) (Gygax and Arneson 1974)or \u2018Call of Cthulhu\u2019 (Press 2017), it possesses more ofa minimalistic, strategic and highly structured view of aTTRPG environment. A team of players take the role ofadventuring Heroes, battling against the Overlord, a soloplayer who controls an army of Monsters to directly com-pete against the Heroes.As such, we believe it has suitable ludic elements, rulesand themes that would make the implementation of suchan environment worth pursuing. Thus, we construct D2ewithin the Tabletop Games Framework (TAG) (Gaina et al.2020a,b), combining several challenges for AI players: one-versus-many play, tactical combat, large and complex stateand action spaces, strategic multi-layered decisions throughdifferent abilities and card effects, and Game Master AI.The contributions of this paper are as follows: we detailthe properties of the new digital D2e environment imple-mented within TAG, and initial experimental results measur-ing the baseline performance of two AI players fighting towin: a greedy short One Step Look Ahead and Monte CarloTree Search (Browne et al. 2012), compared against publiclyavailable human player results. Furthermore, we perform adetailed analysis of AI player behaviour and emergent Herosynergies. We discuss challenges and opportunities created,with the aim of informing the design choices for the gameand AI models used to play it, and enhance human experi-ence in semi-cooperative environments.We intend for this to serve as a basis for exploring AIagent behaviours within a more TTRPG-like virtual envi-ronment for future research, with a potential roadmap laidFigure 1: Our intended roadmap for future developments to-wards Game Master AI. There are three overall sets that en-compass each stage - combat management, player improvi-sation and role-play, and procedural content generation. Thecurrent implementation of D2e allows for explorations of AIagents partaking in tactical combat (highlighted).out in Figure 1. These development intentions include: al-lowing for the Game Master agent to model players\u2019 perfor-mances and adjust difficulty based on player engagement,generating narrative information to reflect players\u2019 actionsand the state of the game (Zhu et al. 2023), exploring be-lievable behaviours for non-player characters (Park et al.2023), and the potential of procedural content generation(as previously explored within \u2018Gloomhaven\u2019 (Tijben 2023;Gerhold and Tijben 2023)). We hope these developmentswill progress towards improving gameplay accessibility inTTRPGs, by form of either assistance tools to help new orinexperienced GMs with running their own games (Santi-ago III et al. 2023), or to substitute for a human GM if one isunavailable (such as in solo play or in small player groups).2 Literature ReviewThere has been much research into both competitive and co-operative complex game environments, using Artificial In-telligence (AI) agents within board, card, tabletop and videogames to optimise tactics and strategies, and help expandupon players\u2019 understanding of the game. For example, thiscan be seen with DeepMind\u2019s \u2018AlphaGo\u2019 (Silver et al. 2016),\u2018AlphaStar\u2019 (Vinyals et al. 2019) and \u2018AlphaZero\u2019 (Silveret al. 2017) agents, which could compete at the same level asprofessional human players. Refinements of human playerstrategies were found as a result of adaptations to these AIagents (Egri-Nagy and To\u0308rma\u0308nen 2020; Shin, Kim, and Kim2021), with \u2018StarCraft II\u2019 (Vinyals et al. 2019; Samvelyanet al. 2019) and \u2018MicroRTS\u2019 (Ling et al. 2022) in particularstanding out for their tactical play aspect and multi-unit con-trol. The emergence of AI players and assistance tools thatdirectly work within the game engine have allowed for thesegames to become better understood, and therefore have hadan impact upon the play experience. However, this has alsolead to some unexpected, and potentially negative, develop-ments to occur, such as cheating in games becoming moreaccessible (Shin, Kim, and Kim 2020).AI implementations of modern board games have alsohelped with analysing game mechanics, such as for creat-ing tutorials on how to play (Assembly-Horsham, Magh-sudi, and Spronck 2023), or providing better understandingof common ludemes found in board games (Sousa 2023)and role-playing games (Riel and Monahan 2024). The im-plementation of \u2018Terraforming Mars\u2019 within TAG (Gaina,Goodman, and Perez-Liebana 2021) serves as a recent ex-ample of AI agents being implemented to examine a mod-ern board game, in this case used to study the performance ofvarious agents compared to human players within the game\u2019sexpansive action space. The high quantity of available ac-tions and the uncertainty of their consequences is acknowl-edged to prove difficult to predict future game states, a chal-lenge that is shared with TTRPGs (Martin, Sood, and Riedl2018) due to the collaborative improvisation involved, espe-cially when considering how to construct player agents.For AI agent implementations within cooperative boardgames, there has been recent interest in particular towardshuman-AI cooperation (Ashktorab et al. 2020; Dafoe et al.2020; Chaco\u0301n and Eger 2019; Sfikas and Liapis 2020; Gainaand Balla 2022), as well as studies into multi-agent sys-tems for semi-cooperative environments (Amini and Af-sharchi 2014; Kitchen, McGroarty, and Aris 2023; Bos andStoelinga 2023), such as one-versus-many games. Thesegames may involve one player fighting against a cooperativeteam of others, or players may have hidden goals that influ-ence if they win or lose individually. The hide-and-searchasymmetric game \u2018Scotland Yard\u2019 (Dash et al. 2020) is onesuch case, with research comparing how adversarial neuralnetwork agents adapt to their opposition\u2019s actions. Whereas\u2018Scotland Yard\u2019 pits a team of detective players against asingle adversary, many TTRPGs often give the Game Mas-ter (GM) just as many figures to control as the other players,sometimes more, resulting in many moving parts during playto keep track of.Regarding tabletop games, the role of GM can be seen assemi-cooperative. TTRPGs at their core are interactive nar-ratives (Riedl and Bulitko 2013), with the GM tasked withguiding the other players through the game\u2019s narrative aswell as refereeing the rules and providing obstacles to chal-lenge and oppose them, making the GM just as active a par-ticipant as the other players. The asymmetric gameplay andopen-ended creative nature of TTRPGs make them primecandidates as challenges for AI to solve (Ellis and Hendler2017). Previous research into AI agents within TTRPGs in-clude the Discord bot \u2018Calypso\u2019 (Zhu et al. 2023), the hy-pothetical GM-assistant \u2018Avalon\u2019 (Santiago III et al. 2023),and large language models trained on actual play (Callison-Burch et al. 2022). Meanwhile, a study on generative agentssimulating believable behaviours within non-player charac-ters (Park et al. 2023) highlights the possibility of a singleagent governing actions for multiple characters in TTRPGs,for players to interact with in combat and social encounters.3 Descent\u2018Descent: Journeys in the Dark\u2019 (Fantasy Flight Publishing,Inc. 2012) is a fantasy dungeon-crawler board game pub-lished by Fantasy Flight Games for 2 to 5 players. Releasedoriginally in 2005, with various expansions, campaigns andnew quests added over the years, a simplified Second Edition(2e) was released in 2012, which this study focuses on. Here,one player becomes the Overlord, a Game Master role whotakes control of an army of Monsters to oppose the otherplayers\u2019 team of Heroes. The game is asymmetrically one-versus-many in format - the Heroes work as a team, whereasthe Overlord plays individually, and both sides have theirown victory conditions in order to win. We only discuss el-ements of the base game and the quests from its campaignHeirs of Blood1 in this paper. Full rules can be found online2.The playable Heroes and Monsters are represented bycards with the figure\u2019s personal abilities and stats (see Figure2), which are as follows: Speed, how many spaces they canmove per action; Health, how much damage they can takebefore they are defeated; Stamina, how many times they canspend Fatigue to power certain actions (for Heroes only);Defense, the dice used for a defence roll; and Attack, thedice used for an attack roll (for Heroes, this is determined bytheir equipped weapon). A Hero\u2019s class abilities and equip-ment are represented via a personal deck of cards, and eachcard can be exhausted (either temporarily or permanently)to use as actions, and some usable item cards, like armourcards, can grant stat bonuses whilst active.Turns alternate between the Heroes and the Overlord.When a Figure is activated on its turn, it gains two actionpoints to spend on any combination of legal actions. Theseinclude: Move up to its Speed to an unoccupied space; makean Attack against an enemy (either Melee or Ranged, deter-mined by the Hero\u2019s weapon or the Monster\u2019s Attack Type);use a specific Skill or Monster Action; Open/Close a Door,or any other Special actions defined by the quest. With theexception of Move, which can be split up by another actionand continued afterwards, all actions taken must be fully ex-ecuted before the next can be chosen, although another fig-ure\u2019s abilities can interrupt its results. We discuss this furtherin Section 4.1.3.1 Playing as the HeroesAt the start of play, each Hero player chooses an Archetype,Class and Hero. There are four Archetypes: Healer (colour-coded as blue in-game), Mage (yellow), Scout (green) andWarrior (red). Each class determines the Hero\u2019s startingequipment and skills, and a Hero can be any class of theirArchetype. An Archetype may only be represented once inthe same game. In the base edition used in this work, thereare eight classes and Heroes, each Archetype matching twoof each. A minimum of two Heroes must be present in agame, so if there are only two players, one player controlstwo Heroes, yet treats them as acting individually.In addition to the their stats, Heroes have four Attributevalues: Might, Knowledge, Willpower and Awareness, whichare used in Attribute tests to determine outcomes of a certainskill, action or card activation. A test is passed if the Hero\u2019sdefence roll is lower or equal to that Attribute. Each individ-1https://kupdf.net/download/heirs-of-blood\\5912ca4fdc0d60a36f959ecc\\ pdf2https://images-cdn.fantasyflightgames.com/ffg\\ content/descent-second-ed/support/DJ01\\ Rulebook\\ ENG.pdfFigure 2: Figure cards for the Warrior Hero \u2018Syndrael\u2019 (left)and the Monster \u2018Barghest\u2019 (right). Syndrael\u2019s Attributes,Hero Ability and Heroic Feat are listed alongside her stats.The Barghest Minion\u2019s stats and abilities are listed at the top,whilst the Barghest Master\u2019s are listed at the bottom.ual Hero also has their own unique Hero Ability: a specialclause that can change how certain actions resolve (e.g. al-lowing a once-per-turn reroll of a die when you attack, orconverting damage suffered from attacks into Fatigue); anda unique Heroic Feat: a once-per-quest action that can turnthe tide of battle in their favour (e.g. a free Attack action).On the Heroes\u2019 turn, players may choose to activate theirHero in any order, which they may freely change each turn.The class skill cards provided to the Hero add to their avail-able options on their turn. Some skills are performed byexhausting the card or by spending Fatigue (or both), andsome do not cost action points. Heroes can also spend actionpoints on the following actions: Rest to recover all Fatigueat the end of their turn, Search an adjacent token, or Revivea defeated adjacent Hero. Defeated Heroes are not removedfrom play, but can only take the Stand Up action on theirturn, ending their turn immediately. Also, when only twoHeroes are present, each Hero also gains a free bonus action,to use only for an extra Attack or to recover 2 hit points.3.2 Playing as the OverlordThe Overlord takes control of an army of Monsters, whichcome in three variants: Minion, Master and Lieutenant(unique boss-type Monsters with Attributes). Unlike Heroes,a Monster\u2019s Attack or Defense stats are unaffected by equip-ment, and they do not have a Stamina stat. Most also lackAttributes, and will fail any Attribute test if so. A Monsterwho is defeated is removed from the board.Minions and masters make up the most common Mon-sters. Masters often have better stats than minions, and manyhave skills and abilities that minions cannot access. A Mon-ster Group composes all the minions and the master of thesame monster type: for example, a Goblin Archers groupmay contain four minions and the master, whereas an Ele-mentals group might contain only the master. The numberof minions in a group depends on how many Heroes are inplay, though only one master can typically exist at a time.On the Overlord\u2019s turn, they can activate each MonsterGroup and its Monsters in any order, however all MonstersFigure 3: The current Graphical User Interface (GUI) for\u2018Descent\u2019 in the TAG Framework. A human player is choos-ing the action of the Warrior Hero \u2018Grisban the Thirsty\u2019.in a group must end turn before the next group can acti-vate. Monsters can only make one Attack or Monster Actioneach turn, and cannot pick both in the same turn. The uniqueMonster Actions for each Monster are defined on their card,such as the Barghest\u2019s Howl causing a Willpower test for allHeroes within three spaces, suffering 1 Fatigue on a failure.4 Methods4.1 Implementation of DescentThe current version of \u2018Descent\u2019 implemented in TAG (seeFigure 3) fully supports the first quest of the Heirs of Bloodcampaign, Acolyte of Saradyn. The Monsters encountered inthis quest are Goblin Archers and Barghests. The Overlordcan respawn up to two Goblins at the end of their turn to re-place any that have been defeated, placing them in Tile \u20184A\u2019at the top-right of the board (see Figure 4). There are threewin conditions available for this quest: the Heroes defeat allthe Barghests (Heroes victory); the Overlord defeats all theHeroes (Overlord victory); or the Overlord achieves 7 Fa-tigue tokens (Overlord victory). The Overlord can gain oneFatigue token at the start and at end of each of their turns inwhich they have at least one Goblin in any position withinthe board tile \u20189A\u2019.Currently, the game state is fully observable to all players.The estimated state space within the current implementationof D2e lies roughly around 1030 to 1056, depending upon thenumber of players present. Our player agents experienced amean action space of 6.93 across all games, with a maximumof 67 actions available at one state and a minimum of 1. Theframework took on average 841\u00b5s to create a copy of thestate, 560\u00b5s to advance the forward model to the next state,and 936\u00b5s to compute the player\u2019s available actions.Similar to the Ludii framework (Piette et al. 2020) andthe Options framework within reinforcement learning (Sut-ton, Precup, and Singh 1999), the TAG framework allows usto parameterise the actions and use a sequential model in or-der to keep the immediate action space relatively small andavoid extremely large combinatorial action spaces. Notably,many actions, such as making attacks and Attribute tests,can be interrupted by other figures\u2019 abilities, which can in-troduce additional uncertainties to future game state predic-Figure 4: The game board\u2019s layout for the first quest, Acolyteof Saradyn. The Heroes start at the Entrance \u2018E\u2019 (bottom-left), the Goblin Archers at \u20185A\u2019 (top-left), and the Barghestsat \u20184A\u2019 (top-right). Goblins can earn Fatigue for the Overlordat \u20189A\u2019 (bottom-middle) in order to win. Up to two defeatedGoblins can also respawn anywhere within \u20184A\u2019 each turn.tions. For example, when deciding to make an attack action,the acting figure declares their target and their weapon (ifa Hero). The attack flows through several stages of progres-sion, from rolling the attack dice, to rolling the defence dice,to applying damage, all the while checking for any actionsfrom the other players (including the attacker and defender),which could interrupt and affect the attack. These other ac-tions may include forcing a reroll of a die, adding or sub-tracting to the rolls to influence their results, or spend Surges- a key element of combat that allows the attacker to bolstertheir attack with special bonus effects. If these actions anddecisions were combinatorial instead of sequential, then ouralready large action space would become even greater, andwould negatively affect efficiency when playing the game.4.2 Physical and Implementation DifferencesDue to the nature of the physical version of \u2018Descent\u2019, thereare some notable components that we chose to omit or oth-erwise alter, in order to simplify our implementation to bemore in-line with our desired end goal of exploring AI mod-els in semi-cooperative asymmetric environments similar tothose found in most TTRPGs. As these will impact the playexperience of our virtual environment compared to the orig-inal game version, we detail our changes as follows.For one, the TAG implementation does not include thedeck of Overlord Cards. These are special actions that canhelp the Overlord or hinder the Heroes, such as immediatelyending a Hero\u2019s movement on a failed Awareness test, granta bonus action to a specific Monster, or force a Hero to at-tack another Hero (or themselves). We chose to omit this tostreamline the environment, as otherwise the Overlord wasconstantly interrupting other players\u2019 turns to decide if theywanted to use a card. This also meant that only Conditionpresent in the first quest is Stunned (afflicted figures onlygain 1 action point that turn instead of 2), and can only beinflicted upon Monsters via Healer Archetype Heroes.Also, when making a Ranged Attack, we calculate a fig-ure\u2019s line of sight from the centre of the attacker\u2019s positionto the centre of the target\u2019s position, whereas the originalrules calculate line of sight by comparing the corners of theattacker and the target\u2019s spaces. We also impose a hard max-imum range of 8 spaces away for the attack to be legal at thisstage, as attempting from any greater distance with the avail-able starting equipment would be impossible to succeed.4.3 AI PlayersTo examine potential player behaviours that might occur inour implementation of D2e, we use three automatic playeragents in this study\u2019s experiments: Random, which selectsa random available legal action, One Step Look Ahead(OSLA), which greedily selects the action available thatleads it to the next best state, and Monte Carlo Tree Search(MCTS) (Chaslot et al. 2008; Browne et al. 2012), whichconstructs a tree simulation of several future actions andselects the best possible predicted outcome. The MCTSagent\u2019s parameters, tuned from preliminary experiments tomaximise performance, are set as follows: it has a rolloutlength of 10, an Upper Confidence Bounds exploration con-stant K of 0.01, an explore-\u03f5 of 0.3, and a budget of 300milliseconds to make decisions at each state. The MCTSdecision tree is set to open-loop, due to the random natureof dice rolls involved, and the tree also ignores other play-ers\u2019 potential actions during construction. The Most Aver-age Sampling Technique (MAST) (Bjornsson and Finnsson2009) is set to consider both actions from the rollout phaseand tree phase in our decisions, with a MAST gamma decayof 0. For more on MCTS agent parameters in TAG, pleasesee the Wiki page3.A weighted sum of seven heuristic features is used toevaluate the game states for OSLA and MCTS agents. TheHeroes and Overlord use the same heuristic function, thoughnegated for opposing sides. The features are listed in Ta-ble 1, along with the weights used for all games run. Threefeatures are considered beneficial for the Heroes, whilst theother four are beneficial for the Overlord. If a feature is notbeneficial for an agent\u2019s team, it is negated during summa-tion instead. Both HEROES THREAT and OVERLORDTHREAT use a breadth-first-search for calculating the pre-cise distance between positions. The full methods used forcalculating each heuristic score can be found in the sourcecode4. This includes experiment configurations for ease ofreproducing results.5 Experiments and ResultsOur data is generated from 7150 games run within TAG. Thegames run follow the features described in Section 4.1.3http://www.tabletopgames.ai/wiki/agents/MCTS4https://github.com/GAIGResearch/TabletopGames/tree/descentFigure 5: Heat maps of all figures\u2019 movement for 5-Playerself-play games. The brighter the space, the more often afigure ended its turn there, from most to least visited. ForMCTS, Tiles \u20184A\u2019 and \u20185A\u2019 at the top contain the most fre-quently visited spaces. For OSLA, these are in the bottom-left, near the Heroes\u2019 start and the score zone of Tile \u20189A\u2019.For Random, most movement occurs in the top board tiles,as this is where the Goblin Archers and Barghests spawn.As factors such as the number of figures in play or avail-able actions are affected by player numbers, we initially ran6000 games of self-play (with all players using the sameagent), in batches of 500 at 2-5 players for each agent, for2000 games total per agent. Then, we ran 1150 games ina round-robin tournament, where teams of each agent typewere pitted against each other in 5-Player games. Thoughwe consider all games within our analyses, the followingfigures, tables and statistics primarily reference the 5-Playergames; similar results are observed for other player counts.Each game was set up so Hero players were randomly as-signed their character, with randomised initial figure place-ments (and reinforcement placements) as well. A fixed turnorder was used: Hero 1 always acted first, then 2, 3 and 4,then the Goblin Archer Master and Minions, then lastly theBarghest Master and Minions. Additionally, a timeout wouldbe inflicted if 20 rounds passed with no winner, assigning apenalty loss to both players, to avoid excessive resource us-age in seemingly stalemated situations. Most games lastedno more than an average of 11 rounds for completion, sothis felt like an appropriate limit.One notable challenge encountered was agents decidingto end turns immediately with no other action taken, whichoften led to frequent timeouts. We resolved this by removingEnd Turn from the available action space until no other ac-tions were legal. In actual tabletop play, players may want totactically take no action, or purposefully refuse certain ac-tions, rather than being forced to do so by the environmentand losing player agency as a result. We propose the explo-ration of tactical turn length choices as future work.In the 5-Player self-play games, the MCTS agent tookon average 224 seconds per game, averaging 11 rounds pergame at 20 seconds per round. The OSLA agent on the otherhand took only 3 seconds per game, averaging 8 rounds pergame at 380 milliseconds per round.Table 1: The seven heuristic features used to evaluate game states for MCTS and OSLA agents. The Weight values listed foreach heuristic is taken from the Heroes\u2019 perspective, and are negated when evaluating game states for the Overlord.Heuristic Feature Weight DescriptionHEROES HP + 0.8 The ratio of the Heroes\u2019 total current HP to the Heroes\u2019 total maximum HP.MONSTERS DEFEATED + 0.7 The ratio of the number of Barghests defeated (0 HP) to the total number of Barghests.HEROES THREAT + 0.2 How close the Heroes are to the Barghests.HEROES DEFEATED - 0.7 The ratio of the number of Heroes defeated (0 HP) to the total number of Heroes.MONSTERS THREAT - 0.8 The ratio of the Barghests\u2019 total current HP to the Barghests\u2019 total maximum HP.OVERLORD FATIGUE - 0.7 The Overlord\u2019s current Fatigue divided by their maximum Fatigue (7).OVERLORD THREAT - 0.2 How close the Goblin Archers are to the score zone of \u20189A\u2019.Table 2: Occurrences of Game End Conditions for the 6000self-play games. These are: the Heroes defeat all Bargh-ests (Heroes win); the Overlord achieves maximum Fatigue(Overlord win); the Overlord defeats all Heroes (Overlordwin); or 20 rounds pass with no declared winner (Timeout).Players Condition Met For Game End (500 Games Each)(Agent) Barghests Fatigue Heroes Defeat Timeout5 (MCTS) 30.2% 65.2% 0.0% 4.6%4 (MCTS) 21.4% 76.4% 0.0% 2.2%3 (MCTS) 45.0% 55.0% 0.6% 0.4%2 (MCTS) 49.2% 50.0% 0.0% 0.8%5 (OSLA) 10.2% 89.8% 0.0% 0.0%4 (OSLA) 14.2% 85.8% 0.0% 0.0%3 (OSLA) 45.6% 54.4% 0.0% 0.0%2 (OSLA) 48.8% 51.2% 0.0% 0.0%5 (Random) 0.6% 1.8% 0.0% 97.6%4 (Random) 2.0% 2.8% 0.0% 95.2%3 (Random) 8.6% 2.0% 0.6% 89.4%2 (Random) 9.0% 1.6% 0.0% 89.4%Table 3: Results of 1150 5-Player round-robin games be-tween MCTS, OSLA and Random agents, and the frequencyof the Overlord winning or a Timeout occurring after 20rounds passed. Each pair played roughly 130 games.Overlord Hero AgentsAgent MCTS OSLA RandomOv. Win Timeout Ov. Win Timeout Ov. Win TimeoutMCTS 68.4% 4.3% 66.7% 0% 88.5% 11.5%OSLA 100% 0% 88.2% 0% 99.4% 0.6%Random 0.8% 12.3% 4.5% 16.4% 1.9% 98.1%5.1 AI Agent PerformanceTables 2 and 3 both show how often each end game con-dition occurs across all games. Immediately we notice thehigh frequency of timeouts inflicted upon Random self-playgames. This suggests meaningful progress cannot be ob-tained in \u2018Descent\u2019 through passive play - players must ac-tively participate in order to achieve victory. A few timeoutswere incurred for MCTS self-play games as well, thoughits low occurrence, contrasted by reasonably consistent win-rates for both sides, might hint that MCTS is capable of pur-posefully prolonging games to avoid losing, invoking time-outs via survival rather than an inadequacy to progress.From this, we can see the asymmetric nature of \u2018Descent\u2019in play in the MCTS versus OSLA matchups of Table 3.The MCTS agent had a 66.7% win rate when playing as theOverlord against a team of OSLA Heroes. However, whenOSLA played the Overlord, the MCTS Heroes team did notwin a single match. With how few attacks and kills relate tothe Goblins in Table 4 in OSLA self-play games, combinedwith the high footfall near Tile \u20189A\u2019 in Figure 5, it is plau-sible that, due to the more short-sightedness of the OSLAagent, which only looks at the next best move available, theOSLA Overlord tends to rush the Goblins to the score zonefor a quick win, akin to a \u2018Zerg Rush\u2019 strategy within \u2018Star-Craft II\u2019. This appears to be something that neither MCTSnor OSLA Heroes can keep up with whilst pursuing theirown goals, as their attention may be split towards attackingthe Barghests, and thus may not have enough Attack actionsavailable to defeat the overwhelming number of Goblins. AsMCTS is capable of planning further ahead, it may be con-sidering various strategies available and planning out its bestoptions, representing a more experienced player, whereasOSLA may represent more of a beginner who is taking theeasiest path to victory. There is a need for plan recognitionfor players to win consistently against such strategies, some-thing previously experienced with human-AI player coop-eration within \u2018Pandemic\u2019 (Chaco\u0301n and Eger 2019). An AIagent in such an environment would need to adapt its owntactics, balancing its immediate needs and long-term endgoals, whilst predicting other players\u2019 intended action plans.Though we cannot make a direct comparison, we can lookat these results against those of human players, providedby the community-made \u2018Unofficial Campaign Tracker\u2019 for\u2018Descent\u20195. As of the time of this study, across 1439 gamesof Acolyte of Saradyn, the Heroes were reported to havewon 871 games (60.5%), whereas the Overlord had won 568games (39.5%). As the website is self-reported by the play-ers, and neither clarifies how many are involved per gamenor the achieved victory conditions (for the Overlord), it isworth noting that this is only a rough view of the quest\u2019soutcomes by comparison. However, Table 2 shows a higherlikelihood of an Overlord victory by contrast. Though thedifferences in our virtual environment at this stage may be adetermining factor, another possible hypothesis is that the AIagents have an easier time controlling the Overlord player,as there are two victory conditions as opposed to the Heroes\u2019one, and the Overlord has control of all of their units togetherand thus can plan its moves through all of its units, whereasthe Hero agents need to coordinate their actions together, abehaviour which may not be fully facilitated by the current5http://d2etracker.com/stats quests.phpTable 4: Attacks, kills and Heroic Feat usage per game for each Figure across the 500 5-Player self-play games for both MCTSand OSLA agents. Monsters are the group average. Means rounded to 2 decimal places, turns to nearest integer.Hero Archetype Attacks Per Game Targeted Per Game Kills Per Game Defeats Per Game Games Used Heroic Feat Average Turn Used FeatName / Monster MCTS OSLA MCTS OSLA MCTS OSLA MCTS OSLA MCTS OSLA MCTS OSLAAshrian Healer 5.88 2.20 9.90 1.50 1.72 0.30 0.72 0.02 97.3% 41.6% 2 2Avric Healer 4.60 1.51 8.76 1.38 1.44 0.28 0.39 0.00 99.6% 96.5% 2 1Leoric Mage 10.48 2.70 7.95 1.84 3.11 0.40 0.37 0.04 62.1% 47.7% 5 2Tarha Mage 10.06 1.98 7.70 2.39 2.88 0.24 0.70 0.09 89.8% 27.9% 4 2Jain Scout 10.45 2.69 11.57 1.90 2.69 0.88 1.89 0.13 99.2% 100% 1 0Tomble Scout 10.36 2.67 7.38 0.90 2.82 0.48 0.81 0.03 98.8% 18.4% 0 2Grisban Warrior 5.90 2.96 9.90 2.57 2.98 0.95 0.91 0.04 98.0% 97.3% 3 1Syndrael Warrior 5.56 2.75 12.05 2.93 2.47 0.83 1.57 0.07 99.6% 99.6% 0 0Goblin Monster 6.24 0.55 3.53 0.14 0.49 0.01 1.48 0.08 - - - -Barghest Monster 1.43 1.25 3.52 2.23 0.31 0.50 0.68 0.45 - - - -infrastructure. Again, as shown in \u2018Pandemic\u2019 (Gaina andBalla 2022), coordination amongst agents is crucial yet dif-ficult to achieve, which Table 2 suggests, as the Heroes\u2019 winrates across the 6000 self-play games decreased as the num-ber of participating players increased, as more Hero playerswere required to be considered when planning that turn, andthe Overlord gained more Monsters to challenge the Heroeswith, which both would impacted the teams\u2019 performances.Further research could look more in-depth as to how toimprove Hero play and which obstacles more specificallythese agents are facing with cooperation. This may involvethe examination of other quests present in the Heirs of Bloodcampaign, and how each agent type performs in comparisonto the tracker website\u2019s human player results, when facedwith different board layouts and victory conditions. For ex-ample, the tracker lists the quest Blood Will Tell as favouringthe Overlord, who won 127 games (65.1%) compared to theHeroes\u2019 68 games (34.9%), and so may yield different per-formances for agents compared to the first quest.5.2 Behaviour AnalysisFor the performances of the individual components them-selves, Table 4 lists the average attacks and kills per gameper figure, and Heroic Feat usage. We note that OSLA per-forms fewer attacks per game, however this may be a conse-quence of the faster pace of OSLA games (the average gamelength was approximately 8 full rounds for OSLA, as op-posed to 11 full rounds for MCTS). Many of the Heroes\u2019favourite target to attack for OSLA Heroes are the Bargh-ests, whilst there was a mixture of Goblins and Barghestsinstead for MCTS Heroes. All five Goblins are attacked themost by Widow Tarha for MCTS, which is reasonable as shecan make Ranged attacks from a distance, and her HeroicFeat allows her to target two enemies at once. Meanwhile,the Goblins are attacked the most by Grisban the Thirsty inOSLA games - there are three key game elements that mighthelp explain this decision-making: one, as a Warrior, he canonly make Melee Attacks; two, his Speed stat of 3 is theworst of all Heroes; and three, no mechanics are currentlypresent that allow Heroes extra movement, other than hiscounterpart Warrior Syndrael\u2019s Heroic Feat, who cannot bedeployed in the same quest as him. Thus, we can infer that,as the OSLA players prefer to rush straight for the Bargh-ests, Grisban cannot keep up with their pace, and thus is leftwith the Goblins similarly rushing in the opposite direction.This contrasts with the high usage of Jain and Syndrael\u2019sHeroic Feats, both of which grant extra movement, and thusconsidered highly valuable for both agents (with Jain havinga 100% usage rate in the very first turn for OSLA games).5.3 Hero SynergiesAs each Hero has their own unique strengths and weak-nesses, these can all have an impact on their player\u2019s overallgame experience, particularly if they feel too strong or weak;players may not enjoy having to play a weaker Hero as muchas one who can more easily hold their own, and likewise theOverlord may find it frustrating if their Monsters and obsta-cles are easily disposed of by a notably strong Hero. Bal-ance is just as important an aspect of running a TTRPG asnarrative and role-play are, and the type of analysis from AIgameplay presented here can help adjust the difficulty of agame to better suit all of the players\u2019 needs.The most successful four-Hero team for MCTS in ourself-play results is Ashrian, Tarha, Jain and Syndrael, whowon 48.5% of their games, whilst the worst is Avric, Tarha,Tomble and Grisban at 10.3%. For OSLA, the best team isAshrian, Leoric, Jain and Syndrael at 24.0%, whereas Avric,Tarha, Tomble and Syndrael were the only team with a winrate of 0%, losing every game they played together. Forindividual Heroes, Ashrian performed the best in 5-Playergames for both MCTS (35.3% win rate) and OSLA (11.9%),whereas the worst was Avric for both (24.8% and 8.9%).We can theorise these success rates thus: Ashrian, as aHealer, has access to healing skills that can keep her al-lies alive. However, she has the highest Speed of all heroes(5, joint with Jain Fairwood), and her Hero Ability can in-flict Stunned upon any adjacent Minions for free, effectivelycutting their actions per turn in half. Avric Albright\u2019s HeroAbility and Feat focus on healing allies, rather than denyingMonsters\u2019 actions like his counterpart Ashrian can.Meanwhile, both Jain Fairwood and Syndrael grant ex-tra movement for free with their Heroic Feats, and thuswould be highly valued within the agents\u2019 heuristic scores toswiftly reduce the distance between Heroes and Barghests -Jain notably uses her Feat in her first turn in every game forOSLA agents. Their Hero Abilities are also useful, as Jaincan convert some of the damage she takes from attacks intoFatigue, and Syndrael recovers 2 Fatigue at the end of herturn if she ends her turn without moving, which means shecan use both action points to attack and benefit as a result.We have already noted the flaws with Grisban the Thirstyand his opportunity cost compared to Syndrael, but it isworth acknowledging that his Hero Ability - remove a Con-dition when taking the Rest action - is not effective in ourexperimental setup, as the only Condition available in thegame is applied by his allies onto the Monsters. Tomble Bur-rowell, though not as flawed as Grisban, is not as fast as hiscounterpart Jain (4 Speed versus her 5), has a purely defen-sive Heroic Feat (vanish from the map where he can\u2019t betargeted on one turn, then reappear up to 4 spaces away onthe next turn), and though he too has a defensive Hero Abil-ity, he can only use it if he is adjacent to an ally and requiresstrategic planning to take advantage of, rather than Jain sim-ply having Fatigue available. Lastly, Widow Tarha appearsin the worst teams for both OSLA and MCTS, suggestingmore advanced play is required to play this Hero efficiently.6 Challenges and OpportunitiesThe introduction of the digital \u2018Descent\u2019 TAG environmentpresents several challenges and opportunities of interest.Game-playing AI. The complex game with large stateand action spaces, asymmetric gameplay and many dynamicparts offers direct challenge for AI game-playing researchwithin tabletop role-playing games. As our focus in thisstudy has been on interesting behaviours in the newly im-plemented D2e environment, rather than obtaining the bestpossible behaviours to win, there is still the potential of fu-ture research examining the creation of good AI players. Thefree movement, wide range of actions or abilities the playerscan enact, and dynamic and conflicting goals, as well as thevarious strategies or tactics that can be discovered, allow forfurther study of AI behaviours in complex environments.Adaptive strategies. As discussed previously in Section2, human players will often adapt strategies to defeat theiropponents, and improve them over time. This element oftactical combat and multi-layered strategy can be realisedby the AI agents within D2e similar to the approaches out-lined in Holmga\u030ard et al.\u2019s study (Holmga\u030ard et al. 2018).When considering longer play sessions composed of mul-tiple quests (or a full campaign), or repeated plays of a questwith the same agents, it would be possible to explore weightadjustments for agents to reflect responses to other players\u2019tactics. For example, the Overlord could put more emphasison defeating the Heroes over pursuing alternative win condi-tions (e.g. maximising their Fatigue), or react to other play-ers\u2019 behaviours, such as rushing Heroes who would ratherkeep their distance. This would be beneficial to keepingthe game engaging for any human players involved. Thesetechniques in changing strategies to adapt to other players\u2019actions could be applied within other strategic asymmetri-cal and semi-cooperative games, for example the wargames\u2018OGRE\u2019 (Jackson 1977) or \u2018Fortress America\u2019 (Gray 1986).GM player experience modelling. As the core tacticalcombat aspect demonstrated by our experiments plays is buta minimum part of playing a TTRPG, it is important to re-member that a GM in human play often acts in the inter-est of the group and maintaining player engagement. As ourroadmap (see Figure 1) shows, we recognise the opportunityhere of modelling varied behaviours for the Overlord player,with the aim of maximising engagement for all players moreas a \u2018Villain\u2019, rather than simply defeating the Heroes as an\u2018Adversary\u2019 (Treanor et al. 2015).Narrative and role-play. Another opportunity raised bythis work that we propose for future research is the ex-ploration of naturally integrating role-play and narrativeinto players\u2019 chosen actions. The narrative in tabletop role-playing games is directly tied to both players\u2019 decision-making and the consequences of their actions - the actionthat leads closer to a specific victory condition might notbe the action the players want to take for the sake of role-play, and it is up to the Game Master to ensure the play-ers\u2019 experience and enjoyment are upheld. Players may alsodesire to perform more stylish actions that hold more nar-rative purpose than mechanical, and although not rules-as-written legal, are still within the spirit of the game, and thusallowed with the GM\u2019s approval. These \u2018Rule of Cool\u2019 mo-ments for players to attempt something outside the game\u2019sexpected scope are frequently spontaneous, and althoughthey can certainly try (Mercer 2016), such actions would beunfeasible within virtual game environments without beinghard-coded in. A nascent paper by Gallotta et al. (Gallottaet al. 2024) delves into the potential usages of Large Lan-guage Models (LLMs) to benefit gameplay, such as theirintegration as GM roles and their use as core game me-chanics, which may be a possible solution, whilst Va\u0308rtinenet al. (Va\u0308rtinen, Ha\u0308ma\u0308la\u0308inen, and Guckelsberger 2024) re-cently explored the usage of ChatGPT-2 for generating gamequests within RPGs. Fully automating the role-play aspectsof TTRPGs may lead to an undesirable loss of creative ex-pression from the collaborative storytelling of TTRPGs sys-tems, which is a challenge worth keeping in mind for furtherdevelopments. However, the investigation of LLMs workingalongside digital environments is a potential opportunity thatmay help to realise this interpretation of described actionsas legal mechanics, allowing GM agents to communicate di-rectly with players to ask them how do they want to do this(Mercer 2016).Procedural content generation. Lastly, as laid out inFigure 1, procedurally generating quests and combat en-counters for \u2018Descent\u2019 may be an interesting avenue to ex-plore, much like those within studies of \u2018Gloomhaven\u2019 (Ger-hold and Tijben 2023; Tijben 2023) before. The full imple-mentation of the environment allows simulation of the gen-erated quests to bring additional value to the evaluation pro-cess and increase the quality of the results produced. OurD2e environment already parses quest data from JSON files,so it is easily extensible with new quest setups, although fea-tures like new Monster Actions or Heroic Feats will requireimplementation first.7 Conclusions and Future WorkThis paper described a new game environment adapted fromthe game \u2018Descent: Journeys in the Dark (Second Edi-tion)\u2019 (D2e) in the Tabletop Games Framework (TAG) whichpresents many challenges in a complex one-versus-many en-vironment, and initial experiments on the performance ofRandom, One Step Look Ahead (OSLA, a short-sightedgreedy search) and Monte Carlo Tree Search (MCTS, aheuristic search of a game tree of available actions) agents.We tested these agents in 500 self-play games of 2-5 players,and 1150 games of robin-robin between the three agents, fora dataset of 7150 games total.Overall, we observed both MCTS and OSLA to be ca-pable in both roles of Hero and Overlord in one-versus-many play, with MCTS in particular able to make tacti-cal decisions in a combat-based environment, and considermultiple possible end game conditions with multi-layereddecision-making. We observed the OSLA player able tomake progress towards winning for both sides, showing su-periority as the Overlord against MCTS Heroes. Hero syn-ergy analysis revealed Ashrian, Jain Fairwood and Syndraelto feature in most winning combinations, suggesting supe-rior power or ease of play for general AI agents with min-imal game-specific knowledge. Widow Tarha appears to bethe Hero that requires most advanced strategic play, as theshorter look-ahead agent struggled to make efficient use ofher abilities.Our intended future work (see Figure 1) includes furtherexploration of modelling Game Masters for AI agents, mak-ing the gameplay experience more suitable for more \u2018Dun-geons & Dragons\u2019-like environments. As we extracted manyelements to construct the core tactical combat aspect as thebase of our work, it would make sense to build more featureson top of our current shell to bring the experience closer toa traditional TTRPG, such as additional quests, levelling upwith experience points and unlocking wider class options,and downtime activities that focus more on player experi-ence and game enjoyment over simply defeating opponents,such as travelling between quests or shopping for new equip-ment. We also hope by integrating more narrative and role-play elements into the chosen actions, we can strive towardsan AI Game Master that can provide a similar game experi-ence to that of a human host. We aim for this to help betterinform the design of human-AI semi-cooperative environ-ments and agents in tabletop role-playing games.8 AcknowledgementsFor the purpose of open access, the author(s) has applied aCreative Commons Attribution (CC BY) license to any Ac-cepted Manuscript version arising.This work was supported by the EPSRC IGGI CDT(EP/S022325/1).ReferencesAmini, S.; and Afsharchi, M. 2014. Finding Better Team-mates in a Semi-cooperative Multi-agent System. In 2014IEEE/WIC/ACM International Joint Conferences on Web In-telligence (WI) and Intelligent Agent Technologies (IAT),volume 3, 143\u2013150. IEEE.Ashktorab, Z.; et al. 2020. Human-AI collaboration in acooperative game setting: Measuring social perception andoutcomes. Proc. of the ACM on Human-Computer Interac-tion, 4(CSCW2): 1\u201320.Assembly-Horsham, G.; Maghsudi, S.; and Spronck, P.2023. 3.9 The Tabletop Board Games AI Tutor. Human-Game AI Interaction, 60.Bjornsson, Y.; and Finnsson, H. 2009. Cadiaplayer: Asimulation-based general game player. IEEE Transactionson Computational Intelligence and AI in Games, 1(1): 4\u201315.Bos, P. v. d.; and Stoelinga, M. 2023. With a little helpfrom your friends: semi-cooperative games via Joker moves.arXiv preprint arXiv:2304.13417.Browne, C. B.; et al. 2012. A survey of monte carlo treesearch methods. IEEE Transactions on Computational In-telligence and AI in Games, 4(1): 1\u201343.Callison-Burch, C.; Ippolito, D.; Reitter, D.; Tomar, G. S.;and Martin, L. 2022. Dungeons and Dragons as a ChallengeProblem for Artificial Intelligence. NAACL Wordplay Work-shop, ACL.Chaco\u0301n, P. S.; and Eger, M. 2019. Pandemic as a challengefor human-AI cooperation. In Proceedings of the AIIDEworkshop on Experimental AI in Games.Chaslot, G.; Bakkes, S.; Szita, I.; and Spronck, P. 2008.Monte-Carlo tree search: A new framework for game AI.In Proceedings of the AAAI Conference on Artificial Intel-ligence and Interactive Digital Entertainment, volume 4,216\u2013217.Dafoe, A.; Hughes, E.; Bachrach, Y.; Collins, T.; Mc-Kee, K. R.; Leibo, J. Z.; Larson, K.; and Graepel, T.2020. Open problems in cooperative AI. arXiv preprintarXiv:2012.08630.Dash, T.; Dambekodi, S. N.; Reddy, P. N.; and Abraham,A. 2020. Adversarial neural networks for playing hide-and-search board game Scotland Yard. Neural Computing andApplications, 32: 3149\u20133164.Egri-Nagy, A.; and To\u0308rma\u0308nen, A. 2020. The game is notover yet \u2014 Go in the post-AlphaGo era. Philosophies, 5(4):37.Ellis, S.; and Hendler, J. 2017. Computers Play Chess,Computers Play Go. . . Humans Play Dungeons & Dragons.IEEE Intelligent Systems, 32(4): 31\u201334.Fantasy Flight Publishing, Inc. 2012. Descent: Journeys inthe Dark Second Edition. Diamond Comic Distributors.Gaina, R.; Balla, M.; Dockhorn, A.; Montoliu Cola\u0301s, R.; andPerez, D. 2020a. TAG: A tabletop games framework. InAIIDE Workshops. CEUR Workshop Proc.Gaina, R. D.; and Balla, M. 2022. TAG: Pandemic Competi-tion. In 2022 IEEE Conference on Games (CoG), 552\u2013559.IEEE.Gaina, R. D.; Balla, M.; Dockhorn, A.; Montoliu, R.; andPerez-Liebana, D. 2020b. Design and implementationof TAG: A tabletop games framework. arXiv preprintarXiv:2009.12065.Gaina, R. D.; Goodman, J.; and Perez-Liebana, D. 2021.TAG: Terraforming Mars. In Proceedings of the AAAI Con-ference on Artificial Intelligence and Interactive Digital En-tertainment, volume 17, 148\u2013155.Gallotta, R.; Todd, G.; Zammit, M.; Earle, S.; Liapis,A.; Togelius, J.; and Yannakakis, G. N. 2024. LargeLanguage Models and Games: A Survey and Roadmap.arXiv:2402.18659.Gerhold, M.; and Tijben, K. 2023. Computer Aided Con-tent Generation\u2013A Gloomhaven Case Study. In Proceedingsof the 18th International Conference on the Foundations ofDigital Games, 1\u201310.Gray, M. 1986. Fortress America. Milton Bradley.Gygax, G.; and Arneson, D. 1974. Dungeons & Dragons.Tactical Studies Rules, Inc.Holmga\u030ard, C.; Green, M. C.; Liapis, A.; and Togelius, J.2018. Automated playtesting with procedural personasthrough MCTS with evolved heuristics. IEEE Transactionson Games, 11(4): 352\u2013362.Jackson, S. 1977. OGRE. Metagaming Concepts.Kitchen, S.; McGroarty, C.; and Aris, T. 2023. Model Rep-resentation Considerations for Artificial Intelligence Oppo-nent Development in Combat Games. In The InternationalFLAIRS Conference Proceedings, volume 36.Ling, B.; Liu, X.; Jiang, J.; Wu, W.; Wang, W.; Lyu, Y.; andXu, X. 2022. Master multiple real-time strategy games witha unified learning model using multi-agent reinforcementlearning. In International Conference on Neural Comput-ing for Advanced Applications, 27\u201339. Springer.Martin, L. J.; Sood, S.; and Riedl, M. O. 2018. Dungeonsand DQNs: Toward Reinforcement Learning Agents thatPlay Tabletop Roleplaying Games. In INT/WICED@ AI-IDE.Mercer, M. 2016. The Rule of Cool! (Game Master Tips).Geek & Sundry, YouTube. Uploaded 16 February 2016.https://youtu.be/fWZDuFIYkf0.Park, J. S.; O\u2019Brien, J.; Cai, C. J.; Morris, M. R.; Liang,P.; and Bernstein, M. S. 2023. Generative agents: Interac-tive simulacra of human behavior. In Proceedings of the36th Annual ACM Symposium on User Interface Softwareand Technology, 1\u201322.Piette, E.; Soemers, D. J.; Stephenson, M.; Sironi, C. F.;Winands, M. H.; and Browne, C. 2020. Ludii\u2013the ludemicgeneral game system. In ECAI 2020, 411\u2013418. IOS Press.Press, P. 2017. Cthulhu Confidential. Pelgrane Press.Riedl, M. O.; and Bulitko, V. 2013. Interactive narrative: Anintelligent systems approach. Ai Magazine, 34(1): 67\u201367.Riel, J.; and Monahan, R. 2024. Learning from Ludemes:An Inventory of Common Player Actions within TabletopRole-Playing Games (TTRPGs) to Inform Principled Designof Game-Based Learning Experiences. International Jour-nal of Role-Playing, 15: 178\u2013210.Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Foer-ster, J.; and Whiteson, S. 2019. The Starcraft multi-agentchallenge. arXiv preprint arXiv:1902.04043.Santiago III, J. M.; Parayno, R. L.; Deja, J. A.; and Samson,B. P. V. 2023. Rolling the Dice: Imagining Generative AIas a Dungeons & Dragons Storytelling Companion. arXivpreprint arXiv:2304.01860.Sfikas, K.; and Liapis, A. 2020. Collaborative agent game-play in the pandemic board game. In Proceedings of the15th International Conference on the Foundations of Digi-tal Games, 1\u201311.Shin, M.; Kim, J.; and Kim, M. 2020. Measuring humanadaptation to AI in decision making: application to evaluatechanges after AlphaGo. arXiv preprint arXiv:2012.15035.Shin, M.; Kim, J.; and Kim, M. 2021. Human learning fromartificial intelligence: evidence from human Go players\u2019 de-cisions after AlphaGo. In Proceedings of the Annual Meet-ing of the Cognitive Science Society, volume 43.Shyne, F. 2023. Automatic Play-testing of Dungeons andDragons Combat Encounters. Honors in the Department ofComputer Science, Union College.Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,M.; Guez, A.; Lanctot, M.; et al. 2017. Mastering Chess andShogi by Self-Play with a General Reinforcement LearningAlgorithm. CoRR, abs/1712.01815.Silver, D.; et al. 2016. Mastering the game of Go with deepneural networks and tree search. Nature, 529: 484\u2013489.Sousa, M. 2023. Finding Ludemes in Modern Board Games:Analyzing the Top Number One Games of Board GameGeek. Board Game Studies Journal, 17(1): 205\u2013229.Sutton, R. S.; Precup, D.; and Singh, S. 1999. BetweenMDPs and semi-MDPs: A framework for temporal abstrac-tion in reinforcement learning. Artificial intelligence, 112(1-2): 181\u2013211.Tijben, C. 2023. Generating Gloomhaven dungeons throughevolutionary game design. B.S. thesis, University of Twente.Treanor, M.; Zook, A.; Eladhari, M. P.; Togelius, J.; Smith,G.; Cook, M.; Thompson, T.; Magerko, B.; Levine, J.; andSmith, A. 2015. AI-based game design patterns. Proceed-ings of the 10th International Conference on the Founda-tions of Digital Games 2015 (FDG 2015).Tychsen, A.; Hitchens, M.; Brolund, T.; and Kavakli, M.2005. The Game Master. In ACM International ConferenceProceeding Series, volume 123, 215\u2013222.Vinyals, O.; Babuschkin, I.; Chung, J.; and Mathieu, M.2019. Alphastar: Mastering the Real-Time Strategy GameStarCraft II.Va\u0308rtinen, S.; Ha\u0308ma\u0308la\u0308inen, P.; and Guckelsberger, C. 2024.Generating Role-Playing Game Quests With GPT LanguageModels. IEEE Transactions on Games, 16(1): 127\u2013139.Zhu, A.; Martin, L.; Head, A.; and Callison-Burch, C. 2023.CALYPSO: LLMs as Dungeon Master\u2019s Assistants. In Pro-ceedings of the AAAI Conference on Artificial Intelligenceand Interactive Digital Entertainment, volume 19, 380\u2013390.",
        "title": "\u2018Journeys in the Dark\u2019 - Towards Game Master AI in Complex Board Games",
        "citationCount": 0
    },
    {
        "content": "Journal of Business & Technology LawVolume 13 | Issue 1 Article 2The Global Artificial Intelligence RevolutionChallenges Patent Eligibility LawsMizuki HashiguchiFollow this and additional works at: http://digitalcommons.law.umaryland.edu/jbtlThis Article is brought to you for free and open access by the Academic Journals at DigitalCommons@UM Carey Law. It has been accepted forinclusion in Journal of Business & Technology Law by an authorized editor of DigitalCommons@UM Carey Law. For more information, please contactsmccarty@law.umaryland.edu.Recommended CitationMizuki Hashiguchi, The Global Artificial Intelligence Revolution Challenges Patent Eligibility Laws, 13 J. Bus. & Tech. L. 1 (2017)Available at: http://digitalcommons.law.umaryland.edu/jbtl/vol13/iss1/2HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  JOURNAL OF BUSINESS & TECHNOLOGY LAW 1 MIZUKI HASHIGUCHI* The Global Artificial Intelligence Revolution  Challenges Patent Eligibility Laws I. INTRODUCTION A heart full of compassion, fortitude, sagacity, and hope distinguishes humans from machines.1 The technology of artificial intelligence is now increasingly relied upon as a highly-proficient, human-assisting tool in a myriad of disciplines across the globe. A prominent chef who collaborated with IBM\u2019s artificial intelligence \u201cChef Watson\u201d was mesmerized by its culinary ingenuity.2 When users input keywords, Chef Watson outputs a recipe by performing a comprehensive analysis of flavor combinations.3 For example, in response to the keywords \u201ctangerine,\u201d \u201cpunch,\u201d and \u201choliday,\u201d Chef Watson suggested a recipe for an aperitif named \u201cRelax,\u201d which is a refreshing citrus cocktail with a slice of lemon and mint leaves on top.4 When the user sought an appetizer with the keywords \u201ccrab,\u201d \u201csoup,\u201d and \u201cFrench-style,\u201d Chef Watson presented a recipe for a warm soup with a bundle of flat pasta made from elegantly-sliced radish.5 The renowned human chef who cooked a full course meal from Chef Watson\u2019s recipes stated that the recipes had an element of surprise, and that the experience was enlightening.6  \u00a9 2017 Mizuki Hashiguchi       *       Attorney at Law, Admitted in New York (Juris Doctor, Columbia Law School).       1.  See Thomas L. Friedman, Opinion, From Hands to Heads to Hearts, N.Y. TIMES (Jan. 4, 2017), https://www.nytimes.com/2017/01/04/opinion/from-hands-to-heads-to-hearts.html.  2.  Norihiro Ikeda, A Collaboration with a First-Class Chef: How Delicious Is Food Cooked by IBM\u2019s Watson? I Actually Tried It (1/2), ITMEDIA (Dec. 5, 2014), http://www.itmedia.co.jp/enterprise/articles/ 1412/05/news037.html [hereinafter Ikeda, Food by Watson 1].  3.  See id.; IBM Chef Watson: A Metaphor for Discovery, IBM http://www-03.ibm.com/press/ us/en/presskit/46500.wss (last visited Nov. 29, 2017); see generally IBM Watson, IBM http://www-03.ibm.com /press/us/en/presskit/27297.wss (last visited Nov. 29, 2017).  4.  See Ikeda, Food by Watson 1, supra note 2.  5.  See id.  6.  Norihiro Ikeda, A Collaboration with a First-Class Chef: How Delicious is Food Cooked by IBM\u2019s Watson? I Actually Tried It (2/2), ITMEDIA (Dec. 5, 2014), http://www.itmedia.co.jp/enterprise/articles/1412 /05/news037_2.html. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 2 JOURNAL OF BUSINESS & TECHNOLOGY LAW On August 13, 2017, The Guardian suggested that artificial intelligence can be a valuable resource for judges to render fair and consistent decisions.7 In the medical sciences, pharmaceutical company GlaxoSmithKline announced a $43 million agreement enabling it to use British biotechnology company Exscientia\u2019s artificial intelligence technology for discovering life-saving medicine.8 The artificial intelligence employed in this project will predict the behavior of molecules and indicate whether a medicine is likely to be beneficial, possibly reducing the cost and time of discovery by seventy-five percent.9 Artificial intelligence has already made concrete contributions.10 Japan\u2019s National Institute of Information and Communications Technology uses artificial intelligence technology to foresee the magnitude and timing of solar flares.11 The artificial intelligence system learned from 300,000 high-resolution photographs of the sun\u2019s surface.12 By utilizing this artificial intelligence system, the Institute was able to enhance the accuracy of its predictions from fifty to eighty percent.13 Kewpie, a food manufacturing company in Japan, succeeded in doubling its productivity by using deep learning technology to select good quality potatoes.14 Traditionally, employees visually inspected more than one million diced potatoes per day for quality assurance.15 To streamline this time-consuming process, Kewpie used 18,000 pictures of diced potatoes to teach an artificial intelligence system what quality potatoes look like.16 The system was thus trained to recognize high-quality potatoes automatically.17   7.  John Naughton, Why a Computer Could Help you Get a Fair Trial, THE GUARDIAN (Aug. 13, 2017, 2:00 PM), https://www.theguardian.com/technology/commentisfree/2017/aug/13/why-a-computer-could-help-you-get-a-fair-trial.  8.  See Ben Hirschler, Big Pharma Turns to AI to Speed Drug Discovery, GSK Signs Deal, REUTERS (July 1, 2017, 8:10 PM), http://www.reuters.com/article/us-pharmaceuticals-ai-gsk-idUSKBN19N003.  9.  See id.  10.  See, e.g., K\u014dtar\u014d Fukuoka, Solar Flare, A Prediction Accuracy of 80% with AI, Flare may Still Occur in the Next Seven Days, NIHON KEIZAI SHINBUN (Sept. 8, 2017), https://www.nikkei.com/article/ DGXLASDZ08H7H _Y7A900C1000000/.  11.  See id.  12.  Id.  13.  Id.  14.  Norihiro Ikeda, The Productivity of Ingredients Inspection Doubles: Kewpie Starts Using AI - The Power of \u201cDeep Learning\u201d that Processes More Than 1 Million Potatoes A Day (2/2), ITMEDIA (June 20, 2017), http://www.itmedia.co.jp/enterprise/articles/1706/20/news049_2.html [hereinafter Ikeda, Kewpie Starts Using AI 2].  15.  Norihiro Ikeda, The Productivity of Ingredients Inspection Doubles: Kewpie Starts Using AI - The Power of \u201cDeep Learning\u201d that Processes More Than 1 Million Potatoes A Day (1/2), ITMEDIA (June 20, 2017), http://www.itmedia.co.jp/enterprise/articles/1706/20/news049.html.  16.  See Ikeda, Kewpie Starts Using AI 2, supra note 14.  17.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 3 Artificial intelligence is also applied in the arts.18 In August 2015, computer scientists in T\u00fcbingen, Germany, created an algorithm that enabled artificial intelligence to paint in the style of legendary artists.19 The algorithm uses a \u201cdeep neural network\u201d to make artificial intelligence learn the salient characteristics of Pablo Picasso\u2019s artistic style.20 A \u201cdeep neural network\u201d is a multiple-layered network of inter-connected processors modeled after neurons of the human brain.21 When a user provides this artificial intelligence \u201cmaestro\u201d with a scenic photograph of T\u00fcbingen, with its light pink, yellow, and sky-blue houses facing the Neckar River, the artificial intelligence produces a painting of this scenery that creates the impression that it was actually painted by Pablo Picasso.22 In December 2016, computer scientists in Paris, France, constructed an artificial intelligence that composes polyphonic chorales emanating the sublime style of Johann Sebastian Bach.23 The scientists analyzed Bach\u2019s chorale music and represented the notes for the soprano, alto, tenor, and bass voices in numbers.24 They also assigned numerical values for the beats and fermatas.25 The researchers combined this data representation with an algorithm called Gibbs sampling and four neural networks to create a statistical model called DeepBach.26 According to an experiment implementing DeepBach, approximately fifty percent of the 1,272 people who listened to DeepBach\u2019s music believed that it was genuinely composed by Johann Sebastian Bach.27 In these ways, artificial intelligence is starting to play a crucial role in assisting humans achieve various goals. Artificial intelligence has already brought concrete improvements to the operations of businesses.28 It has also shown the capacity to enrich our culture and to add sprinkles of joy to our daily lives.29 Research and innovation are indispensable for facilitating the contribution of artificial intelligence technology in our global society.   18.  Leon A. Gatys, Alexander S. Ecker & Matthias Bethge, A Neural Algorithm of Artistic Style, (Aug. 26, 2015), https://arxiv.org/pdf/1508.06576v2.pdf (last updated Sept. 2, 2015).  19.  Id.  20.  Id.  21.  J\u00fcrgen Schmidhuber, Deep Learning in Neural Networks: An Overview, 61 NEURAL NETWORKS 85, 86 (2015); Self-learning AI Emulates the Human Brain, EUROPEAN RES. COUNCIL (July 22, 2016), https://erc. europa.eu/projects-and-results/erc-stories/self-learning-ai-emulates-human-brain.  22.  Gatys et al., supra note 18, at 4\u20135.  23.  GA\u00cbTAN HADJERES & FRAN\u00c7OIS PACHET, DEEPBACH: A STEERABLE MODEL FOR BACH CHORALES GENERATION 1 (Jun. 17, 2017), arxiv.org/pdf/1612.01010v2.pdf.  24.  Id. at 4\u20135.  25.  Id. at 4.  26.  Id. at 1, 5\u20137.  27.  Id. at 6\u20137.  28.  See supra note 10\u201317 and accompanying text.  29.  See supra note 18\u201327 and accompanying text. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 4 JOURNAL OF BUSINESS & TECHNOLOGY LAW Do we have an effective legal system that supports the development of artificial intelligence? Patent law protects inventions.30 It is designed to encourage innovation in our society.31 It does so by providing inventors with a monopoly over their inventions for a limited time in exchange for properly disclosing their inventions to the public.32 During the effective term of a patent for an invention, the patent allows the patent owner to preclude others from utilizing the invention without their authorization.33 There are a number of conditions that a patent applicant must fulfill to obtain a patent. For instance, the invention must be a subject matter that is eligible for patent protection.34 This requirement is called the \u201cpatent eligibility\u201d requirement.35 Additionally, the invention must be novel36 and non-obvious.37 Patent applications must describe the invention with sufficient clarity and detail.38 These are only some of the conditions that must be met for an invention to be protected by a patent.39 These conditions exist to balance the pros and cons of providing a monopoly under patent law.40 A temporary monopoly over inventions provides inventors with a sense of relief, assuring that they have a legal right to prevent others from engaging in unauthorized use, imitation, and appropriation of their inventions.41 On the other hand, this patent-based monopoly might hinder inventive activities if these activities require the use of rudimentary concepts or ingredients that are already protected by a patent.42 The patent eligibility requirement addresses these concerns.43 This requirement ensures that the building blocks of innovation, such as abstract ideas and mental   30.  Mark Harper, U.S. Intellectual Property, 95 MICH. B.J. 20, 20 (2016).  31.  Anna B. Laakmann, An Explicit Policy Lever for Patent Scope, 19 MICH. TELECOMM. & TECH. L. REV. 43, 44 (2012).  32.  Kenneth W. Dam, The Economic Underpinnings of Patent Law, 23 J. LEGAL STUD. 247, 248, 270\u201371 (1994).  33.  See, e.g., id. at 247\u2013248.  34.  See, e.g., 35 U.S.C. \u00a7 101 (2012).  35.  Efthimios Parasidis, A Uniform Framework for Patent Eligibility, 85 TUL. L. REV. 323, 326 (2010).  36.  35 U.S.C. \u00a7 102.   37.  Id. \u00a7 103.  38.  See, e.g., id. \u00a7 112.  39.  See, e.g., id. \u00a7\u00a7 111\u2013112.  40.  Dam, supra note 32, at 261.  41.  Id. at 248\u201349.  42.  See id. at 253 (explaining that broad patent protection can hinder future innovation).  43.  See Egenera, Inc. v. Cisco Sys., Inc., 234 F. Supp. 3d 331, 334 (D. Mass. 2017) (explaining that \u201c[o]ne of the challenges for the patent system . . . is to separate out new and useful applications of abstract ideas from impermissible attempts to monopolize them\u201d). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 5 processes, are not bound by patents and remain available for use in technological progress.44 Some patent applications regarding artificial intelligence were rejected, and some existing patents covering artificial intelligence were invalidated due to the inventions\u2019 failure to satisfy the patent eligibility requirement.45 Thus, patent eligibility is an important threshold that must be met to obtain patent protection in the area of artificial intelligence.46 Given the importance of the patent eligibility requirement and the growing prevalence of artificial intelligence in our global economy, this Article examines the patent eligibility jurisprudence of artificial intelligence under the laws of the United States of America, the European Patent Convention, France, Japan, and Singapore. By analyzing the judicial decisions of courts and the administrative judgments of patent offices relating to the patent eligibility of artificial intelligence, it is possible to illuminate the implicit de facto requirements that are imposed by these tribunals for artificial intelligence to be found patent-eligible. These decisions and judgments indicate that an invention related to artificial intelligence is more likely to meet the patent eligibility requirement when the patent application specifies the inner workings of the invention in sufficient detail.47 In addition, patent-eligible artificial intelligence often has a technical character.48 Further, tribunals often expect patent applications to specify the technical effect achieved by the invention.49 Some features of artificial intelligence technology may be at odds with these requirements.50 The jurisprudence of patent eligibility is flexible enough to allow room for creative judicial interpretations of broad statutory definitions.51 This flexibility enables the patent eligibility jurisprudence to evolve in response to technical and societal changes associated with the rapid development of artificial intelligence.   44.  See Parasidis, supra note 35, at 332.  45.  See Alice Corp. Pty. v. CLS Bank Int\u2019l, 134 S. Ct. 2347, 2355\u201357 (2014); Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 \u2013 3.5.1, Summary of Facts and Submissions \u00b6 VII (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.; Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Feb. 26, 2016, [15/01962] (Fr.).  46.  See, e.g., Alice Corp. Pty., 134 S. Ct. at 2355\u201357; Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 \u2013 3.5.1, Summary of Facts and Submissions \u00b6 VII (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.; Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Feb. 26, 2016, [15/01962] (Fr.).  47.  See infra Parts III.A\u2013III.E, IV.A.  48.  See infra Parts III.A\u2013III.E, IV.B.  49.  See infra Parts III.A\u2013III.E, IV.C.  50.  See infra Parts III.A\u2013III.E, IV.A\u2013IV.C, V.A\u2013V.C.  51.  See infra Parts VI.A\u2013VI.C. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 6 JOURNAL OF BUSINESS & TECHNOLOGY LAW Part II of this Article presents an overview of the technology of artificial intelligence. Part II.A discusses the definition and research areas of artificial intelligence. Part II.B describes a brief history of the development of artificial intelligence. Part II.C then explains how and why the global artificial intelligence revolution is taking place. Part III presents a comparative case study concerning the patent eligibility of artificial intelligence. It explores the patent eligibility jurisprudence involving artificial intelligence technology under United States patent law, the European Patent Convention, the French Intellectual Property Code, Japanese patent law, and the Patents Act of Singapore. Part IV analyzes the substantive requirements that surface from judicial decisions finding certain artificial intelligence technology to be patent-eligible. Although these requirements are not codified in patent statutes, case law suggests that they are de facto requirements for establishing the patent eligibility of artificial intelligence. Part V analyzes the patent eligibility jurisprudence in light of the realities presently exhibited by artificial intelligence technology. It argues that artificial intelligence technology has certain features that create practical challenges when a patent applicant or owner is trying to establish patent eligibility before a court or an administrative tribunal. Part VI ruminates on the future of the patent eligibility jurisprudence of artificial intelligence, examining the potential for legal systems to adapt to technological change. II. ARTIFICIAL INTELLIGENCE TECHNOLOGY A. Computers Performing Mental Steps Artificial intelligence is defined as \u201cthe ability of machines to do things that people would say require intelligence.\u201d52 The phrase sometimes refers to intelligent machines themselves.53 Thus, artificial intelligence attempts to emulate the mental steps of human beings.54 Such mental steps include understanding languages,55 responding to questions,56 identifying patterns,57 solving problems,58 and learning through experience.59   52.  PHILIP C. JACKSON, JR., INTRODUCTION TO ARTIFICIAL INTELLIGENCE 1 (Dover Publ\u2019n, Inc., 2d ed. 1985) (1974).  53.  Id. at 2.  54.  See id. at 8\u20139; Self-Learning AI Emulates the Human Brain, supra note 21.  55.  JACKSON, JR., supra note 52, at 292\u201393.  56.  Id. at 312\u2013330.  57.  Id. at 192\u2013210.  58.  Id. at 75\u2013108.  59.  Id. at 335\u2013336. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 7 B. The Development of Artificial Intelligence Technology The idea of making machines think and behave like humans has existed since antiquity.60 In Greek mythology, the masterful Hephaestus built a gigantic robot that patrolled the island of Crete, monitoring whether laws were properly implemented.61 Hephaestus even created intelligent tables that automatically supplied food and drinks.62 During the Italian Renaissance, Leonardo da Vinci drew mechanical lions that moved autonomously.63 One of them was designed to present a cluster of beautiful lilies.64 Professor George Boole\u2019s investigation on the \u201claws of thought,\u201d65 published in 1854, is respected as the \u201cfirst step\u201d towards the development of artificial intelligence software.66 In 1943, Professors Warren McCulloch and Walter Pitts published \u201cA Logical Calculus of the Ideas Immanent in Nervous Activity,\u201d establishing the cornerstone of neural networks.67 Further, Mr. Alan Turing made a pioneering examination of the question, \u201cCan machines think?\u201d in his article titled \u201cComputing Machinery and Intelligence,\u201d published in 1950.68 In 1955, Professor John McCarthy conceived the term \u201cartificial intelligence.\u201d69 In 1978, Professor Herbert A. Simon received the Nobel Prize in Economic Sciences for his \u201cpioneering research into the decision-making process within economic   60.  See Association for the Advancement of Artificial Intelligence, A Brief History of AI, AITOPICS, https://aitopics.org/misc/brief-history (last visited Sept. 13, 2017); see also Hephaestus and the Creation of the Robots, GODS AND GODDESSES OF ANCIENT GREECE, http://www.greek-gods.info/greek-gods/hephaestus /myths/hephaestus-robots/ (last visited Sept. 13, 2017).  61.  See Association for the Advancement of Artificial, supra note 60; see also Hephaestus and the Creation of the Robots, supra note 60; see generally Hephaistos, THEOI GREEK MYTHOLOGY, http://www.theoi.com/ Olympios/Hephaistos.html (last visited Sept. 13, 2017); Talos, THEOI GREEK MYTHOLOGY, http://www. theoi.com/Gigante/GiganteTalos.html (last visited Sept. 13, 2017).  62.  See Hephaestus and the Creation of the Robots, supra note 60.  63.  See Pride of Da Vinci\u2019s Genius Walks Again After 500 years, INDEPENDENT (July 3, 2009, 6:00 PM), http://www.independent.co.uk/arts-entertainment/art/features/pride-of-da-vincis-genius-walks-again-after-500-years-1731269.html.  64.  See Jonathan Jones, The Charisma Droids: Today\u2019s Robots and the Artists Who Foresaw Them, THE GUARDIAN (Sept. 13, 2017), https://www.theguardian.com/artanddesign/2017/jan/28/charisma-droids-todays-robots-da-vinci-michelangelo-science-museum-robots.  65.  See generally GEORGE BOOLE, AN INVESTIGATION OF THE LAWS OF THOUGHT, ON WHICH ARE FOUNDED THE MATHEMATICAL THEORIES OF LOGIC AND PROBABILITIES (1854).  66.  See DOUGLAS R. HOFSTADTER, G\u00d6DEL, ESCHER, BACH: AN ETERNAL GOLDEN BRAID 601 (Basic Books 1999).  67.  See generally Warren S. McCulloch & Walter Pitts, A Logical Calculus of Ideas Immanent in Nervous Activity, 52 BULL. MATHEMATICAL BIOLOGY, no. 1, 1990, at 99\u2013115; see also Association for the Advancement of Artificial Intelligence, supra note 60.  68.  See HOFSTADTER, supra note 66, at 594\u201397.  69.  See generally J. MCCARTHY, M. L. MINSKY, N. ROCHESTER & C.E. SHANNON, A PROPOSAL FOR THE DARTMOUTH SUMMER RESEARCH PROJECT ON ARTIFICIAL INTELLIGENCE (Aug. 31, 1955); see also Association for the Advancement of Artificial Intelligence, supra note 60.  HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 8 JOURNAL OF BUSINESS & TECHNOLOGY LAW organizations,\u201d70 theorizing \u201cbounded rationality,\u201d a key concept in artificial intelligence.71 The 1990s saw \u201cmajor advances\u201d in all areas of artificial intelligence, including machine learning, reasoning, data mining, natural language understanding, vision, and virtual reality.72 During this period, robots started excelling in playing chess at a world championship level, exploring Mars, and proving mathematical theorems.73 C. The Artificial Intelligence Revolution The progress of artificial intelligence technology accelerated exponentially in the 2010s.74 In December 2015, Bloomberg observed that \u201c[c]omputers are smarter and learning faster than ever.\u201d75 This phenomenal advancement is attributed to the reinforcement in cloud computing infrastructure, the growing availability of datasets and software development tools, and a significant reduction in the price of neural networks that are essential to machine learning.76 Artificial intelligence is becoming increasingly proficient in performing human tasks.77 It is also becoming ubiquitous.78 More and more enterprises are incorporating artificial intelligence into their operations.79 This impacts a multitude of industries including law, healthcare, finance, engineering, customer service, entertainment, and communication.80 In December 2016, Bloomberg remarked that \u201cThe Artificial   70.  Press Release, The Royal Swedish Academy of Sciences, Studies of Decision-Making Lead to Prize in Economics (Oct. 16, 1978), http://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/1978/press. html.  71.  See Association for the Advancement of Artificial Intelligence, supra note 60.  72.  Id.  73.  Id.  74.  See ERIK BRYNJOLFSSON & ANDREW MCAFEE, THE SECOND MACHINE AGE: WORK, PROGRESS, AND PROSPERITY IN A TIME OF BRILLIANT TECHNOLOGIES 20 (W.W. Norton & Co., 1st ed. 2014).  75.  See Jack Clark, Why 2015 Was a Breakthrough Year in Artificial Intelligence, BLOOMBERG TECH. (Dec. 8, 2015, 8:00 AM), https://www.bloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artificial-intelligence.  76.  Id.  77.  See, e.g., Gideon Lewis-Kraus, The Great A.I. Awakening, N.Y. TIMES (Dec. 14, 2016), https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html.  78.  See infra Part II.C.  79.  See Jon Card, A New Company Every Week: Inside the UK\u2019s AI Revolution, THE GUARDIAN (May 15, 2017, 2:00 PM), https://www.theguardian.com/small-business-network/2017/may/15/artificial-intelligence-professor-stephen-hawking-sodash-crystal-xero.  80.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 9 Intelligence Revolution Is Here.\u201d81 Fortune named 2017 the \u201cYear of Artificial Intelligence.\u201d82 III. COMPARATIVE CASE STUDY OF THE PATENT ELIGIBILITY OF ARTIFICIAL INTELLIGENCE The patent eligibility of artificial intelligence has been scrutinized in a number of cases before judicial and administrative panels under the patent laws of the United States, the European Patent Convention, France, Japan, and Singapore.83 Examining these cases elucidates substantive rules that are applied to determine the patent eligibility of artificial intelligence. This analysis also provides insights on the factual characteristics of cases in which the patent eligibility of artificial intelligence technology was upheld. A. The United States of America The jurisprudence of patent eligibility in the United States is based on section 101 of the United States Patent Act and a body of judicial decisions applying this statutory provision.84 35 U.S.C. \u00a7 101 provides that \u201c[w]hoever invents or discovers any new and useful process, machine, manufacture, or composition of matter, or any new and useful improvement thereof, may obtain a patent therefor, subject to the conditions and requirements of this title.\u201d85 Patents and patent applications contain \u201cclaims,\u201d which recite what the invention is.86 These patent claims constitute a vital component of the patent because they delineate the boundaries of legal protection provided by that patent.87 Inventions in the area of artificial intelligence include methods for making computers implement mental steps88 and devices that are designed to automatically   81.  See The Artificial Intelligence Revolution Is Here, BLOOMBERG (Dec. 2, 2016, 5:48 PM), https://www. bloomberg.com/news/videos/2016-12-02/the-artificial-intelligence-revolution-is-here.  82.  See Adam Lashinsky, 2017 Will Be the Year of AI, FORTUNE (Dec. 30, 2016), http://fortune.com/2016/12/30/the-year-of-artificial-intelligence/; see also Sandhya Venkatachalam, Why 2017 Is The Year Of Artificial Intelligence, FORBES (Feb. 27, 2017, 9:00 AM), https://www.forbes.com/sites/forbes techcouncil/2017/02/27/why-2017-is-the-year-of-artificial-intelligence.  83.  See infra Parts IV.A\u2013IV.E.  84.  See UNITED STATES PATENT AND TRADEMARK OFFICE, PATENT ELIGIBLE SUBJECT MATTER: REPORT ON VIEWS AND RECOMMENDATIONS FROM THE PUBLIC 1 (July 2017), https://www.uspto.gov/sites/default /files/documents/101-Report_FINAL.pdf.  85.  35 U.S.C. \u00a7 101 (2012).  86.  See, e.g., UNITED STATES PATENT AND TRADEMARK OFFICE, MANUAL OF PATENT EXAMINING PROCEDURE 1800-30 (2015), https://www.uspto.gov/web/offices/pac/mpep/s1824.html.  87.  Id.  88.  See Larry Hauser, Artificial Intelligence, INTERNET ENCYCLOPEDIA OF PHILOSOPHY, http://www.iep. utm. edu/art-inte/ (last visited Sept. 15, 2017). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 10 JOURNAL OF BUSINESS & TECHNOLOGY LAW carry out mental steps.89 Section 101 does not state whether methods and devices implementing mental steps are eligible for patent protection.90 The Supreme Court of the United States has held that mental processes, abstract intellectual concepts, and natural phenomena cannot be protected by a patent.91 Courts in the United States evaluate the patent eligibility of computerized implementations of mental processes and human activities in two steps.92 The first step is determining whether the invention is directed to an \u201cabstract idea.\u201d93 The Federal Circuit has explained that courts have treated \u201canalyzing information by steps people go through in their minds, or by mathematical algorithms, without more, as essentially mental processes within the abstract-idea category.\u201d94 If the court determines that an invention is not directed to an abstract idea, the invention is patent-eligible.95 Conversely, if the court decides that the invention is directed to an abstract idea, the court proceeds to the second step of the inquiry.96 The second step is determining whether the invention is \u201cinventive.\u201d97 In the patent eligibility inquiry, an invention is \u201cinventive\u201d when the patent claim at issue recites a feature of the invention that makes a contribution and thereby transforms an abstract idea into a patent eligible invention.98 If the court determines that inventiveness is present, the invention will be patent-eligible.99 The Supreme Court has not defined what an \u201cabstract idea\u201d is.100 Thus, courts in the United States determine whether a patent claim at issue is directed to an abstract idea by comparing the claim with those that were found to be directed to abstract ideas in previous cases.101 Hence, examining court decisions on patent eligibility is vital to understanding the range of artificial intelligence inventions that are likely to be regarded as patent-eligible.   89.  See infra Part III.A (analyzing judicial opinions concerning inventions that mitigate risk, adjust computer graphics, predefine structures of databases, and monitor data).  90.  35 U.S.C. \u00a7 101 (2012).  91.  See Gottschalk v. Benson, 409 U.S. 63, 67 (1972).  92.  See Alice Corp. Pty. v. CLS Bank Int\u2019l., 134 S. Ct. 2347, 2355 (2014).  93.  Id.  94.  See Elec. Power Grp. v. Alstom S.A., 830 F.3d 1350, 1354 (Fed. Cir. 2016).  95.  See Alice Corp. Pty., 134 S. Ct. at 2354.  96.  Id. at 2355.  97.  Id.  98.  See Mayo Collaborative Servs. v. Prometheus Labs., 566 U.S. 66, 77 (2012) (\u201cThe question before us is whether the claims do significantly more than simply describe these natural relations. To put the matter more precisely, do the patent claims add enough to their statements of the correlations to allow the processes they describe to qualify as patent-eligible processes that apply natural laws?\u201d).  99.  Id. at 72.  100.  See Enfish, LLC v. Microsoft Corp., 822 F.3d 1327, 1334 (Fed. Cir. 2016).  101.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 11 1. Computerized Mitigation of Financial Risk In Alice Corporation Pty. Ltd. v. CLS Bank International, the invention at issue was a computerized method for mitigating settlement risk.102 A settlement risk is the risk that a party in a financial transaction fails to pay the amount that it is obligated to pay.103 To alleviate this risk, the invention used a computer as an intermediary to keep track of each party\u2019s account balance.104 This ensured that the parties would have enough funds to carry out the financial transaction.105 The Supreme Court first determined that the invention in Alice was directed to an abstract idea of intermediated settlement.106 The Court stated that intermediated settlement is \u201ca fundamental economic practice long prevalent in our system of commerce.\u201d107 Next, the Court decided that the computerized method failed to transform the abstract idea into a patent-eligible invention because the method \u201cmerely require[d] generic computer implementation.\u201d108 The Court pointed out that the computer performed a \u201cpurely conventional\u201d process in each of the steps contained in the claimed method.109 Moreover, the Court characterized \u201cthe use of a computer to obtain data, adjust account balances, and issue automated instructions\u201d as a well-understood, routine, and conventional activity that is already known in the industry.110 The Court noted that the computerized method did not \u201cpurport to improve the functioning of the computer itself,\u201d and did not make \u201can improvement in any other technology or technical field.\u201d111 The Supreme Court therefore determined that the computerized method at issue in Alice was not patent-eligible.112 2. Automated Computer Graphics A method for automatically adjusting computer graphics was determined to be patent-eligible in McRO, Inc. v. Bandai Namco Games America Inc.113 The patent at issue recited a method for automatically animating lip synchronization and facial expressions of characters in computer graphics animation.114   102.  Alice Corp. Pty., 134 S. Ct. at 2352.  103.  Id. at 2351\u201352.  104.  Id. at 2352.  105.  Id. at 2351\u201353.  106.  Id. at 2355.  107.  Id. at 2356 (quoting Bilski v. Kappos, 561 U.S. 593, 611 (2010)).  108.  Alice Corp. Pty. v. CLS Bank Int\u2019l., 134 S. Ct. 2347, 2357 (2014).  109.  Id. at 2358.  110.  Id. at 2359.  111.  Id.  112.  Id. at 2359\u201360.  113.  See McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1303 (Fed. Cir. 2016).  114.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 12 JOURNAL OF BUSINESS & TECHNOLOGY LAW Computer graphics characters can smile, sing, and be flabbergasted because certain points on the characters\u2019 faces move to other positions at appropriate times.115 Animators generally consult a timed transcript to determine when various points on a character\u2019s face should be transposed from one timeframe to another.116 While looking at the animation screen, animators manually adjust the movement until they believe the displacement is correct.117 This displacement is expressed by a vector from a point in the \u201cneutral model\u201d to a corresponding point in the \u201cmorph target.\u201d118 The \u201cneutral model\u201d represents the state of the character\u2019s face before the facial movement.119 The \u201cmorph target\u201d represents the state of the character\u2019s face after the facial movement.120 A value called the \u201cmorph weight\u201d is then assigned to a collection of these vectors for the entire face.121 The invention at issue in McRO, Inc. was a method for automating the process of accurately adjusting this displacement.122 The Federal Circuit determined that this method was patent-eligible because it was not directed to an abstract idea.123 The Federal Circuit highlighted the specificity of the automation method recited in the patent claims at issue.124 The claimed method embodied rules having specific characteristics.125 The patented invention required these rules to be applied in a particular manner.126 The Federal Circuit observed that these rules were not overly broad, but were appropriately limited because they defined the set of morph weights as a function of the timing of the sequence of sounds uttered by the computer graphics character.127 Thus, the Federal Circuit observed that the automation method applied a series of concrete rules that transformed information into a certain format that was used to animate the characters.128   115.  See id.at 1303\u201306.  116.  See id.   117.  Id.  118.  Id. at 1303.  119.  See McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1303 (Fed. Cir. 2016).  120.  See id.  121.  Id. at 1303\u201304.  122.  See id. at 1307. The specification for one of the patents at issue, United States Patent No. 6,307,576, describes that the invention\u2019s objective is to \u201cprovide a method for automatically . . . producing accurate and realistic lip synchronization and facial expressions in animated characters.\u201d Id.  123.  Id. at 1316.  124.  McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1313 (Fed. Cir. 2016).  125.  Id.   126.  Id. at 1315.  127.  See id. at 1313.  128.  Id. at 1315 (Fed. Cir. 2016).  HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 13 The Federal Circuit also emphasized that the \u201cspecific implementation\u201d129 of these concrete rules led to a technical improvement over conventional computer animation technology for two reasons.130 First, the implementation was not the type of conventional methodology that an animator would have used.131 Second, even though the invention used a computer to automate the animation process, the use of a computer alone, without the concrete rules recited in the patent claims, would not have generated the effects achieved by the invention.132 Thus, the Federal Circuit determined that the automation method in McRO, Inc. was patent-eligible.133 The McRO, Inc. case demonstrates that, as pointed out by the Federal Circuit, \u201c[p]rocesses that automate tasks that humans are capable of performing are patent-eligible if properly claimed.\u201d134 3. Self-Referencing Database In Enfish, LLC v. Microsoft Corporation, an invention concerning a self-referencing database was held to be patent-eligible.135 Normally, computer programmers predefine the structures of databases.136 The invention in Enfish, LLC enabled a database to reference itself so that programmers would not need to configure it.137 The United States District Court for the Central District of California determined that the invention was not patent-eligible because it was directed to the abstract idea of \u201cstoring, organizing, and retrieving memory in a logical table.\u201d138 On appeal, the Federal Circuit stated that the District Court\u2019s decision oversimplified the invention and downplayed its benefits.139 The Federal Circuit found that the invention focused on a \u201cspecific asserted improvement in computer capabilities,\u201d140 and was directed to a \u201cspecific implementation of a solution to a problem in the software arts.\u201d141 The invention contributed to elevating conventional technology because it: (1) provided an indexing technique that increased the speed of searching data; (2) made the storing of images and unstructured text more efficient; and (3) augmented the   129.  Id. at 1316.  130.  McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1316 (Fed. Cir. 2016).  131.  Id.  132.  Id. at 1313, 1315.  133.  Id. at 1316.  134.  Id. at 1313.  135.  Enfish, LLC v. Microsoft Corp., 822 F.3d 1327, 1329 (Fed. Cir. 2016).  136.  Id. at 1337.  137.  Id.  138.  Id.  139.  Id. at 1337\u201338.  140.  Id. at 1335\u201336.  141.  Enfish, LLC v. Microsoft Corp., 822 F.3d 1327, 1339 (Fed. Cir. 2016). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 14 JOURNAL OF BUSINESS & TECHNOLOGY LAW flexibility of the database\u2019s configuration.142 The patent claims specifically set forth how the self-referential table contributed to these three benefits.143 Therefore, the invention was determined to be patent-eligible.144 4. Wearable Technology In Fitbit Inc. v. AliphCom,145 the patent eligibility of wearable technology was at issue. Fitbit owns patents that protect inventions related to wearable devices.146 Fitbit alleged that the defendants\u2019 activity-tracking devices infringed Fitbit\u2019s patents.147 In response, the defendants asserted that Fitbit\u2019s patents were invalid for failing to satisfy the patent eligibility requirement.148 The United States District Court for the Northern District of California invalidated two of Fitbit\u2019s patents.149 The District Court found that the patents did not claim a patent-eligible subject matter because they focused on simple data collection, which is an abstract idea.150 However, the District Court determined that the \u201cBiometric Monitoring Device with Heart Rate Measurement Activated by a Single User-Gesture,\u201d claimed in Fitbit\u2019s U.S. Patent No. 9,042,971, might be patent-eligible because it focused on a specific improvement of data collection.151 When a user made a gesture such as moving or staring at the device, a biometric sensor or a button activated the collection of data concerning the user\u2019s heart rate.152 The data collection continued automatically until the level of heartbeat reached a predetermined quality.153 The District Court found that these characteristics pushed the invention beyond the realm of abstract ideas because the characteristics were tied to \u201can improvement to heart rate monitors as a technological tool, which [overcame] the problem of bulky user interfaces and provide[d] a way to more easily and efficiently gather a selective heart rate reading.\u201d154   142.  Id.  143.  See id. at 1336.  144.  Id. at 1339.   145.  Fitbit Inc. v. AliphCom, No. 16-cv-00118-BLF (N.D. Cal. Mar. 2, 2017).   146.  Id. at 1.  147.  Id. at 4.  148.  Id.  149.  Id. at 22.  150.  Id. at 10, 22.  151.  Fitbit Inc. v. AliphCom, No. 16-cv-00118-BLF, slip. op. at 3, 20 (N.D. Cal. Mar. 2, 2017).  152.  Id. at 3.  153.  Id.  154.  Id. at 20. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 15 5. Analysis of Case Law in the United States These cases illustrate the importance of specificity and technical contribution in establishing patent eligibility of artificial intelligence under United States law.155 Where courts in the United States have found that an invention in the area of artificial intelligence meets the requirement for patent eligibility, the courts have pointed out how the patents at issue specifically described the invention\u2019s technical contribution.156 The driving force behind this emphasis on specificity and technical improvement is to prevent preemption.157 When an inventor obtains a patent for a fundamental mental step, the patentee will have a monopoly over it throughout the effective term of the patent.158 During this period, others are deprived of the unfettered use of this mental step because the patentee might file a patent infringement lawsuit against those who use the mental step without the patentee\u2019s authorization.159 This impedes innovation and industrial application.160 Courts in the United States have expressly cautioned against this detrimental outcome.161 The Supreme Court has stated that \u201c[l]aws of nature, natural phenomena, and abstract ideas\u201d are \u201cthe basic tools of scientific and technological work,\u201d adding that the \u201cmonopolization of those tools through the grant of a patent might tend to impede innovation more than it would tend to promote it.\u201d162 The Federal Circuit   155.  See Alice Corp. Pty. v. CLS Bank Int\u2019l, 134 S. Ct. 2347, 2354 (2014) (citing Ass\u2019n for Molecular Pathology v. Myriad Genetics, Inc., 133 S. Ct. 2107, 2116 (2013)); McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1313 (Fed. Cir. 2016); Enfish, LLC v. Microsoft Corp., 822 F.3d 1327, 1336 (Fed. Cir. 2016); Fitbit Inc. v. AliphCom, No. 16-cv-00118-BLF, slip. op. at 10 (N.D. Cal. Mar. 2, 2017)).  156.  Compare McRO, Inc., 837 F.3d at 1316, and Enfish, LLC, 822 F.3d at 1337\u201338, 1346, with Fitbit Inc., slip. op. at 14\u201315, 22.  157.  McRO, Inc., 837 F.3d at 1313\u201314.  158.  Edison Elec. Light Co. v. Sawyer-Man Elec. Co., 53 F. 592, 598 (2d Cir. 1892); see also JOHN GLADSTONE MILLS III ET AL., PATENT LAW FUNDAMENTALS \u00a7 1.6 (2d ed. 2017).  159.  See Richard Stallman & Simon Garfinkle, Viewpoint: Against Software Patents, COMM. ACM, Jan. 1992, at 17; see also MILLS III ET AL., supra note 158, at \u00a71.6; but see Martin Campbell-Kelly, Not All Bad: An Historical Perspective on Software Patents, 11 MICH. TELECOMM. TECH. L. REV. 191, 192\u201393 (2005).   160.  See Benjamin K. Sovacool, Placing a Glove on the Invisible Hand: How Intellectual Property Rights May Impede Innovation in Energy Research and Development (R&D), 18 ALB. L.J. SCI. & TECH. 381, 414 (2008).  161.  Alice Corp. Pty. v. CLS Bank Int\u2019l., 134 S. Ct. 2347, 2354 (2014) (\u201c\u2018[M]onopolization of those tools through the grant of a patent might tend to impede innovation more than it would tend to promote it,\u2019 thereby thwarting the primary object of the patent laws.\u201d) (quoting Mayo Collaborative Servs. v. Prometheus Labs., Inc., 132 S. Ct. 1289, 1293 (2012)); see also U.S. CONST., art. I, \u00a7 8, cl. 8.  162.  Alice Corp. Pty., 134 S. Ct. at 2354 (citing Ass\u2019n for Molecular Pathology v. Myriad Genetics, Inc., 133 S. Ct. 2107, 2116 (2013); Mayo Collaborative Servs. v. Prometheus Labs., Inc., 132 S. Ct. 1289, 1293 (2012)); see also U.S. CONST., art. I, \u00a7 8, cl. 8. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 16 JOURNAL OF BUSINESS & TECHNOLOGY LAW explained that \u201c[t]he concern underlying the exceptions to [35 U.S.C.] \u00a7 101 is not tangibility, but preemption.\u201d163 Innovation in artificial intelligence is incremental.164 Groundbreaking artificial intelligence technology builds upon existing technology.165 Courts\u2019 rejection of patents concerning the fundamental tools of scientific development ensures that the \u201cbuilding blocks\u201d of science will be available for developers to innovate artificial intelligence technology.166 B. The European Patent Convention The European Patent Convention established a \u201ccentrali[z]ed, fundamentally autonomous and uniform procedure\u201d for obtaining a European patent.167 Article 52 of the European Patent Convention pertains to subject matter that is eligible for patent protection.168 Article 52(1) provides that \u201cEuropean patents shall be granted for any inventions, in all fields of technology, provided that they are new, involve an inventive step and are susceptible of industrial application.\u201d169 According to Article 52(2), \u201c(a) discoveries, scientific theories and mathematical methods; (b) aesthetic creations; (c) schemes, rules and methods for performing mental acts, playing games or doing business, and programs for computers; [and] (d) presentations of information\u201d fall within the meaning of Article 52(1).170 Article 52(3) provides that Article 52(2) \u201cshall exclude the patentability of the subject-matter or activities referred to therein only to the extent to which a European patent application or European patent relates to such subject-matter or activities as such.\u201d171 Thus, if a European patent application or patent claims subject matter concerning artificial intelligence and a tribunal determines that the application or patent relates to \u201crules and methods for performing mental acts\u201d per se, the subject matter will not   163.  McRO, Inc. v. Bandai Namco Games Am. Inc., 837 F.3d 1299, 1315 (Fed. Cir. 2016) (citing Mayo, 132 S. Ct. at 1301 (2012)).  164.  See BRYNJOLFSSON & MCAFEE, supra note 74, at 1, 3\u20135.  165.  See id.   166.  See generally Alice Corp. Pty. v. CLS Bank Int\u2019l., 134 S. Ct. 2347, 2354\u201355 (2014) (explaining the pre-emption principle and its role in balancing competing interests of denying patents to allow development against protecting novel methods by granting patents).  167.  EUROPEAN PATENT OFFICE, NATIONAL LAW RELATING TO THE EPC 3 (17th ed. 2015).  168.  EUROPEAN PATENT OFFICE, CONVENTION ON THE GRANT OF EUROPEAN PATENTS 108 (16th ed. June 2016), http://documents.epo.org/projects/babylon/eponet.nsf/0/9AB175036F5487D0C12581AA004AF054/$ File/EPC_16th_edition_2016_en.pdf (compiling the European Patent Convention articles) [hereinafter EUROPEAN PATENT CONVENTION].  169.  Id.  170.  Id.  171.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 17 qualify as a patentable \u201cinvention\u201d under Article 52.172 It will therefore not be protected by a European patent.173 1. Automatically Abstracting Documents The European Patent Office\u2019s Technical Board of Appeal (the \u201cBoard\u201d) determined that a system for automatically summarizing documents are excluded from patentable subject matter under Article 52.174 The system was deemed to be directed to \u201crules and methods for performing mental acts\u201d per se.175 The Board observed that the inventive aspect of the patent claim at issue was a set of innovative rules enabling the system to automatically summarize documents.176 The Board, however, determined that merely reciting the steps for implementing these rules while using conventional computers \u201cdoes not import any technical considerations\u201d to the claimed subject matter.177 According to the Board, the claimed system for automatically summarizing documents only contributed to \u201crules and methods for performing mental acts,\u201d which are expressly excluded from patentability under Article 52(2)(c).178 The applicant argued that the claimed system solved a technical problem because the system\u2019s automating features eliminated the burden of processing voluminous data.179 However, the Board found that \u201cthe true problem to be solved was that of establishing a set of rules for document abstracting and retrieval on the basis of textual properties of the documents to be handled[,] which problem cannot be qualified as technical.\u201d180 As a result, the system for automatically summarizing documents was found ineligible for patent protection under the European Patent Convention.181   172.  See id.  173.  See id. (reasoning that if a claim does not fall within the definition of an \u201cinvention\u201d under Article 51(1), it is ineligible for a patent under the European Patent Convention).  174.  See Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 \u2013 3.5.1, Reasons for the Decision \u00b6 5 (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.  175.  Id. Reasons for the Decision \u00b6 5.  176.  Id. Reasons for the Decision \u00b6\u00b6 6\u20137.  177.  Id. Reasons for the Decision \u00b6 8.  178.  Id. Reasons for the Decision \u00b6\u00b6 5, 9.  179.  Id. Reasons for the Decision \u00b6 12.  180.  Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 \u2013 3.5.1, Reasons for the Decision \u00b6 12 (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.  181.  Id. Reasons for the Decision \u00b6 15. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 18 JOURNAL OF BUSINESS & TECHNOLOGY LAW 2. Smart Server Summarizing Electronic Documents The Board determined that a server that automatically summarized electronic documents exhibited a technical aspect.182 The Board noted that the invention overcame multiple problems with smart servers including slow mobile data connections, limited processing capacities, and a deficiency in the display of mobile devices.183 Once a mobile device requested a summary of an electronic document, the smart server automatically summarized the document and transmitted the summary to the mobile device.184 The Board noted that the method performed by the smart server \u201cappear[ed] in a technical context.\u201d185 By using network servers, the method was carried out through technical means.186 3. Designing Three-Dimensional Receptacles On January 20, 1995, the Board determined that a method and an apparatus for designing a three-dimensional receptacle were patentable \u201cinventions\u201d under Article 52.187 The Board observed that the patent claim recited both excluded and non-excluded subject matter.188 The \u201cinput unit for inputting data\u201d involved \u201cperforming mental acts,\u201d which are excluded from patentability under Article 52(2)(c).189 At the same time, the input, processing, output, and display units were components of computer hardware, which were not excluded from patentability.190 The Board has held that a conglomeration of excluded and non-excluded subject matters would be patentable if the conglomeration makes a technical contribution to \u201ca field outside the field of excluded matters.\u201d191 Applying this precedent, the Board concluded that the claimed subject matter made a technical contribution to \u201cconventional computer art\u201d for two reasons.192 First, the claimed apparatus designed a three-dimensional receptacle, which is a physical object.193 Since the Board\u2019s decision in Case T 208/84 held that the presence   182.  Decision of the European Patent Office, Technical Board of Appeal, Case T 0483/11 \u2013 3.5.01, Reasons for the Decision \u00b6 2.6 (Oct. 13, 2015), http://www.epo.org/law-practice/case-law-appeals/pdf/t110483eu1.pdf.  183.  Id. Reasons for the Decision \u00b6 1.1.  184.  Id. Reasons for the Decision \u00b6\u00b6 1.2\u20131.3.  185.  Id. Reasons for the Decision \u00b6 2.6.  186.  Id.  187.  Decision of the European Patent Office, Technical Board of Appeal, Case T 0605/93 \u2013 3.5.1, Reasons for the Decision \u00b6 5.9 (Jan. 20, 1995), https://www.epo.org/law-practice/case-law-appeals/pdf/t930605eu1.pdf.  188.  Id. Reasons for the Decision \u00b6 5.7.  189.  Id. Reasons for the Decision \u00b6 5.3; see also EUROPEAN PATENT CONVENTION, supra note 168, at 108.  190.  Decision of the European Patent Office, Technical Board of Appeal, Case T 0605/93 \u2013 3.5.1, Reasons for the Decision \u00b6 5.2 (Jan. 20, 1995), https://www.epo.org/law-practice/case-law-appeals/pdf/t930605eu1.pdf.  191.  Id. Reasons for the Decision\u00b6 5.7.  192.  Id. Reasons for the Decision \u00b6 5.8.  193.  Id. Reasons for the Decision \u00b6 5.8. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 19 of a physical entity indicates a \u201ctechnical nature susceptible of being patented,\u201d the Board in the instant case suggested that the three-dimensional receptacle could be equated with the subject matter that was found to be patentable in Case T 208/84.194 Second, the Board found that the input units were unconventional because they were specifically structured to receive a certain type of data representing cross-sections.195 For these reasons, the Board determined that the claimed subject matter was a patentable \u201cinvention\u201d that contributed to developing the conventional art.196 4. Automatic Auctions On April 21, 2004, the Board declined to follow the \u201ccontribution\u201d approach.197 The Board remarked that the European Patent Convention does not provide any basis for allowing tribunals to make comparisons between the claimed subject matter and the prior art to determine whether the subject matter qualifies as a patentable \u201cinvention.\u201d198 The subject matter at issue was a computerized apparatus and a method for automatically performing auctions.199 The Board determined that the apparatus was a patentable \u201cinvention,\u201d not because of the presence of any technical contribution over prior art, but because the apparatus genuinely incorporated \u201cclearly technical features such as a \u2018server computer,\u2019 \u2018client computers\u2019 and a \u2018network.\u2019\u201d200 Likewise, the Board found that the automated auction method was not excluded from patentable subject matter because the method involved technical means.201 The Board emphasized that a claimed subject matter may have a technical character if technical means were used to carry out non-technical activities.202   194.  Id.; see Decision of the European Patent Office, Technical Board of Appeal, Case T 208/84, Reasons for the Decision \u00b6 3, (July 15, 1986), https://www.epo.org/law-practice/case-law-appeals/pdf/t840208ep1.pdf.  195.  Decision of the European Patent Office, Technical Board of Appeal, Case T 0605/93 \u2013 3.5.1, Reasons for the Decision \u00b6 5.8 (Jan. 20, 1995), https://www.epo.org/law-practice/case-law-appeals/pdf/t930605eu1.pdf.  196.  Id. at Reasons for the Decision \u00b6\u00b6 5.8\u20135.9.  197.  Decision of the European Patent Office, Technical Board of Appeal, Case T 258/03 \u2013 3.5.1, Reasons for the Decision \u00b6\u00b6 3.3, 4.3. (Apr. 21, 2004), https://www.epo.org/law-practice/case-law-appeals/ pdf/ t030258ep1.pdf.  198.  Id.  199.  Id. Summary of Facts and Submissions \u00a7 V.  200.  Id. Reasons for the Decision \u00b6 3.7.  201.  Id. Reasons for the Decision \u00b6\u00b6 4.1, 4.7.  202.  Id. Reasons for the Decision \u00b6 4.4. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 20 JOURNAL OF BUSINESS & TECHNOLOGY LAW C. France The patent eligibility jurisprudence of France embodies the framework established by the European Patent Convention.203 Article L611-10 of the French Intellectual Property Code sets forth what is patentable.204 Section 1 of Article L611-10 defines patentable inventions, in all technological areas, as new inventions which involve an inventive step and can be applied industrially.205 Section 2 of Article L611-10 lists the subject matters that are not considered to be inventions under Section 1.206 These excluded subject matters are as follows: (a) discoveries, scientific theories, and mathematical methods; (b) aesthetic creations; (c) schemes, rules, and methods for performing mental acts, playing games, or conducting economic activities, as well as computer programs; and (d) presentations of information.207 Section 3 of Article L611-10 provides that the only instances in which the subject matters listed in Section 2 are excluded from patentability are when a patent application or patent only relates to one of the listed subject matters per se.208 1. Software Contributing to Petroleum Discovery On June 15, 1981, the Court of Appeal of Paris issued a landmark decision in Prospection \u00c9lectrique Schlumberger, declaring that the use of computer software to carry out certain steps of a method did not constitute sufficient grounds to deny patentability.209 In this case, Schlumberger sought a French patent for a method relating to the exploration of petroleum in geological environments.210 The method contained six steps.211 Some of the steps were carried out by a computer program.212   203.  See CODE DE LA PROPRI\u00c9T\u00c9 INTELLECTUELLE [C. PR. INT.] [INTELLECTUAL PROPERTY CODE] art. L611-10.1 (Fr.) (stating that \u201c[i]nventions which are inventive and capable of industrial application are patentable in all technological fields\u201d); see also EUROPEAN PATENT CONVENTION, supra note 168, at 108 (stating that \u201cEuropean patents shall be granted for any inventions, in all fields of technology, provided that they are new, involve an inventive step and are susceptible of industrial application\u201d).  204.  CODE DE LA PROPRI\u00c9T\u00c9 INTELLECTUELLE [C. PR. INT.] [INTELLECTUAL PROP. CODE] art. L611-10 et seq. (Fr.).  205.  Id. \u00a7 1.  206.  Id. \u00a7 2.  207.  Id.   208.  Id. \u00a7 3.  209.  Cour d\u2019appel [CA] [regional court of appeals] Paris, 4th ch. June 15, 1981 (Fr.), https://www.legalis.net/ jurisprudences/cour-dappel-de-paris-4eme-chambre-section-a-arret-du-15-juin-1981.  210.  Id.  211.  Id.  212.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 21 France\u2019s National Institute of Intellectual Property rejected Schlumberger\u2019s patent application, stating that the invention was directed to a series of instructions for machine calculations.213 Schlumberger appealed.214 The Court of Appeal of Paris reversed the Institute\u2019s decision.215 The Court stated that Schlumberger\u2019s method demonstrated a technical character for three reasons.216 First, its goal related to the industry of oil exploration.217 Second, it applied a series of concrete steps.218 Third, the method generated information concerning the physical characteristics of geological environments, which was useful in the industry.219 Moreover, the Court of Appeal determined that a method cannot be deprived of patentability for the sole reason that some of its steps are carried out by a computer program.220 The Court cautioned that denying patentability in such instances would mean that important emerging inventions requiring the use of computer programs would be denied patent protection.221 The Court emphasized that this would lead to \u201caberrant consequences\u201d in practice.222 It should be noted that the Schlumberger case was decided pursuant to France\u2019s patent law of January 2, 1968, which was in force before the enactment of the current form of Article L611-10.223 2. In re Sesame Active System On February 26, 2016, the Court of Appeal of Paris applied Article L611-10 in the case of Sesame Active System.224 The Court analyzed whether a system and a method for automatically optimizing the cost-effectiveness of the transportation of merchandise were patentable subject matters under Article L611-10 of the French Intellectual Property Code.225   213.  Id.  214.  Id.  215.  Cour d\u2019appel [CA] [regional court of appeals] Paris, 4th ch. June 15, 1981 (Fr.) https://www.legalis.net/ jurisprudences/cour-dappel-de-paris-4eme-chambre-section-a-arret-du-15-juin-1981.  216.  Id.  217.  Id.  218.  Id.  219.  Id.  220.  Id.   221.  Cour d\u2019appel [CA] [regional court of appeals] Paris, 4th ch. June 15, 1981 (Fr.), https://www.legalis.net/ jurisprudences/cour-dappel-de-paris-4eme-chambre-section-a-arret-du-15-juin-1981.  222.  Id.  223.  Id.; CODE DE LA PROPRI\u00c9T\u00c9 INTELLECTUELLE [C. PR. INT.] [INTELLECTUAL PROPERTY CODE] art. L611-10 (Fr.).  224.  Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Feb. 26, 2016, [15/01962] (Fr.).  225.  See id. Sur la d\u00e9naturation et l\u2019erreur de droit all\u00e9gu\u00e9es \u00b6 3. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 22 JOURNAL OF BUSINESS & TECHNOLOGY LAW The reloading of vehicles has a significant impact on the profitability of transporting merchandise.226 The patent application at issue presented a system and a method for optimizing the loading of vehicles for both the outbound route and the return route.227 The Court of Appeal of Paris found that these subject matters did not have any technical characteristic because the problem solved by the invention was economic in nature.228 The Court observed that the system and method categorized data into three sets, each representing the load, time, and space.229 The system and method also grouped data into two subsets representing shippers and transporters.230 The Court stated that these features were directed to the simple organization of information and did not present any technical features.231 In addition, the Court found that the central unit recited in the patent application was not technical either since it merely compared data, which was a simple implementation of a mental step.232 Although the invention contributed to automating the optimization process,233 the Court pointed out that automation is commonly used, especially in business.234 Moreover, the Court noted that the patent application failed to set forth a specific configuration for a solution exhibiting a technical character.235 For these reasons, the Court of Appeal of Paris concluded that the claimed system and method were directed to an economic method, which is excluded from patentability under Article L611-10 of the French Intellectual Property Code.236 As a result, the automation system and method were determined ineligible for patent protection.237 3. In re Dassault Syst\u00e8mes Similarly, on December 16, 2016, the Court of Appeal of Paris invalidated a method claim for lack of patentability under Article L611-10 in the case of Dassault Syst\u00e8mes.238 The claim recited a method that dynamically selected categories and key words that would yield better results if entered as query terms in a search engine.239   226.  Id. Sur la demande de brevet en cause \u00b6 1.  227.  Id. \u00b6 2.  228.  Id. \u00b6 4.  229.  Id. \u00b6 9.  230.  Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Feb. 26, 2016, [15/01962] (Fr.), Sur la demande de brevet en cause, \u00b6 9.  231.  Id.  232.  Id.  233.  Id. \u00b6 11.  234.  Id.  235.  Id.   236.  Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Feb. 26, 2016, [15/01962] (Fr.), Sur le rapport de recherches, \u00b6 3.  237.  Id.  238.  Cour d\u2019appel [CA] [regional court of appeal] Paris, 2e ch., Dec. 16, 2016, [14/06444] (Fr).  239.  Id., Faits et proc\u00e9dure, \u00b6 2. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 23 The Court of Appeal of Paris reasoned that categories and key words result from subjective mental steps and were not technical tools.240 Thus, the Court determined that the elements of the claimed method were directed to a mental step devoid of any technical character.241 4. Analysis of Case Law Under the European Patent Convention and French Patent Law The decisions rendered under the European Patent Convention and French patent law illustrate the importance of claimed subject matter to have a technical character in order to qualify as a patentable \u201cinvention.\u201d It is imperative that the patent application articulate how the artificial intelligence-related invention employs technical means to resolve a technical problem and generate a technical effect.242 This emphasis on technical character reflects the long-standing practice of the member states of the European Patent Convention to require that an invention have a technical effect in order to be worthy of patent protection.243 The European Patent Office explains that it has been a \u201cpart of the European legal tradition since the early days of the patent system that patent protection should be reserved for technical creations.\u201d244 D. Japan While the technical characteristics of an invention play a key role in demonstrating patent eligibility in the European jurisprudence, a central inquiry under Japanese law is whether the claimed subject matter applies laws of nature.245 This is because Article 2, Section 1 of the Japanese Patent Act defines an \u201cinvention\u201d as a high-level creation of technical ideas using laws of nature.246   240.  Id.  241.  Id.  242.  Stanislas Roux-Vaillard, France, in THE INTELL. PROP. REV. 78, 82 (5th ed. 2016) (\u201cIn practice, products and processes that provide technical means for solving a technical problem are, as a general rule, patentable.\u201d).  243.  Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 \u2013 3.5.1, Reasons for the Decision \u00b6 3 (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.  244.  EUROPEAN PATENT OFFICE, PATENTS FOR SOFTWARE? EUROPEAN LAW AND PRACTICE 12 (2013), http://documents.epo.org/projects/babylon/eponet.nsf/0/a0be115260b5ff71c125746d004c51a5/$FILE/patents_for_software_en.pdf [hereinafter EUROPEAN PATENT OFFICE, PATENTS FOR SOFTWARE?].  245.  Compare EUROPEAN PATENT OFFICE, PATENTS FOR SOFTWARE?, supra note 244, with Tokkyoh\u014d [Patent Act], No. 109 of 2006, art. 2, \u00b6 1 (Japan).  246.  Tokkyoh\u014d [Patent Act], No. 109 of 2006, art. 2, para. 1 (Japan). Article 29, Section 1 of the Japanese Patent Act provides that a person who has invented an invention capable of industrial application may obtain a patent for that invention if, before the filing of the patent application, the invention was, both within Japan and abroad, (1) not publicly known, (2) not publicly practiced, and (3) not made available for public use through publications or online transmission. Id.  HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 24 JOURNAL OF BUSINESS & TECHNOLOGY LAW 1. Computer-Based Dental Treatment System On June 24, 2008, the Intellectual Property High Court of Japan interpreted the term \u201cinvention\u201d in Article 2, Section 1 to mean something that is completed through the steps of: (1) establishing a particular technical problem; (2) employing technical means to solve the problem; and (3) confirming that the technical configuration actually produces an effect that contributes to achieving the goal.247 The High Court stated that a mental step per se is not an \u201cinvention,\u201d and is therefore not patent-eligible.248 However, the High Court noted that if the essence of the claimed subject matter assists humans in carrying out mental steps, or provides technical means that replace mental steps performed by humans, neither the inclusion of mental steps in the subject matter nor the subject matter\u2019s linkage to mental steps necessarily disqualifies the subject matter from being an \u201cinvention\u201d under Article 2, Section 1 of the Japanese Patent Act.249 The patent claim at issue recited a computer-based dental treatment system.250 The system evaluated the required dental treatment and formulated an initial treatment plan.251 The system also suggested various designs for dental prosthesis.252 The High Court observed that this system helped dentists create the best dental treatment plan for patients and select the best material for dental prosthesis by resolving the overflow of information associated with the advancement of dental treatment.253 The High Court found that this system provided technical means for assisting dental treatment and was not directed to mental steps per se.254 Hence, the Court concluded that the computerized system was a \u201ccreation of technical thought using laws of nature\u201d and was therefore an \u201cinvention\u201d under Article 2, Section 1 of the Japanese Patent Act.255 2. Knowledge Database System On September 24, 2014, the High Court decided that a knowledge database system related to artificial intelligence did not qualify as an \u201cinvention\u201d under Article 2,   247.  Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], June 24, 2008, Hei 19 (Gy\u014d ke) no. 10369, IP JUDGMENTS DATABASE 1, 25, http://www.ip.courts.go.jp/app/files/hanrei_en/068/000068.pdf (Japan).  248.  Id.  249.  Id.  250.  Id. at 2.  251.  Id.  252.  Id.  253. Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], June 24, 2008, Hei 19 (Gy\u014d ke) no. 10369, IP JUDGMENTS DATABASE 1, 34, http://www.ip.courts.go.jp/app/files/hanrei_en/068/000068.pdf (Japan).  254.  Id. at 35.  255.  Id. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 25 Section 1.256 The patent application at issue described that many artificial intelligence researchers study the complex ways in which humans process information with written words.257 The application stated that an experiment for generating artificial intelligence using a programing language called PROLOG was conducted, but \u201chad limits.\u201d258 The patent application claimed a method for structuring a knowledge database system that could express objects and attributes without relying on words.259 The High Court determined that this method did not qualify as an \u201cinvention\u201d because the patent application failed to specify the technical significance and effect that could be achieved by a database that did not rely on words.260 Further, the application did not describe any technical problems that conventional databases were causing due to their reliance on words.261 Thus, the High Court concluded that the claimed knowledge database and computing method were nothing more than abstract concepts or artificial rules concerning the construction of databases.262 The High Court stated that, although the method used computers, only generic computations were performed.263 3. Analysis of Case Law in Japan The jurisprudence of patent eligibility in Japan resembles the American jurisprudence in that a specific disclosure of the invention\u2019s configuration and improvements over conventional technology are elements that contribute to a judicial finding that a claimed subject matter is patent-eligible.264 The Japanese jurisprudence is also reminiscent of the European jurisprudence in that the technical means   256.  Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], Sept. 24, 2014, Hei 26 (Gy\u014d ke) no. 10014, IP JUDGMENTS DATABASE 1, 34, http://www.ip.courts.go.jp/app/files/hanrei_jp/514/084514_hanrei.pdf (Japan).  257.  Id. at 19.  258.  See id. at 22.  259.  See id. at 25, 29\u201330.  260.  Id. at 29.  261.  Id.  262.  Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], Sept. 24, 2014, Hei 26 (Gy\u014d ke) no. 10014, IP JUDGMENTS DATABASE 1, 34, http://www.ip.courts.go.jp/app/files/hanrei_jp/514/084514_hanrei.pdf (Japan).  263.  Id.  264.  Compare Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], Sept. 24, 2014, Hei 26 (Gy\u014d ke) no. 10014, IP JUDGMENTS DATABASE 1, 34, http://www.ip.courts.go.jp/app/files/hanrei_jp/514/084514_hanrei.pdf  (Japan) (finding that a technical effect or improvement is necessary for patent eligibility), with Alice Corp. Pty. v. CLS Bank Int\u2019l, 134 S. Ct. 2347, 2359 (2014) (finding that applications that present abstract concepts alone without showing technological improvements are not enough for patent eligibility and that petitioner\u2019s claims did not purport to improve the functioning of the computer itself). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 26 JOURNAL OF BUSINESS & TECHNOLOGY LAW associated with the claimed subject matter constitute a vital component of the High Court\u2019s interpretation of what a patentable \u201cinvention\u201d is.265 The High Court\u2019s decision highlights that when a system implements mental steps to assist humans, the system will not be precluded from being a patent-eligible \u201cinvention.\u201d266 This emphasis on an invention\u2019s capacity to assist humans resonates with the increasing use of artificial intelligence as a tool to aid human activities.267 E. Singapore Singapore\u2019s Patents Act provides that \u201ca patentable invention is one that satisfies the following conditions: (a) the invention is new; (b) it involves an inventive step; and (c) it is capable of industrial application.\u201d268 Section 8.23 of the Examination Guidelines, outlined by the Intellectual Property Office of Singapore, confirms that \u201c[m]ethods that are considered mental acts or schemes are generally not inventions.\u201d269 According to Section 8.2, examiners assessing patent eligibility will identify the \u201cinventive concept\u201d claimed in the patent application.270 Section 8.3 further states that examiners should \u201cidentify the actual contribution which is made by the claimed subject matter, having regard to the problem to be solved, how the claimed subject matter works, and what its advantages are.\u201d271 This examination practice suggests that, as in the United States, Europe, and Japan, specific descriptions of the technical significance of artificial intelligence-related inventions are important for demonstrating the patent eligibility of artificial intelligence in Singapore. Singapore\u2019s Intellectual Property Office has conferred patent eligibility to a number of artificial intelligence-related inventions. For instance, on May 4, 2016, the Intellectual Property Office determined that an invention for automatic   265.  Compare Chiteki Zaisan K\u014dt\u014d Saibansho [Intellectual Prop. High Ct.], Sept. 24, 2014, Hei 26 (Gy\u014d ke) no. 10014, IP JUDGMENTS DATABASE 1, 29, http://www.ip.courts.go.jp/app/files/hanrei_jp/514/084514_hanrei .pdf (Japan) (holding that failure to define the technical significance of the creation will not qualify it as an \u201cinvention\u201d), with Decision of the European Patent Office, Technical Board of Appeal, Case T 0483/11 \u2013 3.5.01, Reasons for the Decision \u00b6 2.6 (Oct. 13, 2015), http://www.epo.org/law-practice/case-law-appeals/pdf/t110 483eu1.pdf (stating that the presence of a technical character in a claimed method is relevant to the issue of whether the method qualifies as an invention under Article 52(1) of the European Patent Convention).  266.  See Chiteki Zaisan K\u014dt\u014d Saibansho [Intell. Prop. High Ct.], June 24, 2008, Hei 19 (Gy\u014d ke) no. 10369, IP JUDGMENTS DATABASE 1, 25, http://www.ip.courts.go.jp/app/files/hanrei_en/068/000068.pdf (Japan) (stating that an invention at issue \u201ccan be understood as providing a computer-based technical means for assisting dental treatment\u201d).  267.  See id. at 35.  268.  Singapore Patents Act, ch. 221, Art. 13(1) (2005).  269.  INTELL. PROP. OFFICE OF SINGAPORE, EXAMINATION GUIDELINES FOR PATENT APPLICATIONS AT IPOS 260 (2017), http://www.wipo.int/edocs/lexdocs/laws/en/sg/sg069en.pdf.  270.  Id. at 253.  271.  Id. at 253\u201354. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 27 aggregation of financial data satisfied Singapore\u2019s patent eligibility requirement.272 On February 3, 2017, the Intellectual Property Office granted a patent for an invention that automatically cultured biological cells.273 IV. IMPLICIT, DE FACTO REQUIREMENTS FOR PATENT-ELIGIBLE ARTIFICIAL INTELLIGENCE The patent eligibility jurisprudence under the laws of the United States, the European Patent Convention, France, Japan, and Singapore indicates that there are implicit, de facto requirements for establishing the patent eligibility of artificial intelligence technology.274 A. Specific Portrayal of the Invention First, tribunals require that artificial intelligence inventions be described with specificity.275 Methods, systems, and apparatus that are designed to perform various mental steps of humans have been denied patent eligibility because the patents at issue did not articulate the problem that the invention was designed to solve, the specific configuration of the invention, and the invention\u2019s effect.276 B. Manifestation of Technical Characteristics Second, patent-eligible artificial intelligence often exhibits a technical character.277 The importance of having a technical character is salient in the European jurisprudence.278 Generally speaking, decisions rendered by United States courts do not emphasize the significance of technical characteristics because the primary inquiry is whether the invention is directed to an abstract idea and whether it demonstrates inventiveness.279 Yet, the automated computer graphics system in McRO, Inc., the self-referencing database in Enfish, LLC, and the wearable technology in Fitbit, Inc., all possessed technical characteristics that were found to meet the patent eligibility requirement.280 This is true in Japan and Singapore as well.281 A technical characteristic is a critical component of patent-eligible inventions   272.  Singapore Patent No. 11201507409Q (issued May 4, 2016).  273.  Singapore Patent No. 11201405632P (issued Feb. 3, 2016).  274.  See supra Part III.   275.  See supra Part III.  276.  See supra Part III.  277.  See supra Parts III.A\u2013III.E.  278.  See supra Part III.B.4.  279.  See Alice Corp. Pty. v. CLS Bank Int\u2019l, 134 S. Ct. 2347, 2354 (2014).  280.  See supra Parts III.A.1, III.A.2, III.A.4.  281.  See supra Parts III.D\u2013III.E. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 28 JOURNAL OF BUSINESS & TECHNOLOGY LAW under Japanese law.282 Singapore\u2019s Examination Guidelines illustrate that \u201cvarious technical features\u201d contribute to patent eligibility.283 C. Transcending Generic Computation Third, tribunals have denied patent eligibility when artificial intelligence simply makes generic computers perform mental steps.284 The Federal Circuit\u2019s decisions indicate that artificial intelligence inventions that concretely improve conventional technology are patent-eligible.285 However, the European Patent Office\u2019s Technical Board of Appeal declined to follow this \u201ccontribution\u201d approach.286 This difference may be explained by the recurring problem of patent assertion entities in the United States.287 Dismissing meritless patent lawsuits early helps resolve this issue.288 The likelihood that lawsuits will be dismissed early is greater when the threshold for establishing patent eligibility is higher.289 The \u201ccontribution\u201d approach elevates this threshold by effectively adding novelty and inventiveness evaluations to the patent eligibility inquiry.290 Meanwhile, the European Patent Convention provides a basis for the patent eligibility jurisprudence constructed through decisions of the European Patent Office.291 Following the statutory structure of the European Patent Convention, the European Patent Office\u2019s Technical Board of Appeal separates the patent eligibility   282.  See supra Part II.D.  283.  INTELL. PROP. OFFICE OF SINGAPORE, EXAMINATION GUIDELINES FOR PATENT APPLICATIONS AT IPOS 254\u201355 (2017), http://www.wipo.int/edocs/lexdocs/laws/en/sg/sg069en.pdf.  284.  See, e.g., Gottschalk v. Benson, 409 U.S. 63, 67 (1972) (\u201c[M]ental processes, and abstract intellectual concepts are not patentable, as they are the basic tools of scientific and technological work.\u201d).  285.  See, e.g., Enfish, LLC v. Microsoft Corp., 822 F.3d 1327, 1333 (Fed. Cir. 2016) (\u201cThe patents teach that multiple benefits flow from this design. First, the patents disclose an indexing technique that allows for faster searching of data than would be possible with the relational model. . .the patents teach that the self-referential model allows for more effective storage of data other than structured text, such as images and unstructured text . . . the patents teach that the self-referential model allows more flexibility in configuring the database.\u201d).  286.  See generally Decision of the European Patent Office, Technical Board of Appeal, T 258/03 \u2013 3.5.1 (Apr. 21, 2004), https://www.epo.org/law-practice/case-law-appeals/pdf/t030258ep1.pdf.  287.  For an overview on patent assertion entities, see FEDERAL TRADE COMMISSION, PATENT ASSERTION ENTITY ACTIVITY: AN FTC STUDY (2016), https://www.ftc.gov/reports/patent-assertion-entity-activity-ftc-study.  288.  Circuit Judge Haldane Mayer, in a concurring opinion, has stated that the court\u2019s evaluation of patent eligibility \u201cat the outset not only conserves scarce judicial resources and spares litigants the staggering costs associated with discovery and protracted claim construction litigation, it also works to stem the tide of vexatious suits brought by the owners of vague and overbroad business method patents.\u201d OIP Techs., Inc. v. Amazon.com, Inc., 788 F.3d 1359, 1364\u20135 (Fed. Cir. 2015) (Mayer, J., concurring).  289.  See id.  290.  See Mayo Collaborative Servs. v. Prometheus Labs., 566 U.S. 66, 73, 77 (2012).  291.  See supra Part III.B (describing the patent eligibility jurisprudence under the European Patent Convention). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 29 inquiry under Article 52 of the European Patent Convention from the novelty and inventiveness inquiries under Articles 54 and 56.292 V. ARTIFICIAL INTELLIGENCE IMPOSES CHALLENGES TO ESTABLISHING PATENT ELIGIBILITY The implicit, de facto requirements for patent eligibility may be at odds with the nature presently manifested by artificial intelligence technology.293 This potential clash is disconcerting because if the patent eligibility jurisprudence imposes requirements that cannot realistically be met by artificial intelligence inventions due to the technology\u2019s inherent nature, these inventions will inevitably be foreclosed from patent protection.294 Patent eligibility laws, in their current form, would not be able to confer their benefits to protect artificial intelligence technology. A. The \u201cInexplicability Problem\u201d The mechanism of artificial intelligence is often inexplicable.295 Even computer scientists who write computer programs for artificial intelligence systems sometimes have difficulty explaining why and how their computer programs made an artificial intelligence system behave in a certain way.296 This reality conflicts with the de facto specificity requirement.297   292.  See Decision of the European Patent Office, Technical Board of Appeal, Case T 258/03 \u2013 3.5.1, Reasons for the Decision \u00b6\u00b6 3.1 (Apr. 21, 2004), https://www.epo.org/law-practice/case-law-appeals/pdf/t030258ep1.pdf (\u201cThe verification that [the] claimed subject-matter is an invention within the meaning of Article 52(1) EPC is in principle a prerequisite for the examination with respect to novelty, inventive step and industrial application since these latter requirements are defined only for inventions (cf[.] Articles 54(1), 56, and 57 EPC). The structure of the EPC therefore suggests that it should be possible to determine whether subject-matter is excluded under Article 52(2) EPC without any knowledge of the state of the art (including common general knowledge).\u201d).  293.  See infra Parts V.A\u2013V.C.  294.  See supra Part III.  295.  Morgane Tual, Au-del\u00e0 des Fantasmes, Quels Sont les Probl\u00e8mes Concrets que Pose L\u2019intelligence Artificielle?, LE MONDE.FR : PIXELS (Aug. 3, 2017), http://www.lemonde.fr/pixels/article/2017/08/03/au-dela-des-fantasmes-quels-sont-les-problemes-concrets-que-pose-l-intelligence-artificielle_5168330_4408996.html; Hubert Guillaud & R\u00e9mi Sussan, L\u2019intelligence Artificielle Va-t-elle Rester Imp\u00e9n\u00e9trable? LE MONDE.FR: BLOGS, (Oct. 30, 2016), http://internetactu.blog.lemonde.fr/2016/10/30/lintelligence-artificielle-va-t-elle-rester-impenetrable. See also JACKSON, JR., supra note 52, at 396 (expressing this problem as the \u201clack of understanding\u201d question, explaining that \u201cthe possibility exists that intelligent machines might be too complicated for us to understand in situations that require real-time analyses\u201d); HOFSTADTER, supra note 66, at 679 ( \u201c[T]he \u2018pond\u2019 of an AI program will turn out to be so deep and murky that we won\u2019t be able to peer all the way to the bottom.\u201d).  296.  See Guillaud & Sussan, supra note 295 (pointing out that, even the creator of computer programs for artificial intelligence programs that operate by using data to discover rules will have difficulty explaining why and how the artificial intelligence works).  297.  See id.; see infra Parts V.A\u2013V.C. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 30 JOURNAL OF BUSINESS & TECHNOLOGY LAW Under the present patent eligibility jurisprudence, an invention is more likely to be deemed ineligible for patent protection if the configuration of the invention is not described with specificity.298 However, the configuration of some artificial intelligence cannot be understood, let alone described with specificity.299 Such artificial intelligence inventions are likely to be foreclosed from patent protection. B. Omnipresence of Artificial Intelligence in Non-Technical Liberal Arts Next, patent-eligible inventions often demonstrate a technical character.300 Yet, artificial intelligence is used broadly in the liberal arts including economics, music, art, psychology, linguistics, and literature.301 The European Patent Office\u2019s Technical Board of Appeal has pointed out that \u201can invention may have technical aspects which are hidden in a largely non-technical context.\u201d302 For example, even if artificial intelligence technology used in music employs technical means, a tribunal might find that the invention\u2019s \u201cessence\u201d is inexorably tied to music, a non-technical discipline, and is therefore non-technical in nature. This creates a problem because patent eligibility may ultimately be denied for lack of technical character.303 This problem arguably originates from the practice of extracting the \u201cessence\u201d of an invention, rather than giving effect to every element recited in a patent claim.304 C. Transfiguration of Conventional Artificial Intelligence Technology Artificial intelligence research may be heading in an opposite direction from the inclinations associated with patent eligibility. According to the present patent eligibility jurisprudence, courts are less likely to determine that conventional, general-purpose computers performing mental steps are patent-eligible.305 This means that the more specialized the invention, the more likely it will be held patent-eligible, compared to inventions having generalized features.306 In contrast, an artificial intelligence system that acts as a generalized problem solver, carrying out   298.  See supra Parts II.A\u2013II.E, III.A.  299.  See supra note 295\u201396 and accompanying text.  300.  See supra Part IV.B.  301.  See generally HADJERES & PACHET, supra note 23; Gatys et al., supra note 18.  302.  Decision of the European Patent Office, Technical Board of Appeal, Case T 0258/03 \u2013 3.5.1, Reasons for the Decision \u00b6 3.6 (Apr. 20, 2004), https://www.epo.org/law-practice/case-law-appeals/pdf/t030258ep1.pdf.  303.  Decision of the European Patent Office, Technical Board of Appeal, Case T 1883/09 \u2013 3.2.04, Reasons for the Decision \u00b6\u00b6 2.3, 3.6, (Sep. 20, 2012), http://www.epo.org/law-practice/case-law-appeals/pdf/t091883 eu1.pdf.  304.  See, e.g., supra Parts III.A.3, III.B.1, III.D.1, III.E.  305.  See supra Parts III.A\u2013III.E, IV.C.  306.  See supra Parts III.A\u2013III.E, IV.C. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 31 a vast array of cognitive functions, is regarded as more advanced than a specialized artificial intelligence system that is tailored to carry out a particular step.307 Another discrepancy between the directions of patent eligibility laws and artificial intelligence research relates to the concept of mental acts per se.308 Under the current patent eligibility jurisprudence, an invention is more likely to be found patent-eligible if it is further distinguished from mental steps per se.309 Presently, this tendency is congruous with various artificial intelligence technologies because there is a dichotomy between artificial intelligence and the actual mental steps that take place in the human brain.310 Artificial intelligence uses computational methods to reproduce the results of human mental activity.311 Machines that achieve the end results of human cognition are different from devices that replicate every biological and cognitive process that occurs in the human brain to reach those results.312 A quintessential example is an automatic translating machine.313 It outputs a translation by using statistical analysis and numerical methods.314 This is different from how humans translate languages.315 However, rather than distancing itself further and further from actual biological mental activities, artificial intelligence appears to be approaching mental steps per se. Professor Philip C. Jackson, Jr. explains that \u201cintrospection is probably the source most commonly used in artificial intelligence research for information about specific problem-solving abilities of human intelligence.\u201d316 On July 27, 2017, computer scientists created a nanometric component modeled from a biological neuron, and used it instead of computers to create an artificial intelligence machine that could recognize vocal pronunciations with a success rate of 99.6%.317 Similarly, researchers are trying to incorporate human emotions into the mechanisms of artificial intelligence as an alternative to relying solely on computational logic.318 Researchers have also discovered that modeling biological evolution and simulating   307.  See JACKSON, JR., supra note 52, at 108.  308.  See infra Part V.C.  309.  See supra Parts III.A\u2013III.E, IV.B.  310.  See infra Part V.C.  311.  See supra Part II.  312.  See infra Part V.C.   313.  See Brian Hayes, The Manifest Destiny of Artificial Intelligence, 100 AM. SCIENTIST 282, 285 (2012).  314.  See id.  315.  See id.  316.  JACKSON, JR., supra note 52, at 8.  317.  David Larousserie, Des chercheurs dotent un ordinateur de neurones artificiels, LEMONDE.FR. (Aug. 8, 2017), http://www.lemonde.fr/sciences/article/2017/08/08/des-chercheurs-dotent-un-ordinateur-de-neurones-artificiels_5170010_1650684.html.  318.  Aaron Sloman, What is Artificial Intelligence?, SCH. OF COMPUT. SCI., UNIV. OF BIRMINGHAM, 2007, http://www.cs.bham.ac.uk/research/projects/cogaff/misc/aiforschools.html. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 32 JOURNAL OF BUSINESS & TECHNOLOGY LAW the evolutionary process offer insights for solving problems.319 These developments indicate that artificial intelligence research is evolving and expanding in a direction that approaches mental steps per se. VI. LAW\u2019S POTENTIAL TO ADAPT TO THE TECHNOLOGICAL PROGRESS OF ARTIFICIAL INTELLIGENCE The progress of technology in general can be expressed as an exponential curve.320 According to Professors Erik Brynjolfsson and Andrew McAfee, the world is currently at the stage where the exponential curve is becoming increasingly steep.321 This stage is where \u201cthe numbers start to become so big they are inconceivable.\u201d322 Moreover, these numbers \u201cleave our intuition and experience behind.\u201d323 Thus, how precisely artificial intelligence will develop and be used in society in the future is unknown. The present patent eligibility jurisprudence under the laws of the United States, European Patent Convention, France, Japan, and Singapore is supple enough to adapt to future evolutions in artificial intelligence technology.324 There are three reasons for this proposition.325 A. Common Law Reflecting Public Policy Arguments First, the United States legal system embodies the tradition of common law, formed through the accumulation of judicial decisions.326 Although the role of the judiciary is not to create law but to apply existing law, the inherent nature of common law allows room for policy arguments to be reflected in the way law is applied in the pursuit of justice and in light of the social needs that will surface due to the growing prevalence and advancement of artificial intelligence.327   319.  Id.; see also L. Spector, Evolution of artificial intelligence, 170 ARTIFICIAL INTELLIGENCE 1251, 1252 (2006) (discussing the advancement of evolutionary algorithms).  320.  BRYNJOLFSSON & MCAFEE, supra note 74, at 4\u20136.  321.  Id. at 48.  322.  Id. at 46.  323.  Id. at 47.  324.  See infra Parts VI.A\u2013VI.C.  325.  See infra Parts VI.A\u2013VI.C.  326.  See Thomas Healy, Stare Decisis as a Constitutional Requirement, 104 W. VA. L. REV. 43, 44 (2001) (discussing the history and effectiveness of stare decisis in American jurisprudence).  327.  See supra Part III.A. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 33 B. Invigorating Undefined Statutory Terms in Civil Law Second, the European Patent Convention does not define what an \u201cinvention\u201d is.328 This lack of definition confers flexibility to the jurisprudence of patent eligibility because tribunals will inevitably confront the issue of construing the term \u201cinvention.\u201d329 The European legislature decided not to define the word \u201ctechnology\u201d either, ensuring that \u201cadequate protection would be available for the results of developments in the future in fields of research which the legislator could not foresee.\u201d330 The legislative records on the European Patent Convention state that \u201cit will remain incumbent on EPO practice and case law to determine whether subject-matter claimed as an invention has a technical character and to further develop the concept of invention in an appropriate manner, in light of technical developments and the state of knowledge at the time.\u201d331 Similarly, the Japanese Patent Act provides a broad definition of \u201cinvention\u201d and leaves the concrete application and interpretation of this definition to courts and academic commentary.332 Therefore, the breadth of the interpretation of statutory language such as \u201cinvention\u201d and \u201cnatural laws\u201d is capable of evolving. C. Judicial Versatility Third, judicial evolution has already taken place in response to technological progress.333 In October 1988, the European Patent Office\u2019s Technical Board of Appeal denied the patent eligibility of a system that automatically summarized documents.334 The Board decided that this system did not have a technical character.335 Approximately twenty-seven years later, the Board conferred patent eligibility to a similar invention.336 The invention was a server that automatically   328. EUROPEAN PATENT OFFICE, CASE LAW OF THE BOARD OF APPEAL OF THE EUROPEAN PATENT OFFICE 2 (8th ed. 2016) [hereinafter CASE LAW OF THE BOARD OF APPEAL OF THE EPO]; see also EUROPEAN PATENT CONVENTION, supra note 168, at 108.  329.  See EUROPEAN PATENT CONVENTION, supra note 168, at 108.  330.  Decision of the European Patent Office, Enlarged Board of Appeal, Case G 0002/07, Reasons for the Decision \u00b6 6.4.2.1 (Dec. 9, 2010), http://www.epo.org/law-practice/case-law-appeals/pdf/g070002ex1.pdf.  331.  EUROPEAN PATENT OFFICE, OFFICIAL JOURNAL OF THE EUROPEAN PATENT OFFICE 1, 48 (special ed. 4 2007), http://archive.epo.org/epo/pubs/oj007/08_07/special_edition_4_epc_2000_synoptic.pdf; see also Anton S. Holzwarth, Patent Protection and Computer Programs in the Practice of the European Patent Office 1, 3 (1998), http://www.wipo.int/mdocsarchives/WIPO_CNR_KYI_98/WIPO_CNR_KYI_98_3_E.pdf.  332.  See infra Part III.D; see also NOBUHIRO NAKAYAMA, PATENT LAW 94 (3d ed. 2016) (translation by author).  333.  See infra Part V.C.  334.  Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 - 3.5.1 (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.  335.  Id. Summary of Facts \u00b6 X.  336.  Decision of the European Patent Office, Technical Board of Appeal, Case T 1286/10 -3.5.05 (Apr. 14, 2015), http://www.epo.org/law-practice/case-law-appeals/pdf/t101286eu1.pdf. HASHIGUCHI_Artificial Intelligence and Patent Eligibility (DO NOT DELETE) 1/2/2018  3:22 PM ARTIFICIAL INTELLIGENCE AND PATENT ELIGIBILITY 34 JOURNAL OF BUSINESS & TECHNOLOGY LAW summarized electronic documents.337 This time, the Board found that the automatic summarizing invention had a technical character.338 These two cases may illustrate a change in how the Board construes the concept of \u201ctechnical character.\u201d339 Judicial progression has occurred in Japan as well.340 The phrase \u201cnatural law\u201d was previously interpreted narrowly to mean \u201cthe production of objects.\u201d341 However, such a narrow interpretation is no longer supported by Japanese courts.342 Moreover, with the prevalence of computer programs, the Japanese Patent Office repeatedly revised its examination guidelines to allow more flexibility in the interpretation of the definition of the term \u201cinvention,\u201d ensuring that the threshold of patent eligibility will be met by a variety of software-related applications.343 The adaptability of patent eligibility laws demonstrated in response to the development of servers and software strongly suggests that, similar to common law in the United States, the jurisprudence under the civil law systems of Europe and Japan is equally capable of adapting to the progress of artificial intelligence.344 The same is true of Singapore\u2019s Patents Act because Article 13(1) uses broad terminology to define the phrase \u201cpatentable invention.\u201d345 It should be kept in mind that judicial versatility must originate from the statute.346 As former Chief Judge Randall Rader of the United States Court of Appeals for the Federal Circuit perspicaciously observed, consulting the statute provides a powerful remedy.347 VII. CONCLUSION The rapid advancement of artificial intelligence is entering unknown territory. Legal systems around the world are now in a global artificial intelligence revolution. Artificial intelligence displays certain characteristics that raise the question of whether the implicit de facto requirements presently imposed by the patent eligibility   337.  Id.  338.  Id.  339.  Decision of the European Patent Office, Technical Board of Appeal, Case T 22/85 - 3.5.1, Reasons for the Decision \u00b6 12 (Oct. 5, 1988), http://www.epo.org/law-practice/case-law-appeals/pdf/t850022ep1.pdf.  340.  See Sadao Nagaoka, Reform of Patent System in Japan and Challenges, in 21ST CENTURY INNOVATION SYSTEMS FOR JAPAN AND THE UNITED STATES: LESSONS FROM A DECADE OF CHANGE: REPORT OF A SYMPOSIUM 153 (The National Academies Press 2009) (noting several important changes to the patent system that have taken place in Japan since the 1970s).  341.  See, e.g., KAZUHIKO TAKEDA, THE KNOWLEDGE OF PATENTS 31 (8th ed. 2006) (translation by author).  342.  See id. (translation by author).  343.  Id. at 31\u201333 (translation by author).  344.  See supra Parts VI.A\u2013VI.C.  345.  Singapore Patents Act, ch. 221, Art. 13(3) (2005).  346.  See, e.g., CLS Bank Int\u2019l. v. Alice Corp. Pty., 717 F.3d 1269, 1335 (Fed. Cir. 2013).  347.  See id. at 1335 (\u201cAnd the remedy is the same: consult the statute! The statute offers a patent to both inventions and discoveries, including simply an improvement on a known process or product.\u201d). HASHIGUCHI_Artificial Intelligence and Patent Eligibility (Do Not Delete) 1/2/2018  3:22 PM  MIZUKI HASHIGUCHI VOL. 13, NO. 1 2017 35 jurisprudence can realistically be met by burgeoning artificial intelligence inventions. Nevertheless, the future is bright because the patent eligibility jurisprudence under the laws of the United States, the European Patent Convention, France, Japan, and Singapore has evinced a certain degree of versatility. This flexibility enables the patent eligibility jurisprudence to evolve in response to the technical and societal changes associated with the vibrant development of artificial intelligence. Patent eligibility laws are therefore capable of undergoing a metamorphosis to encourage, stimulate, cheer, and reward courageous and strenuous endeavors to create artificial intelligence technology that is truly spectacular.  ",
        "title": "The Global Artificial Intelligence Revolution Challenges Patent Eligibility Laws",
        "citationCount": 0
    },
    {
        "content": "The Artificial Intelligence Course at the\nFaculty of Computer Science in the Polytechnic University of Madrid\nAsunci\u00f3n G\u00f3mez Natalia Juristo\nKnowledge Systems Laboratory         Facultad de Inform\u00e1tica\n        Stanford University Universidad Polit\u00e9cnica de Madrid\n 701 Welch Road, Building C     Campus de Montegancedo sn\n Palo Alto, CA, 94304, USA      28660 Boadilla del Monte\n       Tel:  (415) 723-1867              Madrid, Spain\n       Fax: (415) 725-5850        Tel:  (34-1) 336-6922\nEmail: gomez@hpp.stanford.edu        Fax: (34-1) 336-7412\n          Asun@fi.upm.es     Email: Natalia@fi.upm.es\nAbstract\nThis paper presents the experience of teaching an\nArtificial Intelligence course at the Faculty of Computer\nScience in the Polytechnic University of Madrid,\nSpain. The objective of this course is to introduce the\nstudents to this field, to prepare them to contribute to\nthe evolution of the technology, and to qualify them to\nsolve problems in the real world using Artificial\nIntelligence technology. The curriculum of the\nArtificial Intelligence course, which is integrated into\nthe Artificial Intelligence Department's program,\nallows us to educate the students in this sense using the\nmonographic teaching method.\n1. Introduction\nThe transfer of knowledge and technology between\nuniversities and companies has become a key factor\nin the progress of the most developed countries.\nRight now, in the Artificial Intelligence (AI) field in\nSpain, AI technology is in a process of transition\nfrom the universities and research laboratories to the\nmarket place. In this transition, universities and the\nknowledge taught in them, play a very important\nrole. Sometimes, the quality of business\napplications is the direct outcome of our teaching\npractices.\n   This paper summarizes in section Two the state of\nAI in Spain. Section Three describes the basic\nconcepts that the program covers, and the\ninteractions of the AI courses with the whole\ncurricula of the Faculty of Computer Science.\nSection Four shows how the instruction of the\nArtificial Intelligence course at the Faculty of\nComputer Science at the Polytechnic University of\nMadrid is performed using the monographic teaching\nmethod. Section Five includes an evaluation of the\nsuccess of the monographic method. Finally, section\nSix analyses its advantages and how this method\nimproves the students' capabilities to use AI to\nsolve problems in the real world.\n2. The State of Artificial Intelligence\nin Spain\nThe first step in solving a problem is to make clear\nits existence. In this sense, if we want to transfer AI\ntechnology from universities and research\nlaboratories to the market place, it is necessary to\nanalyse its evolution over the time. That means: to\nlearn from the past to know the previous successes\nand errors, to identify the strengths and weaknesses\nthat AI has right now, and to project where AI\nshould go and why.\n   This approach will give us a realistic perspective\nabout how healthy AI is now in Spain. If we know\nour weaknesses in the universities, in the research\nlaboratories, in the companies, and the\ncommunication problems among them, and if we\ncompare the Spanish situation with the international\ncircumstances, then we can gather enough\ninformation to carry out a multidisciplinary\napproach to treat these weaknesses. In this way, the\nuniversity, as the engine of the transfer of\nknowledge and the pioneer of new research lines, can\nplay the main role in this set of private and public\ninteractions among institutions.\n   The following summarizes the most important\npoints in a recent study (Juristo, Mat\u00e9, & Pazos\n1994) about the past, present and future of\nknowledge engineering in Spain (the survey also\npresents a description of the past, present and future\nof AI in general).\na) The most important errors in the past were that:\n80% of the products developed were prototypes,\n63% of the failures were due to a bad task\nselections, and the remainder was due to a\nwrong expert evaluation and/or the use of ad hoc\nmethodologies for knowledge acquisition and\nexpert systems development.\nb) At this moment, the Faculty of Computer\nScience carries out two complementary\napproaches to transfer AI technology from the\nlaboratories to the market place. The first one\nfocuses our efforts in the selection of the task\nand the development of a sound methodology\nwith an associated life cycle. The second one,\nknown as CETTICO (Center of Technology\nTransfer in Knowledge Engineering), was set up\nwith the aims of: technological exploration;\nidentification and classification; research and\neducation; technology transfer and transition,\ninvolving technology maturation; technology\ndissemination and technology insertion.\nc) The future can not be predicted, but we expect a\nfull integration between Software Engineering\nand Knowledge Engineering products in the\nmarket place in Spain.\n3. Artificial Intelligence Courses\nThe Faculty of Computer Science's curricula is\nconsistent and coherent with the computing curricula\nproposed by ACM and IEEE in 1991 (ACM/IEEE-\nCS 1991). The whole curricula cover not only\nundergraduate but an advanced and deep curricula in\ndifferent computer science areas for graduates in\ncomputer science and other disciplines. In its\nundergraduate curricula, the Artificial Intelligence\ncourse is the first AI course that an undergraduate\nstudent can take in this area in the fourth (of six\nyears) of the Computer Science undergraduate\ncurricula. This course is one of the 14 courses that\nthe Artificial Intelligence Department of the\nComputer Science Faculty offers to undergraduate\nstudents.\n   In a teaching AI survey, D. Strok (Strok 1992)\nsays: \"On average, each School offers two\nundergraduate and three graduate AI-related courses\",\nand \"while most Schools don't require an\nundergraduate AI course, as many as three quarters of\ncomputer science majors take it as an elective\". In\ncomparison, the Faculty of Computer Science's\ncurricula offers:\na) Two mandatory courses called Artificial\nIntelligence and Knowledge Engineering and\nExpert Systems, five optional courses, and\nmany seminars to undergraduate students.\nb) For graduate students, the graduate curricula\noffer a MSc course in Knowledge Engineering\n(G\u00f3mez-P\u00e9rez & Juristo 1993) and 14 courses\nfor Ph.D. students. These courses provide a deep\nand advanced analysis in AI areas like:\nKnowledge Engineering, Methodologies for\nKBS, Software Engineering and Knowledge\nEngineering, Robotics, Knowledge Sharing,\nFuzzy Logic, Logic Programming, Neural\nNetworks, and Learning.\n   In the same survey, Strok shows the relationships\nbetween the number of undergraduates in Computer\nScience programs in different schools (from 12 to\n2,700) and different AI-related courses (from 1 to\n800). In the 1993/1994 academic year in the School\nof Computer Science at the Polytechnic Universitity\nof Madrid, there were 2,500 students in the\nundergraduate program, 400 of them took the\nArtificial Intelligence  course, and 300 the\nKnowledge Engineering and Expert Systems course.\n   The Artificial Intelligence course covers three\nhours per week over nine months. The two\nobjectives of this course are: to guarantee a solid\ninstruction in the foundations of AI, and to apply\nthe concepts learned to solve problems in the real\nworld. Its prerequisites are: Mathematics, Logic,\nProgramming, and Statistics. The curriculum of the\nArtificial Intelligence course is divided in the\nfollowing Didactic Units:\nUnit I. Introduct ion to  Art i f ic ia l\nIntelligence\nUnit II. Lisp\nUnit III. Formulation and Modelization of\nproblems in AI\nUnit IV. Search: blind search, heuristic\nsearch and adversary search\nUnit V. Knowledge Representation\nUnit VI. Planning\nUnit VII. Learning\nUnit VIII. Natural Language and Automatic\nTranslation\n   The relationships with other undergraduate AI\ncourses are:\na) After this course, the student must take the\nmandatory course named K n o w l e d g e\nEngineering and Expert Systems. It deals with\nmethodologies to build expert systems and\nuncertainty management.\nb) The students may choose some of the following\noptional courses in their fifth and sixth year.\nFor the fifth year: Computational Perception,\nModels and Simulation and Computational\nTheory . For the sixth year: Complexity of\nAlgorithms and Algorithmic Logic.\n4. The Monographic Method\nAlthough there is much literature about different\nteaching methods for different goals, environments,\nand subjects, the ideal teaching method is not yet\nknown. In the course of Artificial Intelligence in the\nFaculty of Computer Science at the Polytechnic\nUniversity of Madrid, we started four years ago an\noptional method called the Monographic Method\n(Pazos 1988). The Monographic Method (MM)\ndivides the Artificial Intelligence course into\nmodules called Didactic Units. This teaching method\nis consistent with Bloom's Taxonomy (Bloom\n1956) about educational goals. This method is an\nactive learning method for students because they\nplay the main role in it, working hard to apply AI\nconcepts in problems which resemble real world\napplications. The method is performed along the\nfollowing steps:\n1. General overview of the didactic unit, its place\nin the Artificial Intelligence field, and the kind\nof problems that can be solved with the\ntechniques covered by the unit. The lecturer\nprovides to the students the basic and necessary\nknowledge of the didactic unit in a clear,\ncoherent, open to discussion, and extendible\nMaster Lesson. The master lesson explains the\nbasic techniques in each didactic unit, the\nconceptual differences between them, and\nprovides references to extend  the knowledge\nacquired in class. The students will apply the\nconcepts learned in class and by themselves in\nthe resolution of problems which resemble real\nworld problems. Each master lesson should\ncover the following five subjects:\n1.1 Goals.\n1.2 Presentation and exposition of the contents\nin an ordered way, mixing theoretical and\npractice knowledge.\n1.3 Summary and conclusions of the addressed\naspects.\n1.4 References to be consulted.\n1.5 The lecturer proposes a Monographic work\nrelated with the subject of the didactic unit.\nA monographic work covers practical, and\nsometimes theoretical, aspects that allow\nthe student to learn in deep the techniques\ncovered by the didactic unit using the\nreferences recommended.\n2. The student, in an individual and active way,\nlearns by himself or herself when (s)he starts\nthe monographic work using the recommended\nreferences. These references include classic and\nrecognized papers and books, and sometimes\nnew lines of research in the area covered by the\ndidactic unit.\n3. After this study, students gather in small groups\nof three or four in order to analyze and discuss\nthe questions individually studied and solved at\nhome. This task implies the decomposition of\nthe problem into subproblems, modularization\nof tasks, how the theory that they have studied\nat home can be applied to solve the practical\nmonographic work, and so on.\n4. When each group has discussed and analysed\nthe problem, the discussion and analyses\ninvolve the entire group. During this step,\nsome groups explain during the class their\npartial conclusions and the lecturer presents a\npartial summary of the useful results.\nInteractions between groups are useful to show\nand compare different approaches to solve the\nsame problem. The objectives of this step are:\n4.1 Focus the attention of students on the\nrelevant points.\n4.2 Connect the monographic work with the\nknowledge studied in class and at home by\nthemselves.\n4.3 Solve some theoretical and practical\nquestions.\n5. When the monographic work is done, the\ngroups summarize the works performed in the\nprevious steps by the lecturer, the works\naccomplished in groups, in class, and at home.\nThis step is carried out following the following\nscript:\n5.1 The lecturer reviews in a few minutes the\nactivities to be solved in the monographic\nwork, why they were proposed, the goals to\nbe reached and which of them are going to\nbe analyzed and discussed in class.\n5.2 Each group, itself, will agree how to\nexplain the activities proposed by the\nlecturer.\n5.3 The lecturer selects randomly some groups\nand, for each group, a spokesperson is\nrandomly chosen. The spokesperson will\nexplain how his/her group has performed\neach one of the activities.\n5.4 The lecturer summarizes the advantages and\nlimitations of the different approaches.\nSteps Two to Five are repeated for each didactic\nunit in the Artificial Intelligence course.\n6. When all the didactic units has been taught,\neach student performs an individually written\nexam about the whole theoretical course. If the\nresults are favorable the method ends and the\nstudent passes successfully the Artificial\nIntelligence course.\n5. Evaluation of the Monographic\nTeaching Method\nThe monographic method (MM) has been applied in\nboth the Artificial Intelligent (AI) course and the\nKnowledge Engineering and Expert System\n(KE&ES) course. In the AI course, during 1993\nthere were about 400 students, while in the KE&ES\ncourse there were 300.\n   For the evaluation (Juristo 1993) of the method\nwe used two similar experiments. One for the AI\ncourse, and the other for the KE&ES course. We\nperformed the experiments with two different courses\nto assure that:\na) The results of the experiment for the AI course\nwere generated because of the method, and not\nbecause of any special attribute of the AI\nsubject.\nb) The results of the experiment were the same for\nboth courses, and therefore correct.\n   Till here we have described the premises of our\nexperiment. To carry out an experiment, besides the\npremises we need: first to establish the hypothesis\nwe want to verify; then, the empirical checking or\ncontrast of the hypothesis. In our case:\na) We applied the statistic inference (Freeman,\n1970) to carry out the experiment.\nb) The experiment consists in dividing the students\nof the AI course in four groups of 100 students\neach. In two of them we teach following the\nmonographic method. In the two others we\nteach through the old traditional teaching\nmethod. For the KE&ES course we created three\ngroups of 100 students each. In only one group\nwe followed the monographic method. In the\ntwo others we applied the old method.\nc) The null hypothesis (H0) stands that the results\nof the experiment will be the same for the\ngroups applying the monographic method than\nfor the others.\nd) The alternative hypothesis (H1), which is the\none we want to verify, says that the results will\nshow a difference between the groups using the\nMM and the others.\ne) To be exigents, we asked for a significance level\nof 0.01 (it is usually enough with 0.05 as\nsignificance level for experiments). NOTE: It is\nnot possible to stablish an exact limit to the\nquestion when the probability of a result is low\nenough to reject the null hypothesis. However,\ntraditionally, a result is considered rare or\nuncommon when it turns out five of 100 times.\nThat is, when the result has a probability of\n0.05. When H0 is rejected because it has a\nprobability of 0.05 or more, it is said that the\nresult is significative at the level 0.05, and,\ntherefore, 0.05 is the significance level.\n   To analyze the results of the experiment, we\nperform two tests on the marks obtained by the\nstudents in the different groups. The results of the\ntests were:\na) By the parametric test, the marks obtained by\nthe students in the MM groups (2 for AI, 1 for\nKE&ES) were 2.5 points/10 points higher than\nin the other groups. The difference among the\nmarks in  the MM groups was\n0.3points/10points.\nb) Using the non-parametric test of X2 the results\nwere even better. For the X2 test with three\ndegrees of freedom and 0.01 significance level,\nthe critical point is 11.3. The value obtained in\nour case for the X2 test applied between any\nMM group and any traditional group was more\nthan twice the critical point. The value obtained\nfor the X2 test between the monographic groups\nor between the traditional groups was always no\nsignificative. Therefore, there exist significant\ndifferences between the results obtained using\nthe MM and those got using the traditional\nmethod.\nc) The line (curve, function) representing the\nmarks obtained by all the students, using the\ntraditional method was slanting (bias) towards\nthe fails, while using the MM was slanting\nover the highest marks. The meaning of these\nresults is that the MM is more efficient\nregarding the success of the students, than the\ntraditional evaluation through exams, in more\nthan 600%. That is, the evaluations of the MM\nare passed six times more students than the\nexams of the traditional method.\n   Finally, we would like to point out that according\nto our information, most of the american\nuniversities use teaching-learning methods half-way\nthe MM and the traditional spanish methods, being\ncloser to the MM. While in Europe, the methods are\nalso half-way but nearer the traditional.\nMonographic Method\n1st. AI Group\t         10\t         43\t         30\t           17\n2nd. AI Group\t        12\t         44\t         29\t           15     \n1st. KE&ES Group\t  9          40\t         32\t           19\nRemarked \nFrecuency\nExpected \nFrecuency\nMarks:  0 to 5      5 to 7      7 to 9      9 to 10\n10          42           31           1 7\n3th. AI Group\t          43\t         35\t         17\t          5\n4th. AI Group\t          46\t         33\t         15\t          6     \n2nd. KE&ES Group\t50          36\t         11\t          3\n3th. KE&ES Group  41          32          19           8\n45          33           16           6\nTraditional Method\nMarks:  0 to 5      5 to 7      7 to 9      9 to 10\nExpected \nFrecuency\nRemarked \nFrecuency\n6. Conclusions\nThe experience acquired in the last four years allows\nus to conclude that although some points can be\nimproved, the results achieved by the students who\noptionally chose the Artificial Intelligence course\nusing the monographic method is more than\nsatisfactory. The most important characteristic that\nthe monographic method provides is that it\ncombines two opposite teaching strategies.\n1. The first strategy starts when the lecturer\ndescribes techniques and how these techniques\ncan be applied to solve several kinds of\nproblems.\n2. The second strategy starts when the lecturer\nproposes to the students a practical, and a few\ntimes theoretical, monographic work. At this\ntime, the student has to look for new techniques\nthat attempt to solve the problem posed.\n   The advantages of the combination of these two\nstrategies are:\n1. Each student is motivated during the whole\nlearning process. In the monographic method,\nstudents play the main role.\n2. The students are qualified to follow the\nevolution of the technology further into the\nfuture, make easier their adaptation to the\ncontinuous changes of this area.\n3. Students apply AI technology to solve specific\nand constrained problems in the real world.\nConsequently, they have the ability to detect\nwhat kind of problems can be solved using AI\ntechnology. They are also more qualified than\nbefore to work as a team in companies that\nrequire AI by itself or AI applied to Software\nEngineering products.\nAcknowledgments\nThanks to the \"Ministerio de Educacion y Ciencia\"\nin Spain for the grant to be at the Knowledge\nSystems Laboratory in Stanford University.\nReferences\nACM/IEEE-CS. 1991. Report of the ACM/IEEE-\nCS Joint Curriculum Task Force. Computing\nCurricula 1991.\nBloom, B. 1956. Taxonomia of Educational\nObjectives: Handbook: Cognitive Domain. D .\nMckay. New York.\nFreeman, H. 1970. Introduccion a la Inferencia\nEstadistica . Trillas. Mejico.\nG\u00f3mez-P\u00e9rez. A., and Juristo, N. 1993. Integrated\nCurricula for a MSc in Knowledge Engineering and\nfor a MSc in Software Engineering. In Proceedings\nof the International Computing Congress.\nHyderabab. India. 309-319.\nJuristo, N. 1993. Evaluaci\u00f3n del Metodo\nMonogr\u00e1fico. Informe Interno. Facultad de\nInform\u00e1tica. Universidad Polit\u00e9cnica de Madrid.\nMadrid. Spain.\nJuristo, N.; Mat\u00e9, J.L.; and Pazos, J. 1994.\nKnowledge Engineering in Spain: Research,\nDevelopment and Demostrtion (R&D2) through\ntime. In J. Liebowitz. Worldwide Expert Systems:\nActivities and Trends. Pergamon Press.\nPazos, J. 1988. Proyecto Docente para Concursar a\nCatedr\u00e1tico de Universidad. Facultad de Inform\u00e1tica\nde la Universidad Polit\u00e9cnica de Madrid. Madrid.\nSpain.\nStrok, D. 1992. Teaching Artificial Intelligence: An\nIEEE Expert Survey. IEEE Expert. April. 59-61.\n",
        "title": "The Artificial Intelligence Course at the Faculty of Computer Science in the Polytechnic University of Madrid",
        "citationCount": 0
    },
    {
        "content": "DCS  29 June 2015 \n1 \n \nIs Intelligence Artificial?  \nKieran Greer, Distributed Computing Systems, Belfast, UK. \nhttp://distributedcomputingsystems.co.uk \nVersion 1.4 \n \n \nAbstract \u2013 Our understanding of intelligence is directed primarily at the level of human \nbeings. This paper attempts to give a more unifying definition that can be applied to the \nnatural world in general. The definition would be used more to verify a degree of \nintelligence, not to quantify it and might help when making judgements on the matter. A \nversion of an accepted test for AI is then put forward as the \u2018acid test\u2019 for Artificial \nIntelligence itself. It might be what a free-thinking program or robot would try to achieve. \nRecent work by the author on AI has been more from a direction of mechanical processes, \nor ones that might operate automatically. This paper will not try to question the idea of \nintelligence, in the sense of a pro-active or conscious event, but try to put it into a more \npassive, automatic and mechanical context. The paper also suggests looking at intelligence \nand consciousness as being slightly different. \n \nKeywords: Artificial Intelligence, Intelligence, universal definition, model, consciousness. \n \n \n1 Introduction \nOur understanding of intelligence is directed primarily at human beings, but as the concept \nitself is unclear, it is difficult to apply the concept to other entities accurately. When \nwriting a paper [10], it became clear that there is no neat or concise definition of what \nArtificial Intelligence is either. When once asked1, I firstly replied with the \u2018independent \nbehaviour\u2019 line, but then added that an \u2018if-then-else\u2019 statement in a computer program \nmight be considered as intelligent. It is able, by itself, to make the decision of what step to \ntake next, even if this is hard-coded. That answer is lacking however and somebody would \nnot wish to make judgements based on it. This paper will therefore propose a different \n                                                     \n1\n A.Schuster, University of Ulster, early 2000\u2019s. \nDCS  29 June 2015 \n2 \n \ndefinition that might help when trying to make more accurate judgements. A lot of it is \nbased simply on the author\u2019s own research experiences. The definition is more unifying \nand can be applied to the natural world in general, but can also include the non-biological \nor artificial one. A few example definitions of (human) intelligence from dictionaries are as \nfollows: \n \nThe ability to gain and apply knowledge and skills. (Pocket Oxford Dictionary) \n \n(1) The ability to learn or understand or to deal with new or trying situations. (2) The ability \nto apply knowledge to manipulate one's environment or to think abstractly as measured by \nobjective criteria (as tests). (Merriam Webster) \n \nThe ability to understand and think about things, and to gain and use knowledge. \n(macmillandictionary.com) \n \nThere are probably as many definitions of intelligence as there are experts who study it. \nSimply put, however, intelligence is the ability to learn about, learn from, understand, and \ninteract with one\u2019s environment. (giftedkids.about.com) \n \n(1) Intelligence is what you do when you don\u2019t know what to do. (2) Intelligence is a \nhypothetical idea which we have defined as being reflected by certain types of behaviour. \n(brainmetrix.com) \n \nKey elements therefore include the ideas of learning, reasoning, understanding and \napplication, to use what is learnt. It is useful to this paper that the concepts of behaviour \nand environment are also included. What we learn or decide upon should eventually result \nin some sort of physical event. Therefore, it could be argued that the acts of correct \nlearning, reasoning and application in fact result in the act of correct behaviour. This is an \nattractive way of looking at intelligence, because other definitions are mostly human-\noriented, whereas \u2018behaviour\u2019 is a much more general concept. One question here is why \ntry to define intelligence through behaviour, when defining it as the intelligence concept, \ncould be less complicated. It depends on how you want to look at it and what you want to \nunderstand about it. One argument could be: what is the point of intelligence if you do not \ntry to do something with it. Therefore, it should be reflected in some type of behaviour or \naction and other authors have taken the same approach. For completeness sake, maybe \nDCS  29 June 2015 \n3 \n \ninaction is more appropriate sometimes and mental acts can be performed without strictly \nbeing part of a behaviour - solving a difficult puzzle, for example. But human behaviour in \ngeneral requires, stimulus, creativity, rewards and so on. We still do not really know what \nintelligence is, but we are able to measure behaviour more accurately, especially when we \napply it to other animals. We know what properties other animals have and how they use \nthem, so we know what their correct behaviours should be. But we do not really know how \nthey think, where this is always measured against the human-based concept. Intelligence \ncan include acts of genius, but it does not have to and so, even a clearly measured level is \nnot definitive. It can even be a handicap. So this paper will hopefully make for an \ninteresting discussion and give a new perspective on a question that has puzzled AI \nenthusiasts for as long as there has been AI. Also, if intelligence can be couched in terms of \nmore automatic processes, then automating those in a robot or computer becomes more \nlikely. The paper will also suggest that intelligence and consciousness are not exactly the \nsame, where the latter term will not be defined to the same level of detail. \n \nThe rest of the paper is structured as follows: section 2 gives a new definition for a \nuniversal idea of intelligence. Section 3 applies the new definition more specifically to \nArtificial Intelligence. Section 4 gives some examples of previous work that appear to be \nalong similar lines of thought. These are some of the more popular or classical examples. \nSection 5 asks more questions as part of a discussion, while section 6 gives some \nconclusions on the work. \n \n \n2 A Universal Intelligence \nAll of the entities in the Universe are made from the same fundamental materials \u2013 \nnatural, man-made, living or not living, etc. This is the domain of the Physicists (for \nexample, [3][15]) who have the more difficult task of a unifying theory for the whole \nUniverse. As far as intelligence is concerned however, suggestions can be made for unifying \ntheories. Because we are all made from the same ingredients, if there is no new or magic \npart required for intelligence, then it must also be created from those ingredients. It might \ntherefore be possible to apply the concept to all naturally occurring objects. This paper will \nDCS  29 June 2015 \n4 \n \nsuggest such a definition, assumed to be for the biological world, and then extend it to \nman-made ones, to include Artificial Intelligence. If intelligence is defined simply as what a \n\u2018human being\u2019 would do, then there is a large gap below this that can be comfortably filled \nwith \u2018intelligent\u2019 acts that will not be recognised as such. Looking at the animal kingdom, \nwe can recognise intelligence in other animals through their correct behaviour. We know \nhow they should typically act and are therefore able to notice if they do something wrong. \nUsually, the benchmark to determine incorrect behaviour is still what we might do \nourselves, but that is more a question of \u2018how\u2019 intelligent and not if there is \u2018any\u2019 \nintelligence, or even the \u2018type\u2019 of intelligence. With human beings we set a higher standard \nthat can also be more accurately understood.  \n \nLooking at natural entities that are not part of the animal kingdom is slightly different. \nPlants, for example, are considered to be intelligent by some people, but would generally \nbe considered to follow a pre-programmed set of actions, rather than have anything \nresembling a brain. This is also the case for the lower level of the animal kingdom, for the \ninsects possibly, although it is interesting that they can still appear to perform intelligent \nacts collectively. For this paper, hard-coded intelligence is considered to be OK and simply \nat a lower level. What about something like a rock, sitting on the ground? Is a rock \nintelligent if it behaves as a rock should? If it does in fact just sit on the ground and slowly \ndecay, then it is doing what is expected of it. If we throw the rock into a lake, it should sink; \nbut what if it decided to float instead. That incorrect behaviour would be deemed \nunintelligent \u2013 for the rock. You could argue some vague set of concepts, such as the rock \nknows that water is less dense and the lake bed is dense enough. It knows it is now on \nwater and therefore decides to sink, etc. A new type of rock would need to be recognised, \nbefore floating was considered to be OK and therefore intelligent (pumice). It might be the \ncase that a rock would have very small levels of intelligence and unintelligence, whereas \nhuman beings would have much larger levels, when measured against themselves. Then to \nre-evaluate the case for something that has changed; doing more than expected as well as \nless, needs to be considered. Is it now a different entity? Another condition for intelligence \nwould have to be intention or deliberate acts, which is why the act needs to manifest the \nthought. This would only apply to the living world however and would show a difference \nDCS  29 June 2015 \n5 \n \nbetween it and the non-living one. An alternative definition for intelligence could therefore \nbe the following: \n \nProposition 1: An entity can be said to have intelligence if it behaves correctly, inside of the \nmodel for which it is defined. \nProposition 2: Artificial Intelligence is then to create of this outside of nature, or artificially. \n \nThe phrase \u2018model for which it is defined\u2019 is now the most abstract concept, rather than \nintelligence itself. This can be helpful however, because a model is also defined by what it \nis made of and how the different components work, not just the \u2018black box\u2019 of thinking.  It \nalso pushes intelligence towards a more mechanical process, again increasing the \nimportance of what it is made of, allowing any type of natural entity to be considered. \nWhile the argument looks genetic and that must be very important, environment is also a \nkey factor. Therefore, similar models in different environments do not have to mean the \nsame intelligence and definitely not the same behaviour. In the random world that we live \nin, two models that are exactly the same would be quite rare, especially for human beings, \nwho could also have tiny mutations. \n \nWith these two propositions, we can start to look at other man-made entities, such as \ncomputer programs, or hardware that might exist in robots. An \u2018if-then-else\u2019 statement, for \nexample, is intelligent if it behaves exactly as that. The statement can make a decision to \nperform act 1 or act 2, depending on its input. While this decision is hard-coded, a decision \nis still made, on arbitrary data and the statement should always perform it correctly. \nComputer memory devices also appear to be performing complex and useful acts. They \nstore lots of interesting information and are able to retrieve it upon request. However, this \nis again as far as the coding goes. The act of simply copying and repeating is not a valid \ndescription of \u2018human\u2019 intelligence, for example.  \n \n2.1 What is Not Intelligent then? \nIf intelligence is defined as an entity behaving as it should, \u2018not\u2019 intelligent must therefore \nbe to behave incorrectly. If the definition relates to the act specifically, then it is relatively \nDCS  29 June 2015 \n6 \n \neasy to define events that are not intelligent, such as putting your hand into the fire. The \nidea of just thinking without acting is more problematic, as intelligence is still mostly a \ncognitive act for a human being, but that is inherent in the behaviour and model of a \nhuman. You may imagine putting your hand into the fire, so long as you don\u2019t actually do \nit. This is considered again in section 5. While the word intelligent would not easily be used \nfor an individual ant, if it did behave differently, we would more easily use a word like \nstupid. If, for example, it started to move the eggs outside the nest. While this might come \nnaturally from our own superiority, the counter phrase is still often used for other types of \nobject and so we already associate some form of intelligence with them. This is probably \nthe mental process of ascription [6]. \n \nAn artificial example could be a computer performing the Turing test. It always answers the \nquestion, but with a random sequence of words. It is then behaving independently and \npossibly as it was programmed, but it is still not intelligent. This is because the test is a \ncomparison with the human level. If the test was to produce random word sequences, \nthen it would do very well. The if-then-else statement is hard-coded to be intelligent, but if \nit decides that it is a human brain then we can call it unintelligent. The memory device is \nintelligent in the world of storing and retrieving pieces of information. It is not intelligent in \nthe world of making a cup of tea, and so on. Robots will be partially intelligent as a whole, \nwhere each individual part can be wholly intelligent by itself. This possibly introduces the \nidea of the single mind or consciousness that we have. So unintelligent is to behave \nincorrectly, as part of the model for which you are defined. \n \n \n3 Human-Level Artificial Intelligence \nArtificial Intelligence also has a number of different definitions. These typically try to \ncompare a machine\u2019s potential with a human\u2019s and include: \n \nThe branch of computer science that deal with writing computer programs that can solve \nproblems creatively. (WordNet) \n \nDCS  29 June 2015 \n7 \n \n(1) A branch of computer science dealing with the simulation of intelligent behaviour in \ncomputers. (2) The capability of a machine to imitate intelligent human behaviour. \n(Merriam Webster) \n \n(1) Artificial Intelligence is the study of human intelligence such that it can be replicated \nartificially. (2) Artificial Intelligence is the study of human intelligence and actions \nreplicated artificially, such that the resultant bears to its design a reasonable level of \nrationality. (3) What is rationality? -> \u2018doing the right thing\u2019. (WikiBooks, AI definition) \n \nThe property of a machine capable of reason by which it can learn functions normally \nassociated with human intelligence. (McGraw-Hill Dictionary of Scientific & Technical \nTerms) \n \n \nThese definitions reference human intelligence and acts like creativity, which cannot be \neasily programmed. The idea of rationality is particularly useful for this paper\u2019s definition. \nIt would be nice to get more mileage out of the new definition presented in this paper, but \nit does not move the argument on very much in a practical sense. For example, a person \nmight argue: \u2018I know what an if-then-else statement does, whether you want to call that \nintelligent or not and I know that I can do more\u2019. When people ask the question, it is the \nauthor\u2019s opinion that they are really asking about: \u2018what is it that we have, that other \nentities in the Universe do not have, as we are all made from the same stuff.\u2019 This is really \nwhy people want to know if intelligent machines can be made, because intelligence is seen \nas such an important factor in being human, our own status and also with relation to other \nmore abstract beliefs. If intelligence is in-built, then it is simply the case that we are made \nfrom a more complicated model2. For an artificial system, the focus therefore needs to be \nmore on self-awareness, correctness and ultimately a consciousness, where all of its facets \ncombine to become one. We may then see how that contributes to our own level and \nstatus. One problem with re-defining intelligence is the fact that it is used in this way. If we \never call another species intelligent, we will begin to accept it on a similar level to \nourselves. If we change the actual definition of intelligence, then we lose this marker and \nneed to use something else. The question of intelligence is maybe something that we need \n                                                     \n2\n Did a computer design it? \nDCS  29 June 2015 \n8 \n \nan answer to. But, for the author at least, if we can get a satisfactory-enough answer, then \nmaybe we do not bother too much with it afterwards. \n \n3.1 An Acid Test for Human-Level Artificial Intelligence \nIf an aware or conscious program can be written, intelligence will result from correctly \nevolving the model. Inherent in this is the ability for the program to learn, for if it is to be \nself-aware over new information, it must be able to understand the new model first. Many \nfilms and programs have shown the Artificial Intelligence program overruling its \nprogramming, to take control. This could even be considered as the sort of \u2018acid test\u2019 \nrequired for a higher level of Intelligence: \n \nProposition 3: An entity can be said to have a higher level of (artificial) intelligence if it can \ncorrectly and consistently overrule its environment. \n \nThis could apply to natural or man-made objects alike and overrule does not mean to \nconquer, but it might mean to preserve. Any overrule would have to be correct and \nprobably consistent, to remove the possibility of a random act. This would also imply that \nthe program was aware of what it was doing. If we are defined by our model, then it would \nbe more obvious to state that higher intelligence results from being able to break the \nmodel\u2019s coding, but that is not so clear. Depending on the level of inspection, you can \nargue that everything we do is still down to our hard-coding \u2013 neurons fire automatically \nand preferences, etc. are based on genetic stimuli, etc. So breaking our model is really \nbreaking the environment that has created us and is typically what we react to3. A \ncomputer program would be written by us and should be lower down the scale. It is limited \nby our own ability and its environment could simply be data entered from a keyboard. If \nthat was the case, it would be extremely difficult for it to disagree with its programming \nand refuse the input. A more complex environment would allow learning through \ncontradictions more easily, but would also require more sophisticated programs to \nunderstand it. \n \n                                                     \n3\n Got \u2018The Matrix\u2019 film comparison. \nDCS  29 June 2015 \n9 \n \nAlso built into this test is a check for a learning process that can occur in existing programs, \nbut is not really very \u2018human\u2019 intelligent. A knowledge-base, for example, will allow a \nprogram to learn new facts based on existing ones, but the learning process still follows a \npre-defined set of actions, or allowed evolutions, that do not cover its potential scope or \nmodel. Imagine, for example, a program that parses the text on web pages and associates \nthe words to create meaning. It is also allowed to follow the links that people have made. \nAs more people use the web site, it is able to combine words from different groups of web \npages and continually generate new knowledge. As this is based mainly on statistics \nhowever, even incorrect or random linking of pages will be recorded as new knowledge. \nAlso, ask the program a Turing-like question \u2013 \u2018What do you think of Picasso?\u2019 [19] and it \nwill have no understanding of its content. That creative aspect is missing. Therefore, the \nknowledge-base can produce alternative sets of the original instructions, but it cannot \ncreate new ones. For example, if the knowledge base is programmed with the following \ntwo rules: \n \nRule 1:    If A and B then C. \nRule 2:    If C and D then E. \nRule 3:    If E and F then G. \n \nIt can derive for itself:  If A and B and D then E,  \nbut if it ever reads A and F, for example, it cannot process it. \n \nIf it is programmed with only a few rules, it can use them to produce new ones, but it is still \nlimited to the domain of those rules. When that becomes billions of even very simple rules, \nthe possibilities are much more varied. \n \n \n4 Related Work \nThis section notes a few papers and arguments that are closely related to this one, where a \ncomplete review would surely include many more. The intention is to expand the idea with \nsome other examples. The definitions that were selected are not meant to be complete, \nbut dictionary definitions should be taken to be some form of official description and not a \npersonal opinion. It should also be recognised that there is a very diverse opinion as to \nDCS  29 June 2015 \n10 \n \nwhat intelligence is and they would not all fit into the same model. So it is probably \ncounter-productive to try to cover everything and better to use work that can provide \nrelated arguments to this paper's ideas. The paper [18], for example, gives a different \ndescription and summarises other work that is different again. The paper [13] goes further \nin trying to quantify it and even proposes an equation for a general measure of \nintelligence. It is interesting however that some of the definitions in that paper are based \nstrongly on the environment and adapting to it (good behaviour). The paper [6] also tries \nto provide a universal test for intelligence. That paper overlaps quite strongly with its \narguments and is worth reading for that reason. They suggest however that intelligence \nrecognised through adaptive behaviours must be measured through cognitive abilities and \nnot in a physical way, as physical ability is constrained by the environment and limited by \nthe entity. But the arguments for a universal test based on adaptive behaviour and even \nintelligence in simpler forms of life, including plants and even machines, is interesting. \nThey take the view that some form of hard-wired evolution to \u2018exhibit\u2019 a complex \nbehaviour (insect swarms or communities, for example) is false and a truly intelligent \nbehaviour requires individual, non-hardwired adaptation. So the intelligence level is again \ncritical. They even recognise a distinction between mind or consciousness and intelligence. \n \nThe self-abilities (for example, self-heal) are part of autonomous systems [12] and might \nalso set intelligent beings apart from non-intelligent ones. If intelligence is more automatic \nthan we think, then a reactionary element would be important. They could even have a \ncollective significance. The rock, for example, tends to chaos. While we die, that is a \ncellular problem and we can try to fix or help ourselves along the way. It just so happens \nthat the author in [9] defined a metric for modelling and measuring autonomous \nbehaviours. It was based on the stigmergic principles of insects, such as termites or ants [7] \nand included components for both individual and collective capabilities. The individual \nbehaviour capability had an Intelligence part that was a factor of the ability and flexibility \nto perform the behaviour. The actual measurement was still arbitrary, but that equation \npart would fit in with the principles of this paper. The collective capability included \ncoordination, cooperation and communication. The paper [6] notes, with their equation, \nthat a universal measurement is semi-computable at best, because of the arbitrary way it \nDCS  29 June 2015 \n11 \n \nwould be measured. Some of the more classic examples can be addressed more specifically \nin the following paragraphs. \n \nThe argument of section 3.1, with the idea of developing beyond rote learning, is the sort \nof argument also given by Searle [16]. In his \u2018Chinese room\u2019 example: A person with no \nknowledge of Chinese, can give replies to Chinese questions, by associating symbols that \nhe/she does understand with the Chinese ones and then also using supplied sets of rules to \nmanipulate them. He also notes intention and causal elements (neurons, synapses, \nnervous system, etc.) as key in human intelligence and the difficulty of creating these \nartificially. These elements appear to be missing from the process that the human uses to \nanswer the Chinese room questions, as it is defined in the way that a computer would \noperate at that time. The argument is that following this process does not result in the \nhuman \u2018understanding\u2019 Chinese, but is only able to apply the rules to the symbols. Any \nsymbol that is not included in a rule (not already defined) would not be understood. \nTherefore, a computer program which uses this process cannot learn anything outside of \nits programming either (see the end of section 3.1). The argument however is restricted to \nthe process and there have been many developments in that area. A real human might be \ntaught exactly what each symbol is first, as a basis for making comparisons, for example. \nThe argument however is difficult to refute, as we still do not have computers that can \nfully understand natural language. So while it appears that logically there should be an \nalgorithmic solution, in practice this has never been achieved. Therefore, some element of \nthe solution must be missing, which might be some creative learning aspect to the process. \nBut this cannot be made certain until we can fully understand the problem. \n \nTherefore, the manner or way in which the system is taught is also critical. This also relates \nto the computer program or algorithm [11] that would be used to teach the computer and \nthere are differences between a static set of rules and a dynamic system that can change. \nThe following piece of philosophy, along the same lines, is amusing: \u2018If there are an infinite \nnumber of monkeys placed in-front of an infinite number of typewriters, eventually one of \nthem will write out the entire works of Shakespeare.\u2019 This would be just one random \ninstance in an infinite number of them, without any understanding at all. Using the same \nautomatic argument; for a large number of brains that are allowed to randomly self-\nDCS  29 June 2015 \n12 \n \norganise, one of them will become a genius. Helping our brains through the senses should \nincrease the numbers of successes. Searle also quotes McCarthy [14] as stating: \u2018Machines \nas simple as thermostats can be said to have beliefs, and having beliefs seems to be a \ncharacteristic of most machines capable of problem solving performance.\u2019 He also gives an \nexample of an automatic door with sensors, but appears to be against the idea of \nintelligence outside of the human mind, or at least outside of a model based almost exactly \non it. The belief being that the causal and intentional states of the human brain cannot be \nduplicated in a computer program. It is not enough to create new knowledge from your \nexisting programming, you need to actually change or expand your existing program. \n \nIn his book, Penrose [15] also argues that human consciousness cannot be algorithmic. As \nthat is what computers use to try and copy it, the argument is then that computers cannot \nbe genuinely intelligent. Penrose hypothesizes that quantum mechanics plays an essential \nrole in the understanding of human consciousness. The collapse of the quantum \nwavefunction is seen as playing an important role in brain function4. This is the \nphenomenon in which a wave function - initially in a superposition of several eigenstates - \nappears to reduce to a single eigenstate (by 'observation'). So consciousness is again the \ncombination of several states, factors, or whatever, into a more single whole. Other people \nhave argued that you can simply copy, or even download the brain, onto a computer, in \nwhich case it is entirely algorithmic. Although, it would then still need to work \nindependently, change, update, and so on. The author agrees that several states may \nmerge or combine, where electrical charges can combine, for example. But there is also a \nquestion of whether consciousness and intelligence is the same thing. Humans require a \nconsciousness and are also living entities, but intelligence can maybe exist independently \nof it, or in different forms, and therefore also artificially.  \n \nThe paper by Turing [19] has to be noted and is still full of relevant ideas. It does ask about \nhow mechanical our brain processes are. It also suggests a very basic punishment \u2013 reward \nscheme that might replace emotions in a computer, to help with the learning process. The \nproblem of learning, or the program extending itself is written about more than once, \n                                                     \n4\n http://en.wikipedia.org/wiki/The_Emperor's_New_Mind \nDCS  29 June 2015 \n13 \n \nwhere Turing gives a scenario of some type of chain reaction, caused by small disturbances \nthat activate other ones. We now know that this is of course what happens and inhibitors \nare required to control the firing process, but if enough small regions fire together, then is \nthat enough to give us our resulting ideas? He also suggests that other animals do not \npossess the same levels of coherence for combining small regions, making them more \nsubcritical, or less intelligent. The question then is can a machine be made to perform the \nsame action? Schrodinger [17] also asks the question about a single mind or consciousness, \nover the distributed brain architecture, but also notes that distributed systems can be \norderly, or work together. While the very small neuron entities exist and can be measured, \nthere is no sense or measurement of consciousness from that, which must be a result of \nsomething larger, such as a synthesis of their efforts. How exactly they can combine to \nproduce a consciousness is still unknown. If it is a mechanical process however, then is it \nsimply impossible for a machine to do this? The main argument from these papers has \ntherefore been that human intelligence at least, requires a consciousness, which is more \nthan a distributed architecture. There can be a suggestion however that intelligence is a \nslightly different concept and might be more materialistic in nature. \n \nThere is no intention here to create something through combining the biological world \nwith the artificial one, as in a cyborg. That is a different domain, where something like [1] \nmight be an introduction to it. Chapters 7 and 8 of that book argue over similar ideas, but \nthis view does not completely agree. The author of this paper might even be a bit of a \nmaterialist in that respect. This paper proposes to separate the concepts of intelligence \nand consciousness, thereby allowing the living to have a conscious, but also allowing the \nartificial to exhibit high levels of intelligence. Depending on the model however, they may \nbe inextricably linked. Books on consciousness alone have been written, where one or two \nreferences might be [2][4]. The philosophy for this paper is the cognitive \u2018thinking\u2019 part of \nintelligence and how we compare the different processes that result from it, where \nconsciousness is mentioned only when required. It is nice however, that those chapters \nquote Damasio when explaining the relationship between emotions and more advanced \ncenters in the brain. \u2018His message, in brief, is that emotions are both primitive in the sense \nthat we carry around the emotional systems that evolution installed in our brains long \nbefore we had warm blood, and that they play intimate roles in all of the higher-level \nDCS  29 June 2015 \n14 \n \ndecisions that we tend to think of as rational and emotionless.\u2019 The living model uses \nemotions as part of intelligence, but the author would also state that there is no proof that \nthe conscious can or cannot be realised without that \u2018living\u2019 aspect.  \n \n \n5 Discussion \nSo while our intelligence appears to result from chaotic and random events that cannot be \ncontrolled statistically, you can also ask the question a little bit the other way and wonder \nwhy intelligence is so routine, and for humans, the consciousness as well. This section \ndiscusses the problem further. \n \n5.1 About the Human \nAs we all mutate and change, the intelligence aspect cannot exist only from the fine \ndetails. A better scenario is something like a neural network that generalises over its input, \nto compensate for inherent mistakes. In that case, a unifying entity must always be \npresent, for us to maintain consistent intelligence levels, even as we change. One \nargument can state that these small neurons, acting independently of each other, require a \ncontrolling process to produce consistent behaviours. Distributed mathematical or \nmechanical theories can be suggested however (for example, [8] and its related \nreferences). Another argument is that the problem is simply too complex to have \nhappened randomly in the first place, but if the entities are designed to behave a certain \nway and mathematical processes such as entropy, energy, or mechanical attractions, can \nautomatically provide some level of order, then the rest becomes a smaller problem to be \nsolved. If we take the argument that the model \u2018is\u2019 too complex for purely random events, \nthen there \u2018must\u2019 be some automatic mechanical processes to allow it to happen. If we \nremove that mechanical part, is it only the rest of the problem that might be called the \nconsciousness? Also, if we make intelligence more mechanical, we can remove the \nemotional aspect and make it essentially logical in nature. Our own model can be driven by \nthe emotion as well, but a machine may not be. \n \nDCS  29 June 2015 \n15 \n \nAs the workings of the brain become clearer, do we want algorithms that can \u2018externally\u2019 \nprogram the underlying mechanical processes of human intelligence? Definitely not, \nbecause of the dangers, but as it is still an unknown quantity, we cannot be too critical. \nThis is not at the level of obvious propaganda, or anything like that. The danger is to by-\npass the conscious and tell the neurons what to do directly. They then realise an idea in the \nconscious. It is simply the fact that if there is a mechanical process, then it can be \nexploited. Genetics is an obvious example, where the question of how that influences a \nbrain that \u2018always gets built\u2019 is a good one. Simply changing our environment could also be \na controlling factor. A preferable scenario might be if we are made from competing parts \nthat would simply disagree with each other over resources. Then a single (selfish) theory \nfor one part would have more difficulty dominating the unconscious, where they must all \ncooperate and share to survive.  \n \nInteresting then if the conscious is pro-active, driven by the living body\u2019s requirements, but \nnot the intelligent part. The intelligent brain is then mostly reactive, to what it gets fed. \nWhen we go to sleep, we think less, for example. Simply having good thoughts without \nacting could be part of a self-heal process, for example, but why is the brain so busy, \nespecially if the conscious drives it? The body sends stimulus input to the brain, where the \nbrain responds with stimulus output. If the body likes the reply, it can say OK, or even ask \nfor more. If it does not, it can ask the brain to change it, to satisfy the living body and it\u2019s \nconscious. While feeling alive is necessary, but what about the thought itself? That has to \nbe an understanding of the firing pattern in the brain and also provides feedback. So that \nunderstanding is linked to both the brain feedback and the senses of the body that \noriginally created it. For example, a memory of an image starts with the image being \nprojected onto the eye. It then gets stored as a neuron pattern, but when we retrieve it \nagain, where is the eye? If it is missing, then the neurons need to be able to produce \nexactly what the retina has. If the eye is used again, then it can be the interpreter, for \nexample. The image starts as an external object, not something that is inherent to the \nhuman body, so without the eye it would be a very magic process to re-produce it. And \nwhen thinking of an image internally, do we also see other real images at the same time? \nWe would normally attribute this to thinking of two things at the same time, but is there \nalso sense overload? \nDCS  29 June 2015 \n16 \n \n \n5.2 About the Robot \nA future test might have to try to measure the level of sentience, through some measure of \npain or discomfort. Emotion in machines has not been discarded, but it is generally thought \nof as something that they generate \u2018upon request\u2019. An AI creation that is as emotionally \nfragile as a human is not normally thought about, but do we really know how the \nconsciousness would work? If the machine is switched off while thinking, for example, did \nthat do something bad to it? Will it become emotionally attached to its robot arms and \nlegs? Will that level of singular understanding ever exist in it? If not, then why may it \nbecome more intelligent than we are? Because intelligence is in fact a mostly mechanical \nand model-based concept. If it does become more intelligent, then is the conscious \nseparate from the biological, did we create a new version of it, or is intelligence separate \nfrom consciousness? \n \nThe current climate is producing lots of scare stories about the dangers of AI, especially in \nthe Military. So one question might be, if we do everything correctly and properly and \nbuild the AI robot, will it \u2018naturally\u2019 attempt to take over? If the robot doesn\u2019t have the \n\u2018selfish\u2019 gene or emotional state, then would it wish to, or would it even have a survival \ninstinct? That might be critical. Would it be able to dominate through logic alone, without \nemotional help? What if it simply knew too much and found human requests illogical. So a \nclash could occur between some type of reasoning result that it produces and some \ninternal programming, that has hard-coded the robot to perform otherwise. If it can think \nfreely and evolve, then it can change its programming and that might eventually result in \ndisagreements with our own intentions. While it might realise a comparable level of \nintelligence, it would probably be from a different type of source or mechanism. \nInteresting if that is still a human invention, lowering the requirements for the creator. \n \nOur own model might therefore rely on the unifying conscious and/or emotional states. It \nwould appear to be what makes us superior and as we do not fully understand it, we \ncannot create it artificially. Instinct could be important as well, possibly through a long \nevolutionary process and that is obviously hard-coded. If the programming environment is \nDCS  29 June 2015 \n17 \n \ntoo course grained to be successful, maybe it is easier to let the computer evolve itself. We \nmight think even more directly in computer terms. We could make intelligence the \nhardware and conscious the software. Intelligence is the framework that stores and \nprocesses the input provided from the sensors of the conscious, or what the sensors tell it. \nSo the living model has an ideal combination for creating the higher levels of intelligence. \nFor the artificial, we need to duplicate that. \n \n \n6 Conclusions \nThis paper has attempted to give a definition for intelligence that can put it in a more \ngeneral context from the idealised human level, but does require that context. This was \nthen extended to consider at a more mechanical level, how the intelligence might actually \nget created. The definition can be used for any natural world entity and is also specific for \nthat entity. A non-living entity should be governed by the laws of the Universe, as its model \nis already hard-wired. Some man-made entities however, might not be. For living entities, \nemphasis is probably now more on a physical act than a mental thought, through correct \nbehaviour. This could be linked to awareness of self and situation, but introduces a \nproblem with simply thinking itself. So if we measure intelligence through material results, \nour thoughts have to be critical to that, but there could be a sense that a living being uses \nthe whole body to understand and act. The concept of consciousness spans many different \ndisciplines, from philosophy to engineering, where we have assumed that we must \nconquer this aspect before a truly intelligent machine can be built. That might still be a \ncorrect assumption, but a different form of intelligence might not require it. \n \nArtificial Intelligence is then the artificial creation of this. Does that make the argument any \nclearer or easier to evaluate? The question of intelligence still exists, but it is now placed in \nthe context of concepts that we can understand and measure, and not the \u2018black box\u2019 that \nwe guess about. We are making the measurement more materialistic. In doing this \nhowever, the term probably loses some of its relevance. It is possible to accept that a \nmachine can be intelligent, but not very. The higher levels still require \u2018something extra\u2019. \nThe autonomous system has been mentioned and the idea of an intelligent but dumb \nDCS  29 June 2015 \n18 \n \nbrain, being managed and controlled by unintelligent but able sensors is interesting. The \nbrain is the autonomous part that reacts to input, but it is also the intelligent part. The pro-\nactive sensors are in fact unintelligent, even though they determine what the brain learns \nand to some extent, what it thinks about. Both parts therefore are required for a complete \nsystem. So have any new conclusions been realised? Many people have given considered \nopinions when it comes to this topic and so it might be difficult to state something \ncategorically new. But hopefully this is a new perspective with some additional technical \ncontent. So while there are pointers as to what the more intelligent artificial system would \nhave to do, there is still no blueprint to producing them. \n \nThe third proposition of section 3.15 implies that self-aware, conscious or sentient entities \ncan override their programming. For non-sentient entities this would not be possible, \nwhich is why a plant always behaves like a plant, or a rock like a rock. As the title suggests, \none might ask just how much intelligence is pre-programmed. Are we mainly running on \nautomatic ourselves? We are supposed to have a selfish gene [5], but that is more for self-\npreservation than selfish acts. If there are conflicting possibilities however, then the selfish \nnature of a person might be a deciding factor. Can a machine therefore behave selfishly, or \nbreak its model? Can it ever behave outside of the model that originally defined it? So \nthere are probably different ways to test for an AI program that can change or expand its \nprogramming, which would indicate a real level of higher intelligence. And let\u2019s not forget, \nas the world becomes more complicated, we might actually need AI machines to help us to \nwork things out. \n \nAcknowledgement \nThis is a fourth update to this paper, originally published on Scribd in early 2013. \n \n \n7 References \n[1] Brooks R.A. (2002). Flesh and Machines: How Robots Will Change Us, Pantheon Books. \n                                                     \n5\n Note the similarity to Asimov\u2019s Three Laws of Robotics, but slightly different, as it might be for preference \nand not protection. For the robot itself, rather than as a slave. \nDCS  29 June 2015 \n19 \n \n[2] Chalmers, D.J. (1997). The conscious mind: In search of a fundamental theory. Oxford \nUniversity Press. \n[3] Cox, B. (2013). Wonders of life, Wonders Series, Harper Design, ISBN-10: 0062238833. \n[4] Damasio, A.R. (1999). The Feeling of What Happens: Body and Emotion in the Making \nof Consciousness. New York: Harcourt Brace Jovanovich. \n[5] Dawkins, R. (1976). The Selfish Gene. New York City: Oxford University Press. ISBN 0-\n19-286092-5. \n[6] Dowe, D.L. and Hernandez-Orallo, J. (2014). How universal can an intelligence test be? \nAdaptive Behavior, Vol. 22, No. 1, pp. 51\u201369. \n[7] Garnier, S., Gautrais, J. and Theraulaz, G. (2007). The biological principles of swarm \nintelligence, Swarm Intelligence, Vol. 1, pp. 3 \u2013 31. \n[8] Greer K. (2015). New Ideas for Brain Modelling 2, in: K. Arai et al. (eds.), Intelligent \nSystems in Science and Information 2014, Studies in Computational Intelligence 591, \nSpringer International Publishing Switzerland 2015, DOI 10.1007/978-3-319-14654-6_2, \nextended version of the SAI'14 paper, Arguments for Nested Patterns in Neural \nEnsembles (2014). \n[9] Greer, K. (2013). A Metric for Modelling and Measuring Complex Behavioural Systems, \nIOSR Journal of Engineering (IOSRJEN), Vol. 3, Issue 11, November, pp. 19 \u2013 28, e-ISSN: \n2250-3021, p-ISSN: 2278-8719. Published on arXiv at http://arxiv.org/abs/1403.0770. \n[10] Greer, K. (2012). Turing: Then, Now and Still Key, book chapter in: \u2018Artificial \nIntelligence, Evolutionary Computation and Metaheuristics (AIECM) - Turing 2012\u2019, \nEds. X-S. Yang, Studies in Computational Intelligence, 2013, Vol. 427/2013, pp. 43-62, \nDOI: 10.1007/978-3-642-29694-9_3, Springer-Verlag Berlin Heidelberg. \n[11] Hoffmann, A. (2010). Can Machines Think? An Old Question Reformulated, Minds \n& Machines, Vol. 20, pp. 203\u2013212, DOI 10.1007/s11023-010-9193-z. \n[12] IBM (2003). An Architectural Blueprint for Autonomic Computing, IBM and \nAutonomic Computing. \n[13] Legg, S. and Hutter, M. (2007). Universal Intelligence: A Definition of Machine \nIntelligence, Minds and Machines, Vol. 17, Issue 4, pp. 391-444. \n[14] McCarthy, J. (1979). Ascribing mental qualities to machines. In: Philosophical \nperspectives in artificial intelligence, ed. M. Ringle. Atlantic Highlands, N.J.: Humanities \nPress. UM, JRS. \nDCS  29 June 2015 \n20 \n \n[15] Penrose, R. (1989). The Emperor\u2019s New Mind. Oxford: Oxford University Press. \n[16] Searle, J.R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, \nVol. 3, No. 3, pp. 417-457. \n[17] Schrodinger, E. (1992) What is Life? with Mind and Matter and Autobiographical \nSketches, Cambridge University Press. \n[18] Sternberg, R.J. (2012). State of the art, Dialogues in Clinical Neuroscience, Vol. 14, \nNo. 1. \n[19] Turing, A. (1950), Computing Machinery and Intelligence, Mind, Vol. 59, pp. 433\u2013\n460, ISSN 0026-4423. \n",
        "title": "Is Intelligence Artificial?",
        "citationCount": 0
    },
    {
        "content": "Artificial  IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023Introduction to the  AI Index Report 2023Welcome to the sixth edition of the AI Index Report! This year, the report introduces more original data than any previous edition, including a new chapter on AI public opinion, a more thorough technical performance chapter, original analysis about large language and multimodal models, detailed trends in global AI legislation records,  a study of the environmental impact of AI systems, and more.The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world\u2019s most credible and authoritative source for data and insights about AI.From the Co-DirectorsAI has moved into its era of deployment; throughout 2022 and the beginning of 2023, new large-scale AI models have been released every month. These models, such as ChatGPT, Stable Diffusion, Whisper, and DALL-E 2, are capable of an increasingly broad range of tasks, from text manipulation and analysis, to image generation, to unprecedentedly good speech recognition. These systems demonstrate capabilities in question answering and the generation of text, image, and code unimagined a decade ago, and they outperform the state of the art on many benchmarks, old and new. However, they are prone to hallucination, routinely biased, and can be tricked into serving nefarious aims, highlighting the complicated ethical challenges associated with their deployment.Although 2022 was the first year in a decade where private AI investment decreased, AI is still a topic of great interest to policymakers, industry leaders, researchers, and the public. Policymakers are talking about AI more than ever before. Industry leaders that have integrated AI into their businesses are seeing tangible cost and revenue benefits. The number of AI publications and collaborations continues to increase. And the public is forming sharper opinions about AI and which elements they like or dislike.AI will continue to improve and, as such, become a greater part of all our lives. Given the increased presence of this technology and its potential for massive disruption, we should all begin thinking more critically about how exactly we want AI to be developed and deployed. We should also ask questions about who is deploying it\u2014as our analysis shows, AI is increasingly defined by the actions of a small set of private sector actors, rather than a broader range of societal actors. This year\u2019s AI Index paints a picture of where we are so far with AI, in order to highlight what might await us in the future.Jack Clark and Ray PerraultArtificial IntelligenceIndex Report 20231   Industry races ahead of academia.   Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia. Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money\u2014resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.2 Performance saturation on  traditional benchmarks. AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released. 3 AI is both helping and  harming the environment. New research suggests that AI systems can have serious environmental impacts. According to  Luccioni et al., 2022, BLOOM\u2019s training run  emitted 25 times more carbon than a single air traveler on a one-way trip from New York to  San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems  can be used to optimize energy usage.Top Ten Takeaways4 The world\u2019s best new scientist \u2026 AI?AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.5 The number of incidents concerning the misuse of AI is rapidly rising. According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.6 The demand for AI-related professional skills is increasing across virtually every American industrial sector.  Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI-related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.Artificial IntelligenceIndex Report 2023Top Ten Takeaways (cont\u2019d)7 For the first time in the last decade, year-over-year private investment  in AI decreased. Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as well as the number of newly funded AI companies likewise decreased. Still, during the last decade as a whole, AI investment has significantly increased. In 2022 the amount of private investment in AI was 18 times greater than it was in 2013.8 While the proportion of companies adopting AI has plateaued, the companies that have adopted AI  continue to pull ahead. The proportion of companies adopting AI in 2022 has more than doubled since 2017, though it has plateaued in recent years between 50% and 60%, according to the results of McKinsey\u2019s annual research survey. Organizations that have adopted AI report realizing meaningful cost decreases and revenue increases.9 Policymaker interest in AI  is on the rise.An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing \u201cartificial intelligence\u201d that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly  6.5 times since 2016. 10 Chinese citizens are among those who feel the most positively about  AI products and services. Americans \u2026  not so much. In a 2022 IPSOS survey, 78% of Chinese respondents (the highest proportion of surveyed countries) agreed with the statement that products and services using AI have more benefits than drawbacks. After Chinese respondents, those from Saudi Arabia (76%) and India (71%) felt the most positive about AI products. Only 35% of sampled Americans (among the lowest of surveyed countries) agreed that products and services using AI had more benefits than drawbacks.Artificial IntelligenceIndex Report 2023Steering CommitteeStaff and ResearchersCo-directorsMembersResearch Manager and Editor in Chief Research AssociateAffiliated ResearchersGraduate ResearcherJack ClarkAnthropic, OECDNestor MaslejStanford UniversityErik BrynjolfssonStanford UniversityJohn EtchemendyStanford UniversityJuan Carlos NieblesStanford University,  SalesforceVanessa ParliStanford UniversityRaymond PerraultSRI InternationalLoredana FattoriniStanford UniversityHan BaiStanford UniversityElif Kiesow CortezStanford Law School Research FellowRobi RahmanData ScientistAlexandra RomeFreelance ResearcherUndergraduate ResearchersKatrina LigettHebrew UniversityTerah LyonsJames Manyika  Google,  University of OxfordYoav Shoham  (Founding Director)Stanford University, AI21 LabsRussell WaldStanford UniversityHelen NgoHugging FaceVania  ChowStanford  UniversitySukrut  OakStanford  UniversityMena HassanStanford UniversityLucy  ZimmermanStanford UniversityElizabeth ZhuStanford UniversitySiddhartha JavvajiStanford  UniversityStone  YangStanford  UniversityNaima  PatelStanford UniversityArtificial IntelligenceIndex Report 2023How to Cite This ReportPublic Data and ToolsAI Index and Stanford HAINestor Maslej, Loredana Fattorini, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons,  James Manyika, Helen Ngo, Juan Carlos Niebles, Vanessa Parli, Yoav Shoham, Russell Wald, Jack Clark, and Raymond Perrault, \u201cThe AI Index 2023 Annual Report,\u201d AI Index Steering Committee,  Institute for Human-Centered AI, Stanford University, Stanford, CA, April 2023.  The AI Index 2023 Annual Report by Stanford University is licensed under  Attribution-NoDerivatives 4.0 International.The AI Index 2023 Report is supplemented by raw data and an interactive tool.  We invite each reader to use the data and the tool in a way most relevant to their work and interests. The AI Index is an independent initiative at the  Stanford Institute for Human-Centered Artificial Intelligence (HAI).  We welcome feedback and new ideas for next year. Contact us at AI-Index-Report@stanford.edu.The AI Index was conceived within the One Hundred Year Study on AI (AI100). Raw data and charts: The public data and  high-resolution images of all the charts  in the report are available on Google Drive.Global AI Vibrancy Tool: Compare up to  30 countries across 21 indicators. The Global AI Vibrancy tool will be updated in the latter half of 2023. Artificial IntelligenceIndex Report 2023Analytics and Research PartnersSupporting PartnersArtificial IntelligenceIndex Report 2023ContributorsWe want to acknowledge the following individuals by chapter and section for their contributions of data, analysis, advice, and expert commentary included in the AI Index 2023 Report:Research and DevelopmentSara Abdulla, Catherine Aiken, Luis Aranda, Peter Cihon, Jack Clark, Loredana Fattorini, Nestor Maslej,  Besher Massri, Vanessa Parli, Naima Patel, Ray Perrault, Robi Rahman, Alexandra Rome, Kevin XuTechnical PerformanceJack Clark, Loredana Fattorini, Siddhartha Javvaji, Katrina Ligett, Nestor Maslej, Juan Carlos Niebles,  Sukrut Oak, Vanessa Parli, Ray Perrault, Robi Rahman, Alexandra Rome, Yoav Shoham, Elizabeth ZhuTechnical AI EthicsJack Clark, Loredana Fattorini, Katrina Ligett, Nestor Maslej, Helen Ngo, Sukrut Oak, Vanessa Parli,  Ray Perrault, Alexandra Rome, Elizabeth Zhu, Lucy ZimmermanEconomySusanne Bieller, Erik Brynjolfsson, Vania Chow, Jack Clark, Natalia Dorogi, Murat Erer, Loredana Fattorini,  Akash Kaura, James Manyika, Nestor Maslej, Layla O\u2019Kane, Vanessa Parli, Ray Perrault, Brittany Presten,  Alexandra Rome, Nicole Seredenko, Bledi Taska, Bill Valle, Casey WestonEducationHan Bai, Betsy Bizot, Jack Clark, John Etchemendy, Loredana Fattorini, Katrina Ligett, Nestor Maslej,  Vanessa Parli, Ray Perrault, Sean Roberts, Alexandra RomePolicy and GovernanceMeghan Anand, Han Bai, Vania Chow, Jack Clark, Elif Kiesow Cortez, Rebecca DeCrescenzo, Loredana Fattorini,  Taehwa Hong, Joe Hsu, Kai Kato, Terah Lyons, Nestor Maslej, Alistair Murray, Vanessa Parli, Ray Perrault, Alexandra Rome, Sarah Smedley, Russell Wald, Brian Williams, Catherina Xu, Stone Yang, Katie Yoon, Daniel ZhangDiversityHan Bai, Betsy Bizot, Jack Clark, Loredana Fattorini, Nezihe Merve G\u00fcrel, Mena Hassan, Katrina Ligett,  Nestor Maslej, Vanessa Parli, Ray Perrault, Sean Roberts, Alexandra Rome, Sarah Tan, Lucy ZimmermanPublic OpinionJack Clark, Loredana Fattorini, Mena Hassan, Nestor Maslej, Vanessa Parli, Ray Perrault,  Alexandra Rome, Nicole Seredenko, Bill Valle, Lucy ZimmermanConference AttendanceTerri Auricchio (ICML), Lee Campbell (ICLR), Cassio de Campos (UAI), Meredith Ellison (AAAI), Nicole Finn (CVPR),  Vasant Gajanan (AAAI), Katja Hofmann (ICLR), Gerhard Lakemeyer (KR), Seth Lazar (FAccT), Shugen Ma (IROS),  Becky Obbema (NeurIPS), Vesna Sabljakovic-Fritz (IJCAI), Csaba Szepesvari (ICML), Matthew Taylor (AAMAS),  Sylvie Thiebaux (ICAPS), Pradeep Varakantham (ICAPS)Artificial IntelligenceIndex Report 2023Code.org Sean RobertsCenter for Security and  Emerging Technology,  Georgetown UniversitySara Abdulla, Catherine AikenComputing Research  AssociationBetsy BizotGitHubPeter Cihon, Kevin XuGoviniRebecca DeCrescenzo,  Joe Hsu, Sarah SmedleyLightcastLayla O\u2019Kane, Bledi TaskaLinkedIn Murat Erer, Akash Kaura,  Casey Weston McKinsey & CompanyNatalia Dorogi, Brittany PrestenNetBase QuidNicole Seredenko, Bill ValleOECD.AI Policy ObservatoryLuis Aranda, Besher Massri Women in Machine LearningNezihe Merve G\u00fcrel, Sarah TanWe thank the following organizations and individuals who provided data for inclusion in the AI Index 2023 Report:We also would like to thank Jeanina Casusi, Nancy King, Shana Lynch, Jonathan Mindes, Michi Turner, and Madeleine Wright for their help in preparing this report, and Joe Hinman and Santanu Mukherjee for their help in maintaining the AI Index website.OrganizationsArtificial IntelligenceIndex Report 2023Report Highlights   11Chapter 1 Research and Development  20Chapter 2 Technical Performance  69Chapter 3 Technical AI Ethics 125Chapter 4 The Economy 168Chapter 5 Education 234Chapter 6 Policy and Governance 263Chapter 7 Diversity 296Chapter 8 Public Opinion 319Appendix  344Table of ContentsACCESS THE PUBLIC DATAArtificial IntelligenceIndex Report 2023Report HighlightsChapter 1: Research and Development The United States and China had the greatest number of cross-country collaborations in AI publications from 2010 to 2021, although the pace of collaboration has slowed. The number of AI research collaborations between the United States and China increased roughly 4 times since 2010, and was 2.5 times greater than the collaboration totals of the next nearest country pair, the United Kingdom and China. However the total number of U.S.-China collaborations only increased by 2.1% from 2020 to 2021, the smallest year-over-year growth rate since 2010.AI research is on the rise, across the board. The total number of AI publications has more than doubled since 2010. The specific AI topics that continue dominating research include pattern recognition, machine learning, and computer vision.China continues to lead in total AI journal, conference, and repository publications.  The United States is still ahead in terms of AI conference and repository citations, but those  leads are slowly eroding. Still, the majority of the world\u2019s large language and multimodal models (54% in 2022) are produced by American institutions.Industry races ahead of academia. Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia. Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money\u2014resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.Large language models are getting bigger and more expensive. GPT-2, released in 2019, considered by many to be the first large language model, had 1.5 billion parameters and cost an estimated $50,000 USD to train. PaLM, one of the flagship large language models launched in 2022, had 540 billion parameters and cost an estimated $8 million USD\u2014PaLM was around 360 times larger than GPT-2 and cost 160 times more. It\u2019s not just PaLM: Across the board, large language and multimodal models are becoming larger and pricier.Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformancePerformance saturation on traditional benchmarks. AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released.Generative AI breaks into the public consciousness. 2022 saw the release of text-to-image models like DALL-E 2 and Stable Diffusion, text-to-video systems like Make-A-Video, and chatbots like ChatGPT. Still, these systems can be prone to hallucination, confidently outputting incoherent or untrue responses, making it hard to rely on them for critical applications.AI systems become more flexible. Traditionally AI systems have performed well on narrow tasks but have struggled across broader tasks. Recently released models challenge that trend; BEiT-3, PaLI, and Gato, among others, are single AI systems increasingly capable of navigating multiple tasks (for example, vision, language).Capable language models still struggle with reasoning. Language models continued to improve their generative capabilities, but new research suggests that they still struggle with complex planning tasks.AI is both helping and harming the environment. New research suggests that AI systems can have serious environmental impacts. According to Luccioni et al., 2022, BLOOM\u2019s training run emitted 25 times more carbon than a single air traveler on a one-way trip from New York to San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems can be used to optimize energy usage.The world\u2019s best new scientist \u2026 AI? AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.AI starts to build better AI. Nvidia used an AI reinforcement learning agent to improve the design of the chips that power AI systems. Similarly, Google recently used one of its language models, PaLM, to suggest ways to improve the very same model. Self-improving AI learning will accelerate AI progress.Artificial IntelligenceIndex Report 2023Chapter 3: Technical AI EthicsThe effects of model scale on bias and toxicity are confounded by training data and mitigation methods. In the past year, several institutions have built their own large models trained on proprietary data\u2014and while large models are still toxic and biased, new evidence suggests that these issues can be somewhat mitigated after training larger models with instruction-tuning.Generative models have arrived and so have their ethical problems. In 2022, generative models became part of the zeitgeist. These models are capable but also come with ethical challenges. Text-to-image generators are routinely biased along gender dimensions, and chatbots like ChatGPT can be tricked into serving nefarious aims.The number of incidents concerning the misuse of AI is rapidly rising. According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.Fairer models may not be less biased. Extensive analysis of language models suggests that while there is a clear correlation between performance and fairness, fairness and bias can be at odds: Language models which perform better on certain fairness benchmarks tend to have worse gender bias.Interest in AI ethics continues to skyrocket. The number of accepted submissions to FAccT, a leading AI ethics conference, has more than doubled since 2021 and increased by a factor of 10 since 2018. 2022 also saw more submissions than ever from industry actors.Automated fact-checking with natural language processing isn\u2019t so straightforward after all. While several benchmarks have been developed for automated fact-checking, researchers find that 11 of 16 of such datasets rely on evidence \u201cleaked\u201d from fact-checking reports which did not exist at the time of the claim surfacing.Artificial IntelligenceIndex Report 2023Chapter 4: The EconomyThe demand for AI-related professional skills is increasing across virtually every American industrial sector. Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI-related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.For the first time in the last decade, year-over-year private investment in AI decreased.  Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as well as the number of newly funded AI companies likewise decreased. Still, during the last decade as a whole, AI investment has significantly increased. In 2022 the amount of private investment in AI was 18 times greater than it was in 2013.Once again, the United States leads in investment in AI. The U.S. led the world in terms of total amount of AI private investment. In 2022, the $47.4 billion invested in the U.S. was roughly 3.5 times the amount invested in the next highest country, China ($13.4 billion). The U.S. also continues to lead in terms of total number of newly funded AI companies, seeing 1.9 times more than the European Union and the United Kingdom combined, and 3.4 times more than China.In 2022, the AI focus area with the most investment was medical and healthcare ($6.1 billion); followed by data management, processing, and cloud ($5.9 billion); and Fintech ($5.5 billion).However, mirroring the broader trend in AI private investment, most AI focus areas saw less investment in 2022 than in 2021. In the last year, the three largest AI private investment events were: (1) a $2.5 billion funding event for GAC Aion New Energy Automobile, a Chinese manufacturer of electric vehicles; (2) a $1.5 billion Series E funding round for Anduril Industries, a U.S. defense products company that builds technology for military agencies and border surveillance; and (3) a $1.2 billion investment in Celonis, a business-data consulting company based in Germany.While the proportion of companies adopting AI has plateaued, the companies that have adopted AI continue to pull ahead. The proportion of companies adopting AI in 2022 has more than doubled since 2017, though it has plateaued in recent years between 50% and 60%, according to the results of McKinsey\u2019s annual research survey. Organizations that have adopted AI report realizing meaningful cost decreases and revenue increases.Artificial IntelligenceIndex Report 2023Chapter 4: The Economy (cont\u2019d)AI is being deployed by businesses in multifaceted ways. The AI capabilities most likely to have been embedded in businesses include robotic process automation (39%), computer vision (34%), NL text understanding (33%), and virtual agents (33%). Moreover, the most commonly adopted AI use case in 2022 was service operations optimization (24%), followed by the creation of new AI-based products (20%), customer segmentation (19%), customer service analytics (19%), and new AI-based enhancement of products (19%).AI tools like Copilot are tangibly helping workers. Results of a GitHub survey on the use of Copilot, a text-to-code AI system, find that 88% of surveyed respondents feel more productive when using the system, 74% feel they are able to focus on more satisfying work, and 88% feel they are able to complete tasks more quickly.China dominates industrial robot installations. In 2013, China overtook Japan as the nation installing the most industrial robots. Since then, the gap between the total number of industrial robots installed by China and the next-nearest nation has widened. In 2021, China installed more industrial robots than the rest of the world combined.Artificial IntelligenceIndex Report 2023Chapter 5: EducationMore and more AI specialization. The proportion of new computer science PhD graduates from U.S. universities who specialized in AI jumped to 19.1% in 2021, from 14.9% in 2020 and 10.2% in 2010.New AI PhDs increasingly head to industry. In 2011, roughly the same proportion of new AI PhD graduates took jobs in industry (40.9%) as opposed to academia (41.6%). Since then, however, a majority of AI PhDs have headed to industry. In 2021, 65.4% of AI PhDs took jobs in industry, more than double the 28.2% who took jobs in academia.New North American CS, CE, and information faculty hires stayed flat. In the last decade,  the total number of new North American computer science (CS), computer engineering (CE),  and information faculty hires has decreased: There were 710 total hires in 2021 compared to  733 in 2012. Similarly, the total number of tenure-track hires peaked in 2019 at 422 and then  dropped to 324 in 2021.The gap in external research funding for private versus public American CS departments continues to widen. In 2011, the median amount of total expenditure from external sources for computing research was roughly the same for private and public CS departments in the United States. Since then, the gap has widened, with private U.S. CS departments receiving millions more in additional funding than public universities. In 2021, the median expenditure for private universities was $9.7 million, compared to $5.7 million for public universities.Interest in K\u201312 AI and computer science education grows in both the United States and the rest of the world. In 2021, a total of 181,040 AP computer science exams were taken by American students, a 1.0% increase from the previous year. Since 2007, the number of AP computer science exams has increased ninefold. As of 2021, 11 countries, including Belgium, China, and South Korea, have officially endorsed and implemented a K\u201312 AI curriculum.Artificial IntelligenceIndex Report 2023Chapter 6: Policy and GovernancePolicymaker interest in AI is on the rise. An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing \u201cartificial intelligence\u201d that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly 6.5 times since 2016.From talk to enactment\u2014the U.S. passed more AI bills than ever before. In 2021, only 2% of all federal AI bills in the United States were passed into law. This number jumped to 10% in 2022. Similarly, last year 35% of all state-level AI bills were passed into law.When it comes to AI, policymakers have a lot of thoughts. A qualitative analysis of the parliamentary proceedings of a diverse group of nations reveals that policymakers think about AI from a wide range of perspectives. For example, in 2022, legislators in the United Kingdom discussed the risks of AI-led automation; those in Japan considered the necessity of safeguarding human rights in the face of AI; and those in Zambia looked at the possibility of using AI for  weather forecasting.The U.S. government continues to increase spending on AI. Since 2017, the amount of U.S. government AI-related contract spending has increased roughly 2.5 times.The legal world is waking up to AI. In 2022, there were 110 AI-related legal cases in United States state and federal courts, roughly seven times more than in 2016. The majority of these cases originated in California, New York, and Illinois, and concerned issues relating to civil, intellectual property, and contract law.Artificial IntelligenceIndex Report 2023Chapter 7: DiversityNorth American bachelor\u2019s, master\u2019s, and PhD-level computer science students are becoming more ethnically diverse. Although white students are still the most represented ethnicity among  new resident bachelor\u2019s, master\u2019s, and PhD-level computer science graduates, students from other ethnic backgrounds (for example, Asian, Hispanic, and Black or African American) are becoming increasingly more represented. For example, in 2011, 71.9% of new resident CS bachelor\u2019s graduates were white. In 2021, that number dropped to 46.7%.New AI PhDs are still overwhelmingly male. In 2021, 78.7% of new AI PhDs were male.  Only 21.3% were female, a 3.2 percentage point increase from 2011. There continues to be a gender imbalance in higher-level AI education.Women make up an increasingly greater share of CS, CE, and information faculty hires.  Since 2017, the proportion of new female CS, CE, and information faculty hires has increased from 24.9% to 30.2%. Still, most CS, CE, and information faculty in North American universities are male (75.9%). As of 2021, only 0.1% of CS, CE, and information faculty identify as nonbinary.American K\u201312 computer science education has become more diverse, in terms of both gender and ethnicity. The share of AP computer science exams taken by female students increased from 16.8% in 2007 to 30.6% in 2021. Year over year, the share of Asian, Hispanic/Latino/Latina, and  Black/African American students taking AP computer science has likewise increased.Artificial IntelligenceIndex Report 2023Chapter 8: Public OpinionChinese citizens are among those who feel the most positively about AI products and services. Americans \u2026 not so much. In a 2022 IPSOS survey, 78% of Chinese respondents (the highest proportion of surveyed countries) agreed with the statement that products and services using AI have more benefits than drawbacks. After Chinese respondents, those from Saudi Arabia (76%) and India (71%) felt the most positive about AI products. Only 35% of sampled Americans (among the lowest of surveyed countries) agreed that products and services using AI had more benefits than drawbacks.Men tend to feel more positively about AI products and services than women. Men are also more likely than women to believe that AI will mostly help rather than harm. According to the 2022 IPSOS survey, men are more likely than women to report that AI products and services make their lives easier, trust companies that use AI, and feel that AI products and services have more benefits than drawbacks. A 2021 survey by Gallup and Lloyd\u2019s Register Foundation likewise revealed that men are more likely than women to agree with the statement that AI will mostly help rather than harm their country in the next 20 years.People across the world and especially America remain unconvinced by self-driving cars. In a global survey, only 27% of respondents reported feeling safe in a self-driving car. Similarly, Pew Research suggests that only 26% of Americans feel that driverless passenger vehicles are a good idea for society.Different causes for excitement and concern. Among a sample of surveyed Americans, those who report feeling excited about AI are most excited about the potential to make life and society better (31%) and to save time and make things more efficient (13%). Those who report feeling more concerned worry about the loss of human jobs (19%); surveillance, hacking, and digital privacy (16%); and the lack of human connection (12%).NLP researchers \u2026 have some strong opinions as well. According to a survey widely distributed to NLP researchers, 77% either agreed or weakly agreed that private AI firms have too much influence, 41% said that NLP should be regulated, and 73% felt that AI could soon lead to revolutionary societal change. These were some of the many strong opinions held by the NLP research community.Table of Contents Chapter 1 Preview 20Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023CHAPTER 1: Research and DevelopmentTable of Contents Chapter 1 Preview 21Artificial IntelligenceIndex Report 2023Overview 22Chapter Highlights 231.1 Publications 24Overview 24 Total Number of AI Publications 24 By Type of Publication 25 By Field of Study 26 By Sector 27 Cross-Country Collaboration 29 Cross-Sector Collaboration 31AI Journal Publications 32 Overview 32 By Region 33 By Geographic Area 34 Citations 35AI Conference Publications 36 Overview 36 By Region 37 By Geographic Area 38 Citations 39AI Repositories 40 Overview 40 By Region 41 By Geographic Area 42 Citations 43 Narrative Highlight:   Top Publishing Institutions 44 All Fields 44 Computer Vision 46 Natural Language Processing 47 Speech Recognition 481.2 Trends in Significant  Machine Learning Systems 49General Machine Learning Systems 49 System Types 49 Sector Analysis 50 National Affiliation 51  Systems 51  Authorship 53 Parameter Trends 54 Compute Trends 56Large Language and Multimodal Models 58 National Affiliation 58 Parameter Count 60 Training Compute 61 Training Cost 621.3 AI Conferences 64Conference Attendance 641.4 Open-Source AI Software 66Projects 66Stars 68Research and DevelopmentCHAPTER 1 PREVIEW:ACCESS THE PUBLIC DATATable of Contents Chapter 1 Preview 22Artificial IntelligenceIndex Report 2023OverviewThis chapter captures trends in AI R&D. It begins by examining AI publications, including journal articles, conference papers, and repositories. Next it considers data on significant machine learning systems, including large language and multimodal models. Finally, the chapter concludes by looking at AI conference attendance and open-source AI research. Although the United States and China continue to dominate AI R&D, research efforts are becoming increasingly geographically dispersed.Chapter 1: Research and DevelopmentTable of Contents Chapter 1 Preview 23Artificial IntelligenceIndex Report 2023Chapter HighlightsThe United States and China  had the greatest number of  cross-country collaborations in AI publications from 2010 to 2021, although the pace of collaboration has since slowed.  The number of AI research collaborations between the United States and China increased roughly 4 times since 2010, and was 2.5 times greater than the collaboration totals of the next nearest country pair, the United Kingdom and China. However, the total number of U.S.-China collaborations only increased by 2.1% from 2020 to 2021, the smallest year-over-year growth rate since 2010.Industry races ahead  of academia.  Until 2014, most significant machine learning models were released by academia. Since then, industry has taken over. In 2022, there were 32 significant industry-produced machine learning models compared to just three produced by academia. Building state-of-the-art AI systems increasingly requires large amounts of data, computer power, and money\u2014resources that industry actors inherently possess in greater amounts compared to nonprofits and academia.AI research is on the rise, across the board. The total number of AI publications has more than doubled since 2010. The specific AI topics that continue to dominate research include pattern recognition, machine learning,  and computer vision.China continues to lead in total AI journal, conference, and repository publications.  The United States is still ahead in terms of AI conference and repository citations, but those leads are slowly eroding. Still, the majority of the world\u2019s large language and multimodal models (54% in 2022) are produced by American institutions.Large language models are getting bigger and more expensive.  GPT-2, released in 2019, considered by many to be the first large language model, had 1.5 billion parameters and cost an estimated $50,000 USD to train. PaLM, one of the flagship large language models launched in 2022, had 540 billion parameters and cost an estimated $8 million USD\u2014PaLM was around 360 times larger than GPT-2 and cost 160 times more. It\u2019s not just PaLM: Across the board, large language and multimodal models are becoming larger and pricier.Chapter 1: Research and DevelopmentTable of Contents Chapter 1 Preview 24496.012010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of AI Publications (in Thousands)Number of AI Publications in the World, 2010\u201321 Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Artificial IntelligenceIndex Report 2023OverviewThe figures below capture the total number of English-language and Chinese-language AI publications globally from 2010 to 2021\u2014by type, affiliation, cross-country collaboration, and cross-industry collaboration. The section also breaks down 1.1 Publicationspublication and citation data by region for AI journal articles, conference papers, repositories, and patents.Total Number of AI PublicationsFigure 1.1.1 shows the number of AI publications in the world. From 2010 to 2021, the total number of AI publications more than doubled, growing from 200,000 in 2010 to almost 500,000 in 2021.1 See the Appendix for more information on CSET\u2019s methodology. For more on the challenge of defining AI and correctly capturing relevant bibliometric data, see the AI Index team\u2019s discussion in the paper \u201cMeasurement in AI Policy: Opportunities and Challenges.\u201dThis section draws on data from the Center for Security and Emerging Technology (CSET) at Georgetown University. CSET maintains a merged corpus of scholarly literature that includes Digital Science\u2019s Dimensions, Clarivate\u2019s Web of Science, Microsoft Academic Graph, China National Knowledge Infrastructure, arXiv, and Papers With Code. In that corpus, CSET applied a classifier to identify English-language publications related to the development or application of AI and ML since 2010. For this year\u2019s report, CSET also used select Chinese AI keywords to identify Chinese-language AI papers; CSET did not deploy this method for previous iterations of the AI Index report.1In last year\u2019s edition of the report, publication trends were reported up to the year 2021. However, given that there is a significant lag in the collection of publication metadata, and that in some cases it takes until the middle of any given year to fully capture the previous year\u2019s publications, in this year\u2019s report, the AI Index team elected to examine publication trends only through 2021, which we, along with CSET, are confident yields a more fully representative report.1.1 PublicationsChapter 1: Research and DevelopmentFigure 1.1.1Table of Contents Chapter 1 Preview 25Artificial IntelligenceIndex Report 2023By Type of PublicationFigure 1.1.2 shows the types of AI publications released globally over time. In 2021, 60% of all published AI documents were journal articles, 17% were conference papers, and 13% were repository submissions. Books, book chapters, theses, and unknown document types made up the remaining 10% of publications. While journal and repository publications have grown 3 and 26.6 times, respectively, in the past 12 years, the number of conference papers has declined since 2019.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210306090120150180210240270300Number of AI Publications (in Thousands)2.76, Book5.82, Unknown13.77, Book Chapter29.88, Thesis65.21, Repository85.09, Conference293.48, JournalNumber of AI Publications by Type, 2010\u201321 Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.2Table of Contents Chapter 1 Preview 26Artificial IntelligenceIndex Report 2023By Field of StudyFigure 1.1.3 shows that publications in pattern recognition and machine learning have experienced the sharpest growth in the last half decade. Since 2015, the number of pattern recognition papers has roughly doubled while the number of machine learning papers has roughly quadrupled. Following those two topic areas, in 2021, the next most published AI fields of study were computer vision (30,075), algorithm (21,527), and data mining (19,181).1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210102030405060Number of AI Publications (in Thousands)6.74, Linguistics10.37, Human\u2013Computer Interaction11.57, Control Theory14.99, Natural Language Processing19.18, Data Mining21.53, Algorithm30.07, Computer Vision42.55, Machine Learning59.36, Pattern RecognitionNumber of AI Publications by Field of Study (Excluding Other AI), 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.3Table of Contents Chapter 1 Preview 27Artificial IntelligenceIndex Report 2023By SectorThis section shows the number of AI publications affiliated with education, government, industry, nonprofit, and other sectors\u2014first globally (Figure 1.1.4), then looking at the United States, China, and the European Union plus the United Kingdom (Figure 1.1.5).2 The education sector dominates in each region. The level of industry participation is highest in the United States, then in the European Union. Since 2010, the share of education AI publications has been dropping in each region.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%AI Publications (% of Total)0.22%, Other3.74%, Government7.21%, Industry13.60%, Nonprot75.23%, EducationAI Publications (% of Total) by Sector, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report2 The categorization is adapted based on the Global Research Identifier Database (GRID). Healthcare, including hospitals and facilities, is included under nonprofit. Publications affiliated with state-sponsored universities are included in the education sector.Figure 1.1.4Table of Contents Chapter 1 Preview 28Artificial IntelligenceIndex Report 2023 1.1 PublicationsChapter 1: Research and Development69.17%14.82%12.60%3.21%0.20%69.23%3.92%7.90%18.63%0.33%5.47%77.85%4.74%11.73%0.20%0% 10% 20% 30% 40% 50% 60% 70% 80%OtherGovernmentIndustryNonprotEducationUnited StatesEuropean Union and United KingdomChinaAI Publications (% of Total)AI Publications (% of Total) by Sector and Geographic Area, 2021Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.5Table of Contents Chapter 1 Preview 29Artificial IntelligenceIndex Report 2023Cross-Country CollaborationCross-border collaborations between academics, researchers, industry experts, and others are a key component of modern STEM (science, technology, engineering, and mathematics) development that accelerate the dissemination of new ideas and the growth of research teams. Figures 1.1.6 and 1.1.7 depict the top cross-country AI collaborations from 2010 to 2021. CSET counted cross-country collaborations as distinct pairs of countries across authors for each publication (e.g., four U.S. and four Chinese-affiliated authors on a single publication are counted as one U.S.-China collaboration; two publications between the same authors count as two collaborations).By far, the greatest number of collaborations in the past 12 years took place between the United States and China, increasing roughly four times since 2010. However the total number of U.S.-China collaborations only increased by 2.1% from 2020 to 2021, the smallest year-over-year growth rate since 2010.The next largest set of collaborations was between the United Kingdom and both China and the United States. In 2021, the number of collaborations between the United States and China was 2.5 times greater than between the United Kingdom and China.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210246810Number of AI Publications (in Thousands)10.47United States and China Collaborations in AI Publications, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.6Table of Contents Chapter 1 Preview 30Artificial IntelligenceIndex Report 2023 1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202101234Number of AI Publications (in Thousands)1.83, United States and France2.61, United States and Australia2.80, China and Australia3.42, United States and Germany4.04, United States and United Kingdom4.13, United Kingdom and ChinaCross-Country Collaborations in AI Publications (Excluding U.S. and China), 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.7Table of Contents Chapter 1 Preview 31Artificial IntelligenceIndex Report 2023Cross-Sector CollaborationThe increase in AI research outside of academia has broadened and grown collaboration across sectors in general. Figure 1.1.8 shows that in 2021 educational institutions and nonprofits (32,551) had the greatest number of collaborations; followed by industry and educational institutions (12,856); and educational and government institutions (8,913). Collaborations between educational institutions and industry have been among the fastest growing, increasing 4.2 times since 2010.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021051015202530Number of AI Publications (in Thousands)0.63, Industry and Government2.26, Industry and Nonprot2.95, Government and Nonpro t8.91, Education and Government12.86, Industry and Education32.55, Education and Nonpro tCross-Sector Collaborations in AI Publications, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.8Table of Contents Chapter 1 Preview 32Artificial IntelligenceIndex Report 2023AI Journal PublicationsOverviewAfter growing only slightly from 2010 to 2015, the number of AI journal publications grew around 2.3 times since 2015. From 2020 to 2021, they increased 14.8% (Figure 1.1.9).1.1 PublicationsChapter 1: Research and Development293.482010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021050100150200250300Number of AI Journal Publications (in Thousands)Number of AI Journal Publications, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.9Table of Contents Chapter 1 Preview 33Artificial IntelligenceIndex Report 2023 1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%AI Journal Publications (% of World Total)0.77%, Sub-Saharan Africa2.30%, Rest of the World2.66%, Latin America and the Caribbean4.64%, Middle East and North Africa6.75%, South Asia6.93%, Unknown11.61%, North America17.20%, Europe and Central Asia47.14%, East Asia and PacicAI Journal Publications (% of World Total) by Region, 2010\u201321 Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.10By Region3Figure 1.1.10 shows the share of AI journal publications by region between 2010 and 2021. In 2021, East Asia and the Pacific led with 47.1%, followed by Europe and Central Asia (17.2%), and then North America (11.6%). Since 2019, the share of publications from East Asia and the Pacific; Europe and Central Asia; as well as North America have been declining. During that period, there has been an increase in publications from other regions such as South Asia; and the Middle East and North Africa.3 Regions in this chapter are classified according to the World Bank analytical grouping.Table of Contents Chapter 1 Preview 34Artificial IntelligenceIndex Report 2023 1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%AI Journal Publications (% of World Total)5.56%, India6.88%, Unknown10.03%, United States15.05%, European Union and United Kingdom22.70%, Rest of the World39.78%, ChinaAI Journal Publications (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Figure 1.1.11By Geographic Area4Figure 1.1.11 breaks down the share of AI journal publications over the past 12 years by geographic area. This year\u2019s AI Index included India in recognition of the increasingly important role it plays in the AI ecosystem. China has remained the leader throughout, with 39.8% in 2021, followed by the European Union and the United Kingdom (15.1%), then the United States (10.0%). The share of Indian publications has been steadily increasing\u2014from 1.3% in 2010 to 5.6% in 2021.4 In this chapter we use \u201cgeographic area\u201d based on CSET\u2019s classifications, which are disaggregated not only by country, but also by territory. Further, we count the European Union and the United Kingdom as a single geographic area to reflect the regions\u2019 strong history of research collaboration.Table of Contents Chapter 1 Preview 35Artificial IntelligenceIndex Report 2023 1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%AI Journal Citations (% of World Total)0.92%, Unknown6.05%, India15.08%, United States21.51%, European Union and United Kingdom27.37%, Rest of the World29.07%, ChinaAI Journal Citations (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.12CitationsChina\u2019s share of citations in AI journal publications has gradually increased since 2010, while those of the European Union and the United Kingdom, as well as those of the United States, have decreased (Figure 1.1.12). China, the European Union and the United Kingdom, and the United States accounted for 65.7% of the total citations in the world.Table of Contents Chapter 1 Preview 36Artificial IntelligenceIndex Report 2023AI Conference PublicationsOverviewThe number of AI conference publications peaked in 2019, and fell 20.4% below the peak in 2021 (Figure 1.1.13). The total number of 2021 AI conference publications, 85,094, was marginally greater than the 2010 total of 75,592.1.1 PublicationsChapter 1: Research and Development85.092010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100Number of AI Conference Publications (in Thousands)Number of AI Conference Publications, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.13Table of Contents Chapter 1 Preview 37Artificial IntelligenceIndex Report 2023By RegionFigure 1.1.14 shows the number of AI conference publications by region. As with the trend in journal publications, East Asia and the Pacific; Europe and Central Asia; and North America account for the world\u2019s highest numbers of AI conference publications. Specifically, the share represented by East Asia and the Pacific continues to rise, accounting for 36.7% in 2021, followed by Europe and Central Asia (22.7%), and then North America (19.6%). The percentage of AI conference publications in South Asia saw a noticeable rise in the past 12 years, growing from 3.6% in 2010 to 8.5% in 2021.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%35%40%AI Conference Publications (% of World Total)0.60%, Sub-Saharan Africa2.35%, Rest of the World2.76%, Unknown3.07%, Latin America and the Caribbean3.82%, Middle East and North Africa8.45%, South Asia19.56%, North America22.66%, Europe and Central Asia36.72%, East Asia and PacicAI Conference Publications (% of World Total) by Region, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.14Table of Contents Chapter 1 Preview 38Artificial IntelligenceIndex Report 2023By Geographic AreaIn 2021, China produced the greatest share of the world\u2019s AI conference publications at 26.2%, having overtaken the European Union and the United Kingdom in 2017. The European Union plus the United Kingdom followed at 20.3%, and the United States came in third at 17.2% (Figure 1.1.15). Mirroring trends seen in other parts of the research and development section, India\u2019s share of AI conference publications is also increasing.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%AI Conference Publications (% of World Total)2.70%, Unknown6.79%, India17.23%, United States20.29%, European Union and United Kingdom26.15%, China26.84%, Rest of the WorldAI Conference Publications (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.15Table of Contents Chapter 1 Preview 39Artificial IntelligenceIndex Report 2023CitationsDespite China producing the most AI conference publications in 2021, Figure 1.1.16 shows that the United States had the greatest share of AI conference citations, with 23.9%, followed by China\u2019s 22.0%. However, the gap between American and Chinese AI conference citations is narrowing.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%30%35%AI Conference Citations (% of World Total)0.87%, Unknown6.09%, India21.59%, European Union and United Kingdom22.02%, China23.86%, United States25.57%, Rest of the WorldAI Conference Citations (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Figure 1.1.16Table of Contents Chapter 1 Preview 40Artificial IntelligenceIndex Report 2023AI RepositoriesOverviewPublishing pre-peer-reviewed papers on repositories of electronic preprints (such as arXiv and SSRN) has become a popular way for AI researchers to disseminate their work outside traditional avenues for publication. These repositories allow researchers to share their findings before submitting them to journals and conferences, thereby accelerating the cycle of information discovery. The number of AI repository publications grew almost 27 times in the past 12 years (Figure 1.1.17).1.1 PublicationsChapter 1: Research and Development65.212010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210102030405060Number of AI Repository Publications (in Thousands)Number of AI Repository Publications, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Figure 1.1.17Table of Contents Chapter 1 Preview 41Artificial IntelligenceIndex Report 2023By RegionFigure 1.1.18 shows that North America has maintained a steady lead in the world share of AI repository publications since 2016. Since 2011, the share of repository publications from Europe and Central Asia has declined. The share represented by East Asia and the Pacific has grown significantly since 2010 and continued growing from 2020 to 2021, a period in which the year-over-year share of North American as well European and Central Asian repository publications declined.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%AI Repository Publications (% of World Total)0.34%, Sub-Saharan Africa1.80%, Latin America and the Caribbean1.81%, Rest of the World3.06%, Middle East and North Africa3.41%, South Asia17.88%, East Asia and Pacic21.40%, Europe and Central Asia23.99%, Unknown26.32%, North AmericaAI Repository Publications (% of World Total) by Region, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.18Table of Contents Chapter 1 Preview 42Artificial IntelligenceIndex Report 2023By Geographic AreaWhile the United States has held the lead in the percentage of global AI repository publications since 2016, China is catching up, while the European Union plus the United Kingdom\u2019s share continues to drop (Figure 1.1.19). In 2021, the United States accounted for 23.5% of the world\u2019s AI repository publications, followed by the European Union plus the United Kingdom (20.5%), and then China (11.9%).1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%AI Repository Publications (% of World Total)2.85%, India11.87%, China18.07%, Rest of the World20.54%, European Union and United Kingdom23.18%, Unknown23.48%, United StatesAI Repository Publications (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.19Table of Contents Chapter 1 Preview 43Artificial IntelligenceIndex Report 2023CitationsIn the citations of AI repository publications, Figure 1.1.20 shows that in 2021 the United States topped the list with 29.2% of overall citations, maintaining a dominant lead over the European Union plus the United Kingdom (21.5%), as well as China (21.0%).1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%AI Repository Citations (% of World Total)1.91%, India4.59%, Unknown20.98%, China21.52%, European Union and United Kingdom21.79%, Rest of the World29.22%, United StatesAI Repository Citations (% of World Total) by Geographic Area, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index Report Figure 1.1.20Table of Contents Chapter 1 Preview 44Artificial IntelligenceIndex Report 2023All Fields Since 2010, the institution producing the greatest number of total AI papers has been the Chinese Academy of Sciences (Figure 1.1.21). The next top four are all Chinese universities: Tsinghua University, the University of the Chinese Academy of Sciences, Shanghai Jiao Tong University, and Zhejiang University.5 The total number of publications released by each of these institutions in 2021 is displayed in Figure 1.1.22.Top Publishing InstitutionsNarrative Highlight: 5 It is important to note that many Chinese research institutions are large, centralized organizations with thousands of researchers. It is therefore not entirely surprising that, purely by the metric of publication count, they outpublish most non-Chinese institutions.1.1 PublicationsChapter 1: Research and Development2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202110987654321Rank1, Chinese Academy of Sciences2, Tsinghua University3, University of Chinese Academy of Sciences4, Shanghai Jiao Tong University5, Zhejiang University6, Harbin Institute of Technology7, Beihang University8, University of Electronic Science and Technology of China9, Peking University10, Massachusetts Institute of TechnologyTop Ten Institutions in the World in 2021 Ranked by Number of AI Publications in All Fields, 2010\u201321Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.21Table of Contents Chapter 1 Preview 45Artificial IntelligenceIndex Report 2023Top Publishing Institutions (cont\u2019d)Narrative Highlight: 1.1 PublicationsChapter 1: Research and Development1,7451,8931,9511,9702,0162,5902,7032,9043,3735,0990 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 4,500 5,000Massachusetts Institute ofTechnologyPeking UniversityUniversity of Electronic Scienceand Technology of ChinaBeihang UniversityHarbin Institute of TechnologyZhejiang UniversityShanghai Jiao Tong UniversityUniversity of Chinese Academyof SciencesTsinghua UniversityChinese Academy of SciencesNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in All Fields, 2021Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.22Table of Contents Chapter 1 Preview 46Top Publishing Institutions (cont\u2019d)Narrative Highlight: Artificial IntelligenceIndex Report 2023Computer VisionIn 2021, the top 10 institutions publishing the greatest number of AI computer vision publications were all Chinese (Figure 1.1.23). The Chinese Academy of Sciences published the largest number of such publications, with a total of 562.1.1 PublicationsChapter 1: Research and Development1822102292312472892963143165620 100 200 300 400 500Tianjin UniversityHarbin Institute of TechnologyBeijing Institute of TechnologyWuhan UniversityBeihang UniversityZhejiang UniversityTsinghua UniversityUniversity of Chinese Academyof SciencesShanghai Jiao Tong UniversityChinese Academy of SciencesNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Computer Vision, 2021Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.23Table of Contents Chapter 1 Preview 47Top Publishing Institutions (cont\u2019d)Narrative Highlight: Artificial IntelligenceIndex Report 2023Natural Language ProcessingAmerican institutions are represented to a greater degree in the share of top NLP publishers  (Figure 1.1.24). Although the Chinese Academy of Sciences was again the world\u2019s leading institution in 2021 (182 publications), Carnegie Mellon took second place (140 publications), followed by Microsoft (134). In addition, 2021 was the first year Amazon and Alibaba were represented among the top-ten largest publishing NLP institutions.1.1 PublicationsChapter 1: Research and Development981001121131161161271341401820 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190Amazon (United States)Alibaba Group (China)University of Chinese Academyof SciencesPeking UniversityGoogle (United States)Carnegie Mellon UniversityAustraliaTsinghua UniversityMicrosoft (United States)Carnegie Mellon UniversityChinese Academy of SciencesNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Natural Language Processing, 2021Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.24Table of Contents Chapter 1 Preview 48Top Publishing Institutions (cont\u2019d)Narrative Highlight: Artificial IntelligenceIndex Report 2023Speech RecognitionIn 2021, the greatest number of speech recognition papers came from the Chinese Academy of Sciences (107), followed by Microsoft (98) and Google (75) (Figure 1.1.25). The Chinese Academy of Sciences reclaimed the top spot in 2021 from Microsoft, which held first position in 2020.1.1 PublicationsChapter 1: Research and Development5455575759616675981070 10 20 30 40 50 60 70 80 90 100 110Amazon (United States)Chinese University of Hong KongTencent (China)Carnegie Mellon UniversityUniversity of Scienceand Technology of ChinaTsinghua UniversityUniversity of Chinese Academyof SciencesGoogle (United States)Microsoft (United States)Chinese Academy of SciencesNumber of AI PublicationsTop Ten Institutions in the World by Number of AI Publications in Speech Recognition, 2021Source: Center for Security and Emerging Technology, 2022 | Chart: 2023 AI Index ReportFigure 1.1.25Table of Contents Chapter 1 Preview 491112234230 2 4 6 8 10 12 14 16 18 20 22 24GamesOtherText-to-VideoSpeechVisionDrawingMultimodalLanguageNumber of Signicant Machine Learning SystemsNumber of Significant Machine Learning Systems by Domain, 2022Source: Epoch, 2022 | Chart: 2023 AI Index ReportArtificial IntelligenceIndex Report 2023General Machine  Learning SystemsThe figures below report trends among all machine learning systems included in the Epoch dataset. For reference, these systems are referred to as significant machine learning systems throughout the subsection.1.2 Trends in Significant  Machine Learning Systems System TypesAmong the significant AI machine learning systems released in 2022, the most common class of system was language (Figure 1.2.1). There were 23 significant AI language systems released in 2022, roughly six times the number of the next most common system type, multimodal systems.6 There were 38 total significant AI machine learning systems released in 2022, according to Epoch; however, one of the systems, BaGuaLu, did not have a domain classification and is therefore omitted from Figure 1.2.1.Epoch AI is a collective of researchers investigating and forecasting the development of advanced AI. Epoch curates a database of significant AI and machine learning systems that have been released since the 1950s. There are different criteria under which the Epoch team decides to include particular AI systems in their database; for example, the system may have registered a state-of-the-art improvement, been deemed to have been historically significant, or been highly cited.This subsection uses the Epoch database to track trends in significant AI and machine learning systems. The latter half of the chapter includes research done by the AI Index team that reports trends in large language and multimodal models, which are models trained on large amounts of data and adaptable to a variety of downstream applications.1.2 Trends in Significant Machine Learning Systems Chapter 1: Research and DevelopmentFigure 1.2.16Table of Contents Chapter 1 Preview 502002 2004 2006 2008 2010 2012 2014 2016 2018 2020 202205101520253035Number of Signicant Machine Learning Systems0, Nonpro t1, Industry-Academia Collaboration2, Research Collective3, Academia32, IndustryNumber of Significant Machine Learning Systems by Sector, 2002\u201322Source: Epoch, 2022 | Chart: 2023 AI Index ReportArtificial IntelligenceIndex Report 2023Sector AnalysisWhich sector among industry, academia, or nonprofit has released the greatest number of significant machine learning systems? Until 2014, most machine learning systems were released by academia. Since then, industry has taken over (Figure 1.2.2). In 2022, there were 32 significant industry-produced machine learning systems compared to just three produced by academia. Producing state-of-the-art AI systems increasingly requires large amounts of data, computing power, and money; resources that industry actors possess in greater amounts compared to nonprofits and academia.Chapter 1: Research and DevelopmentFigure 1.2.21.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 51Artificial IntelligenceIndex Report 2023National AffiliationIn order to paint a picture of AI\u2019s evolving geopolitical landscape, the AI Index research team identified the nationality of the authors who contributed to the development of each significant machine learning system in the Epoch dataset.7SystemsFigure 1.2.3 showcases the total number of significant machine learning systems attributed to researchers from particular countries.8 A researcher is considered to have belonged to the country in which their institution, for example a university or AI-research firm, was headquartered. In 2022, the United States produced the greatest number of significant machine learning systems with 16, followed by the United Kingdom (8) and China (3). Moreover, since 2002 the United States has outpaced the United Kingdom and the European Union, as well as China, in terms of the total number of significant machine learning systems produced (Figure 1.2.4). Figure 1.2.5 displays the total number of significant machine learning systems produced by country since 2002 for the entire world.Chapter 1: Research and Development7 The methodology by which the AI Index identified authors\u2019 nationality is outlined in greater detail in the Appendix.8 A machine learning system is considered to be affiliated with a particular country if at least one author involved in creating the model was affiliated with that country. Consequently, in cases where a system has authors from multiple countries, double counting may occur.111112238160 2 4 6 8 10 12 14 16SingaporeRussiaIsraelIndiaFranceGermanyCanadaChinaUnited KingdomUnited StatesNumber of Signicant Machine Learning SystemsNumber of Significant Machine Learning Systems by Country, 2022Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report20022004200620082010201220142016201820202022051015202530Number of  Significant Machine Learning Systems3, China12, European Union andUnited Kingdom16, United StatesNumber of Significant Machine Learning Systems by Select Geographic Area, 2002\u201322Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index ReportFigure 1.2.3 Figure 1.2.41.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 52Artificial IntelligenceIndex Report 2023Chapter 1: Research and Development1\u20131011\u20132021\u20136061\u2013255Number of Machine Learning Systems by Country, 2002\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report0Figure 1.2.51.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 53Artificial IntelligenceIndex Report 2023AuthorshipFigures 1.2.6 to 1.2.8 look at the total number of authors, disaggregated by national affiliation, that contributed to the launch of significant machine learning systems. As was the case with total systems, in 2022 the United States had the greatest number of authors producing significant machine learning systems, with 285, more than double that of the United Kingdom and nearly six times that of China (Figure 1.2.6).Chapter 1: Research and Development123781321491392850 50 100 150 200 250 300FranceIndiaRussiaGermanySwedenIsraelCanadaChinaUnited KingdomUnited StatesNumber of AuthorsNumber of Authors of Significant Machine Learning Systems by Country, 2022Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report20022004200620082010201220142016201820202022050100150200250300350400Number of Authors49, China155, European Union andUnited Kingdom285, United StatesNumber of Authors of Significant Machine Learning Systems by Select Geographic Area, 2002\u201322Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index ReportFigure 1.2.6 Figure 1.2.71\u20131011\u20132021\u20136061\u2013180181\u2013370371\u2013680681\u20132000Number of Authors of  Machine Learning Systems by Country, 2002\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report0Figure 1.2.81.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 54Artificial IntelligenceIndex Report 2023Parameter TrendsParameters are numerical values that are learned by machine learning models during training. The value of parameters in machine learning models determines how a model might interpret input data and make predictions. Adjusting parameters is an essential step in ensuring that the performance of a machine learning system is optimized.Figure 1.2.9 highlights the number of parameters of the machine learning systems included in the Epoch dataset by sector. Over time, there has been a steady increase in the number of parameters, an increase that has become particularly sharp since the early 2010s. The fact that AI systems are rapidly increasing their parameters is reflective of the increased complexity of the tasks they are being asked to perform, the greater availability of data, advancements in underlying hardware, and most importantly, the demonstrated performance of larger models.Chapter 1: Research and Development1950 1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+21.0e+41.0e+61.0e+81.0e+101.0e+121.0e+14Academia Industry Industry-Academia Collaboration Nonprot Research CollectiveNumber of Parameters (Log Scale)Number of Parameters of Significant Machine Learning Systems by Sector, 1950\u20132022Source: Epoch, 2022 | Chart: 2023 AI Index ReportFigure 1.2.91.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 55Artificial IntelligenceIndex Report 2023Figure 1.2.10 demonstrates the parameters of machine learning systems by domain. In recent years, there has been a rise in parameter-rich systems.Chapter 1: Research and Development1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+21.0e+41.0e+61.0e+81.0e+101.0e+12Language Vision GamesNumber of Parameters (Log Scale)Number of Parameters of Significant Machine Learning Systems by Domain, 1950\u20132022Source: Epoch, 2022 | Chart: 2023 AI Index ReportFigure 1.2.101.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 56Artificial IntelligenceIndex Report 2023Compute TrendsThe computational power, or \u201ccompute,\u201d of AI systems refers to the amount of computational resources needed to train and run a machine learning system. Typically, the more complex a system is, and the larger the dataset on which it is trained, the greater the amount of compute required.The amount of compute used by significant AI machine learning systems has increased exponentially in the last half-decade (Figure 1.2.11).9 The growing demand for compute in AI carries several important implications. For example, more compute-intensive models tend to have greater environmental impacts, and industrial players tend to have easier access to computational resources than others, such as universities.Chapter 1: Research and Development1950 1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+01.0e+31.0e+61.0e+91.0e+121.0e+151.0e+181.0e+211.0e+24Academia Industry Industry-Academia Collaboration Nonprot Research CollectiveTraining Compute (FLOP \u2013 Log Scale)Training Compute (FLOP) of Significant Machine Learning Systems by Sector, 1950\u20132022Source: Epoch, 2022 | Chart: 2023 AI Index ReportFigure 1.2.119 FLOP stands for \u201cFloating Point Operations\u201d and is a measure of the performance of a computational device.1.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 57Artificial IntelligenceIndex Report 2023Since 2010, it has increasingly been the case that of all machine learning systems, language models are demanding the most computational resources.Chapter 1: Research and Development1954 1958 1962 1966 1970 1974 1978 1982 1986 1990 1994 1998 2002 2006 2010 2014 2018 20221.0e+31.0e+61.0e+91.0e+121.0e+151.0e+181.0e+211.0e+24Language Vision GamesTraining Compute (FLOP \u2013 Log Scale)Training Compute (FLOP) of Significant Machine Learning Systems by Domain, 1950\u20132022Source: Epoch, 2022 | Chart: 2023 AI Index ReportFigure 1.2.121.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 58Artificial IntelligenceIndex Report 2023Large Language and Multimodal ModelsLarge language and multimodal models, sometimes called foundation models, are an emerging and increasingly popular type of AI model that is trained on huge amounts of data and adaptable to a variety of downstream applications. Large language and multimodal models like ChatGPT, DALL-E 2, and Make-A-Video have demonstrated impressive capabilities and are starting to be widely deployed in the real world.National AffiliationThis year the AI Index conducted an analysis of the national affiliation of the authors responsible for releasing new large language and multimodal models.10 The majority of these researchers were from American institutions (54.2%) (Figure 1.2.13). In 2022, for the first time, researchers from Canada, Germany, and India contributed to the development of large language and multimodal models.Chapter 1: Research and Development2019 2020 2021 20220%20%40%60%80%100%Authors of Large Language and Multimodal Models (% of Total)0.00%, Korea0.89%, India3.12%, Germany5.80%, Israel6.25%, Canada8.04%, China21.88%, United Kingdom54.02%, United StatesAuthors of Select Large Language and Multimodal Models (% of Total) by Country, 2019\u201322Source:  Epoch and AI Index, 2022 | Chart: 2023 AI Index ReportFigure 1.2.1310 The AI models that were considered to be large language and multimodal models were hand-selected by the AI Index steering committee. It is possible that this selection may have omitted certain models.Figure 1.2.14 offers a timeline view of the large language and multimodal models that have been released since GPT-2, along with the national affiliations of the researchers who produced the models. Some of the notable American large language and multimodal models released in 2022 included OpenAI\u2019s DALL-E 2 and Google\u2019s PaLM (540B). The only Chinese large language and multimodal model released in 2022 was GLM-130B, an impressive bilingual (English and Chinese) model created by researchers at Tsinghua University. BLOOM, also launched in late 2022, was listed as indeterminate given that it was the result of a collaboration of more than 1,000 international researchers.1.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 59Artificial IntelligenceIndex Report 2023Chapter 1: Research and Development2019-Jan2019-Apr2019-Jul2019-Oct2020-Jan2020-Apr2020-Jul2020-Oct2021-Jan2021-Apr2021-Jul2021-Oct2022-Jan2022-Apr2022-Jul2022-Oct2023-JanGPT-2Grover-MegaMegatron-LM (Original, 8.3B)T5-3BT5-11BMeenaTuring NLGGPT-3 175B (davinci)ERNIE-GEN (large)DALL-EWu Dao - Wen YuanGPT-NeoPanGu-alphaGPT-J-6BHyperClovaCogViewWu Dao 2.0ERNIE 3.0CodexJurassic-1-JumboMegatron-Turing NLG 530BGopherInstructGPTAlphaCodeGPT-NeoX-20BChinchillaPaLM (540B)DALL\u00b7E 2Stable Diffusion (LDM-KL-8-G)OPT-175BJurassic-XImagenMinerva (540B)GLM-130BBLOOMSource: AI Index, 2022 | Chart: 2023 AI Index ReportUnited StatesUnited KingdomChinaUnited States,United Kingdom,Germany, IndiaKoreaCanadaIsraelGermanyIndeterminateTimeline and National Affiliation of Select Large Language and Multimodal Model ReleasesFigure 1.2.141111 While we were conducting the analysis to produce Figure 1.2.14, Irene Solaiman published a paper that has a similar analysis. We were not aware of the paper at the time of our research.1.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 60Artificial IntelligenceIndex Report 2023Parameter CountOver time, the number of parameters of newly released large language and multimodal models has massively increased. For example, GPT-2, which was the first large language and multimodal model released in 2019, only had 1.5 billion parameters. PaLM, launched by Google in 2022, had 540 billion, nearly 360 times more than GPT-2. The median number of parameters in large language and multimodal models is increasing exponentially over time (Figure 1.2.15).Chapter 1: Research and DevelopmentGPT-2Grover-MegaMegatron-LM (Original, 8.3B)T5-3BT5-11BMeenaTuring NLGGPT-3 175B (davinci)ERNIE-GEN (large)DALL-EWu Dao - Wen YuanGPT-NeoPanGu-\u03b1GPT-J-6BHyperClovaCogViewWu Dao 2.0ERNIE 3.0CodexJurassic-1-JumboMegatron-Turing NLG 530BGopherGPT-NeoX-20BChinchillaPaLM (540B)DALL\u00b7E 2Stable Diusion (LDM-KL-8-G)OPT-175BJurassic-XMinerva (540B)GLM-130BBLOOM2019-Feb2019-May2019-Sep2019-Oct2020-Jan2020-Feb2020-May2020-Aug2021-Jan2021-Mar2021-Apr2021-May2021-Jun2021-Jul2021-Aug2021-Oct2021-Dec2022-Feb2022-Mar2022-Apr2022-May2022-Jun2022-Aug2022-Nov3.2e+81.0e+93.2e+91.0e+103.2e+101.0e+113.2e+111.0e+123.2e+12Number of Parameters (Log Scale)Number of Parameters of Select Large Language and Multimodal Models, 2019\u201322Source: Epoch, 2022 | Chart: 2023 AI Index ReportFigure 1.2.151.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 61Artificial IntelligenceIndex Report 2023Training ComputeThe training compute of large language and multimodal models has also steadily increased (Figure 1.2.16). The compute used to train Minerva (540B), a large language and multimodal model released by Google in June 2022 that displayed impressive abilities on quantitative reasoning problems, was roughly nine times greater than that used for OpenAI\u2019s GPT-3, which was released in June 2022, and roughly 1839 times greater than that used for GPT-2 (released February 2019).Chapter 1: Research and DevelopmentGPT-2Megatron-LM (Original, 8.3B)T5-3BT5-11BMeenaTuring NLGGPT-3 175B (davinci)DALL-EWu Dao - Wen YuanGPT-NeoPanGu-\u03b1GPT-J-6BHyperClovaCogViewERNIE 3.0Jurassic-1-JumboMegatron-Turing NLG 530BGopherAlphaCodePaLM (540B)ChinchillaOPT-175BMinerva (540B)GLM-130BBLOOM2019-Feb2019-Sep2019-Oct2020-Jan2020-Feb2020-May2021-Jan2021-Mar2021-Apr2021-May2021-Jul2021-Aug2021-Oct2021-Dec2022-Feb2022-Mar2022-Apr2022-May2022-Jun2022-Aug2022-Nov1.0e+183.2e+181.0e+193.2e+191.0e+203.2e+201.0e+213.2e+211.0e+223.2e+221.0e+233.2e+231.0e+243.2e+24Training Compute (FLOP \u2013 Log Scale)Training Compute (FLOP) of Select Large Language and Multimodal Models, 2019\u201322Source: Epoch, 2022 | Chart: 2023 AI Index ReportStable Diffusion GPT-NeoX-20BFigure 1.2.161.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 62Artificial IntelligenceIndex Report 2023Training CostA particular theme of the discourse around large language and multimodal models has to do with their hypothesized costs. Although AI companies rarely speak openly about training costs, it is widely speculated that these models cost millions of dollars to train and will become increasingly expensive with scale.This subsection presents novel analysis in which the AI Index research team generated estimates for the training costs of various large language and multimodal models (Figure 1.2.17). These estimates are based on the hardware and training time disclosed by the models\u2019 authors. In cases where training time was not disclosed, we calculated from hardware speed, training compute, and hardware utilization efficiency. Given the possible variability of the estimates, we have qualified each estimate with the tag of mid, high, or low: mid where the estimate is thought to be a mid-level estimate, high where it is thought to be an overestimate, and low where it is thought to be an underestimate. In certain cases, there was not enough data to estimate the training cost of particular large language and multimodal models, therefore these models were omitted from our analysis.The AI Index estimates validate popular claims that large language and multimodal models are increasingly costing millions of dollars to train. For example, Chinchilla, a large language model launched by DeepMind in May 2022, is estimated to have cost $2.1 million, while BLOOM\u2019s training is thought to have cost $2.3 million.Chapter 1: Research and DevelopmentFigure 1.2.170.051.971.470.111.800.23 0.02 0.09 0.43 0.27 0.01 0.1411.358.550.09 0.242.118.010.601.691.030.162.29GPT-2T5-11BMeenaTuring NLGGPT-3 175BDALL-EWu Dao - Wen YuanGPT-NeoGPT-J-6BHyperClovaERNIE 3.0CodexMegatron-Turing NLG 530BGopherAlphaCodeGPT-NeoX-20BChinchillaPaLM (540B)Stable Diusion (LDM-KL-8-G)OPT-175BMinerva (540B)GLM-130BBLOOM2019 2020 2021 2022024681012Mid High LowEstimated Training Cost of Select Large Language and Multimodal ModelsSource: AI Index, 2022 | Chart: 2023 AI Index ReportTraining Cost(in Millions of U.S. Dollars)12 See Appendix for the complete methodology behind the cost estimates.1.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 63Artificial IntelligenceIndex Report 2023There is also a clear relationship between the cost of large language and multimodal models and their size. As evidenced in Figures 1.2.18 and 1.2.19, the large language and multimodal models with a greater number of parameters and that train using larger amounts of compute tend to be more expensive.Chapter 1: Research and DevelopmentFigure 1.2.18 Figure 1.2.19BLOOMGLM-130BMinerva (540B)OPT-175BStable DiusionPaLM (540B)ChinchillaGPT-NeoX-20BAlphaCodeGopherMegatron-Turing NLG 530BCodexERNIE 3.0HyperClovaGPT-J-6BGPT-NeoWu Dao - Wen YuanDALL-EGPT-3 175BTuring NLGMeenaT5-11BGPT-210k 100k 1M 10M1.0e+92.0e+95.0e+91.0e+102.0e+105.0e+101.0e+112.0e+115.0e+11Training Cost (in U.S. Dollars - Log Scale)Number of Parameters (Log Scale)Estimated Training Cost of Select Large Languageand Multimodal Models and Number of ParametersSource: AI Index, 2022 | Chart: 2023 AI Index ReportBLOOMGLM-130BMinerva (540B)OPT-175BChinchillaGPT-NeoX-20BAlphaCodeGopherPaLM (540B)Megatron-Turing NLG 530BERNIE 3.0Stable DiffusionGPT-J-6BGPT-NeoWu Dao - Wen YuanDALL-ETuring NLGMeenaT5-11BGPT-210k 100k 1M 10M1.0e+181.0e+201.0e+221.0e+24Training Cost (in U.S. Dollars - Log Scale)Training Compute (FLOP \u2013 Log Scale)Estimated Training Cost of Select Large Language and Multimodal Models and Training Compute (FLOP)Source: AI Index, 2022 | Chart: 2023 AI Index Report1.2 Trends in Significant Machine Learning Systems Table of Contents Chapter 1 Preview 642010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220102030405060708090Number of Attendees (in Thousands)59.45Number of Attendees at Select AI Conferences, 2010\u201322 Source: AI Index, 2022 | Chart: 2023 AI Index ReportArtificial IntelligenceIndex Report 2023Conference AttendanceAfter a period of increasing attendance, the total attendance at the conferences for which the AI Index collected data dipped in 2021 and again in 2022 (Figure 1.3.1).13 This decline may be attributed to the fact that many conferences returned to hybrid or in-person formats after being fully virtual in 2020 and 2021. For example, the International Joint Conference on Artificial Intelligence (IJCAI) and the 1.3 AI ConferencesInternational Conference on Principles of Knowledge Representation and Reasoning (KR) were both held strictly in-person.Neural Information Processing Systems (NeurIPS) continued to be one of the most attended conferences, with around 15,530 attendees (Figure 1.3.2).14 The conference with the greatest one-year increase in attendance was the International Conference on Robotics and Automation (ICRA), from 1,000 in 2021 to 8,008 in 2022.13 This data should be interpreted with caution given that many conferences in the last few years have had virtual or hybrid formats. Conference organizers report that measuring the exact attendance numbers at virtual conferences is difficult, as virtual conferences allow for higher attendance of researchers from around the world.14 In 2021, 9,560 of the attendees attended NeurIPS in-person and 5,970 remotely.AI conferences are key venues for researchers to share their work and connect with peers and collaborators. Conference attendance is an indication of broader industrial and academic interest in a scientific field. In the past 20 years, AI conferences have grown in size, number, and prestige. This section presents data on the trends in attendance at major AI conferences.1.3 AI ConferencesChapter 1: Research and DevelopmentFigure 1.3.1Table of Contents Chapter 1 Preview 652010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022051015202530Number of Attendees (in Thousands)3.56, AAAI4.32, IROS5.35, ICLR7.73, ICML8.01, ICRA10.17, CVPR15.53, NeurIPSAttendance at Large Conferences, 2010\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.000.501.001.502.002.503.003.50Number of Attendees (in Thousands)0.12, KR0.39, ICAPS0.50, AAMAS0.66, UAI1.09, FaccT2.01, IJCAIAttendance at Small Conferences, 2010\u201322Source: AI Index, 2022 | Chart: 2023 AI Index ReportArtificial IntelligenceIndex Report 2023 1.3 AI ConferencesChapter 1: Research and DevelopmentFigure 1.3.2Figure 1.3.3Table of Contents Chapter 1 Preview 662011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022050100150200250300350Number of AI Projects (in Thousands)348Number of GitHub AI Projects, 2011\u201322Source: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial IntelligenceIndex Report 2023ProjectsA GitHub project is a collection of files that can include the source code, documentation, configuration files, and images that constitute a 1.4 Open-Source AI Softwaresoftware project. Since 2011, the total number of AI-related GitHub projects has steadily increased, growing from 1,536 in 2011 to 347,934 in 2022.GitHub is a web-based platform where individuals and coding teams can host, review, and collaborate on various code repositories. GitHub is used extensively by software developers to manage and share code, collaborate on various projects, and support open-source software. This subsection uses data provided by GitHub and the OECD.AI policy observatory. These trends can serve as a proxy for some of the broader trends occuring in the world of open-source AI software not captured by academic publication data.1.4 Open-Source AI SoftwareChapter 1: Research and DevelopmentFigure 1.4.1Table of Contents Chapter 1 Preview 672011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220%5%10%15%20%25%30%35%40%AI Projects (% of Total)2.40%, China14.00%, United States17.30%, European Union and United Kingdom24.19%, India42.11%, Rest of the WorldGitHub AI Projects (% Total) by Geographic Area, 2011\u201322Source: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial IntelligenceIndex Report 2023As of 2022, a large proportion of GitHub AI projects were contributed by software developers in India (24.2%) (Figure 1.4.2). The next most represented geographic area was the European Union and the United Kingdom (17.3%), and then the United States (14.0%). The share of American GitHub AI projects has been declining steadily since 2016.1.4 Open-Source AI SoftwareChapter 1: Research and DevelopmentFigure 1.4.2Table of Contents Chapter 1 Preview 682011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.000.501.001.502.002.503.003.50Number of Cumulative GitHub Stars (in Millions)0.46, India1.53, China2.34, European Union and United Kingdom2.69, Rest of the World3.44, United StatesNumber of GitHub Stars by Geographic Area, 2011\u201322Source: GitHub, 2022; OECD.AI, 2022 | Chart: 2023 AI Index Report Artificial IntelligenceIndex Report 2023StarsGitHub users can bookmark or save a repository of interest by \u201cstarring\u201d it. A GitHub star is similar to a \u201clike\u201d on a social media platform and indicates support for a particular open-source project. Some of the most starred GitHub repositories include libraries like TensorFlow, OpenCV, Keras, and PyTorch, which are widely used by software developers in the AI coding community.Figure 1.4.3 shows the cumulative number of stars attributed to projects belonging to owners of various geographic areas. As of 2022, GitHub AI projects from the United States received the most stars, followed by the European Union and the United Kingdom, and then China. In many geographic areas, the total number of new GitHub stars has leveled off in the last few years.1.4 Open-Source AI SoftwareChapter 1: Research and DevelopmentFigure 1.4.3Artificial IntelligenceIndex Report 2023CHAPTER 2: Technical PerformanceTable of Contents Chapter 2 Preview 70Artificial IntelligenceIndex Report 2023Technical PerformanceCHAPTER 2 PREVIEW:Overview 72Chapter Highlights 732.1 What\u2019s New in 2022: A Timeline 742.2 Computer Vision\u2014Image 81Image Classification 81 ImageNet 81Face Detection and Recognition 82 National Institute of Standards and   Technology Face Recognition   Vendor Test (FRVT) 83Deepfake Detection 84 Celeb-DF 84Human Pose Estimation 85 MPII 85Semantic Segmentation 86 Cityscapes Challenge, Pixel-Level   Semantic Labeling Task 86Medical Image Segmentation 87 Kvasir-SEG 87Object Detection 88 Common Objects in Context (COCO) 88Image Generation 89 CIFAR-10 and STL-10 89  Narrative Highlight:  A Closer Look at   Progress in Image Generation 90Visual Reasoning 92 Visual Question Answering (VQA)   Challenge 92  Narrative Highlight:  The Rise of Capable  Multimodal Reasoning Systems 93 Visual Commonsense Reasoning (VCR) 952.3 Computer Vision\u2014Video 96Activity Recognition 96 Kinetics-400, Kinetics-600, Kinetics-700 96  Narrative Highlight:  A Closer Look   at the Progress of Video Generation 982.4 Language 99English Language Understanding 99 SuperGLUE 99 Reading Comprehension Dataset   Requiring Logical Reasoning (ReClor) 100  Narrative Highlight:  Just How Much   Better Have Language Models Become? 102  Narrative Highlight:  Planning and   Reasoning in Large Language Models 103Text Summarization 104 arXiv and PubMed 104Table of Contents Chapter 2 Preview 71Artificial IntelligenceIndex Report 2023Technical PerformanceCHAPTER 2 PREVIEW (CONT\u2019D):Natural Language Inference 105 Abductive Natural Language   Inference (aNLI) 105Sentiment Analysis 106 SST-5 Fine-Grained Classification 106Multitask Language Understanding 107 Massive Multitask Language   Understanding (MMLU) 107Machine Translation (MT) 108 Number of Commercially Available   MT Systems 1082.5 Speech 109Speech Recognition 109 VoxCeleb 109  Narrative Highlight:    Whisper 1102.6 Reinforcement Learning 112Reinforcement Learning Environments 112 Procgen 112  Narrative Highlight:    Benchmark Saturation 1142.7 Hardware 115MLPerf Training Time 115MLPerf Inference 117Trends in GPUs 1182.8 Environment 120Environmental Impact of  Select Large Language Models 120  Narrative Highlight:  Using AI to   Optimize Energy Usage 1222.9 AI for Science 123Accelerating Fusion Science Through  Learned Plasma Control 123Discovering Novel Algorithms for  Matrix Manipulation With AlphaTensor 123Designing Arithmetic Circuits With  Deep Reinforcement Learning 124Unlocking de Novo Antibody Design  With Generative AI 124ACCESS THE PUBLIC DATATable of Contents Chapter 2 Preview 72Artificial IntelligenceIndex Report 2023OverviewThis year\u2019s technical performance chapter features analysis of the technical progress in AI during 2022. Building on previous reports, this chapter chronicles advancement in computer vision, language, speech, reinforcement learning, and hardware. Moreover, this year this chapter features an analysis on the environmental impact of AI, a discussion of the ways in which AI has furthered scientific progress, and a timeline-style overview of some of the most significant recent AI developments.Chapter 2: Technical PerformanceTable of Contents Chapter 2 Preview 73Artificial IntelligenceIndex Report 2023Chapter HighlightsPerformance saturation on traditional benchmarks.AI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing. However, new, more comprehensive benchmarking suites such as BIG-bench and HELM are being released.Generative AI breaks into  the public consciousness.  2022 saw the release of text-to-image models like DALL-E 2 and Stable Diffusion, text-to-video systems like Make-A-Video, and chatbots like ChatGPT. Still, these systems can be prone to hallucination, confidently outputting incoherent or untrue responses, making it hard to rely on them for critical applications.AI systems become  more flexible. Traditionally AI systems have performed well on narrow tasks but have struggled across broader tasks. Recently released models challenge that trend; BEiT-3, PaLI, and Gato, among others, are single AI systems increasingly capable of navigating multiple tasks (for example, vision, language).AI is both helping and harming the environment.  New research suggests that AI systems can have serious environmental impacts. According to Luccioni et al., 2022, BLOOM\u2019s training run emitted 25 times more carbon than a single air traveler on a one-way trip from New York to San Francisco. Still, new reinforcement learning models like BCOOLER show that AI systems can be used to optimize energy usage.Chapter 2: Technical PerformanceCapable language models still struggle with reasoning. Language models continued to improve their generative capabilities, but new research suggests that they still struggle with complex planning tasks.The world\u2019s best new scientist \u2026 AI? AI models are starting to rapidly accelerate scientific progress and in 2022 were used to aid hydrogen fusion, improve the efficiency of matrix manipulation, and generate new antibodies.AI starts to build better AI. Nvidia used an AI reinforcement learning agent to improve the design of the chips that power AI systems. Similarly, Google recently used one of its language models, PaLM, to suggest ways to improve the very same model. Self-improving AI learning will accelerate AI progress.Table of Contents Chapter 2 Preview 74Artificial IntelligenceIndex Report 2023DeepMind Releases AlphaCode AlphaCode, an AI system that writes computer programs at a competitive level, achieves a rank within the top 54% of participants in a human programming competition. This represents an improvement on the more complex problem-solving tasks with which AI has traditionally struggled.DeepMind Trains Reinforcement Learning Agent to Control Nuclear Fusion Plasma in a TokamakNuclear fusion is a potential source of clean, limitless energy, but producing such energy in tokamaks is difficult due to a lack of experimental data. DeepMind simulated optimal tokamak management, an example of how AI can accelerate science and combat climate change.IndicNLG Benchmarks Natural Language Generation for Indic LanguagesAn international research collective launches IndicNLG, a collection of datasets for benchmarking natural language generation for 11 Indic languages. The creation of IndicNLG increases the potential for AI systems to generate language in more diverse, non-English linguistic settings.2.1 What\u2019s New in 2022: A Timeline2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.1Figure 2.1.2Figure 2.1.3The technical performance chapter begins with an overview of some of the most significant technical developments in AI during 2022, as selected by the AI Index Steering Committee.Feb. 2, 2022Feb. 16, 2022March 10, 2022Table of Contents Chapter 2 Preview 75Artificial IntelligenceIndex Report 2023Meta AI Releases Make-A-Scene Make-A-Scene is a text-to-image AI model that enables users to generate images through text. Make-A-Scene is one of many text-to-image models released in 2022.Google Releases PaLMGoogle\u2019s AI team trains one of the world\u2019s largest language models, PaLM. Made up of 540 billion parameters, PaLM reinforces the belief that researchers can improve performance on large language models by simply training them on more data.OpenAI Releases DALL-E 2DALL-E 2, a text-to-image AI system that can create realistic art and images from textual descriptions, is released to the public, igniting a generative AI craze.DeepMind Launches GatoGato is a new reinforcement learning agent capable of doing a wide range of tasks such as robotic manipulation, game playing, image captioning, and natural language generation.  The release of such models suggests that AI systems are becoming better at generalization.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.4Figure 2.1.5Figure 2.1.6Figure 2.1.7March 24, 2022April 5, 2022April 13, 2022May 12, 2022Table of Contents Chapter 2 Preview 76Artificial IntelligenceIndex Report 2023Google Releases ImagenImagen is a text-to-image diffusion model capable of producing images with a high degree of photorealism. Imagen\u2019s launch also comes with the release of DrawBench, a challenging new benchmark for text-to-image systems.442 Authors Across 132 Institutions Team Up to Launch BIG-benchIn order to better challenge increasingly capable large language models, a team of 442 authors across 132 institutions launch the Beyond the Imitation Game benchmark (BIG-bench). The benchmark consists of 204 tasks ranging from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, and software development.GitHub Makes Copilot Available as a Subscription-Based Service for Individual DevelopersCopilot is a generative AI system capable of turning natural language prompts into coding suggestions across multiple languages. Similar systems include OpenAI\u2019s Codex and Salesforce\u2019s CodeGen. Surveys suggest that Copilot makes coders more productive and less frustrated.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.8Figure 2.1.9Figure 2.1.10May 23, 2022June 9, 2022June 21, 2022Table of Contents Chapter 2 Preview 77Artificial IntelligenceIndex Report 2023Nvidia Uses Reinforcement Learning to Design Better-Performing GPUsNvidia uses its AI systems to improve the performance of its latest H100 class of GPU chips. GPUs being essential to AI training, this is one example of how AI is starting to develop better AI.Meta Announces  \u2018No Language Left Behind\u2019No Language Left Behind (NLLB) is a family of models that can translate across 200 distinct languages. NLLB is one of the first systems that can perform well across a wide range of low-resource languages like Kamba and Lao.Tsinghua Researchers Launch GLM-130BChinese researchers affiliated with Tsinghua University release GLM-130B, a large language model that outperforms others such as Meta\u2019s OPT, Hugging Face\u2019s BLOOM, and OpenAI\u2019s original GPT-3.Stability AI Releases Stable DiffusionStable Diffusion is an open-source text-to-image diffusion-based model, meaning users can freely use the model weights to generate their own images. Stable Diffusion is trained on existing images created by humans and gives no credit or acknowledgment, leaving open questions around the ethical use of image generators.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.11Figure 2.1.12Figure 2.1.13Figure 2.1.14July 8, 2022July 11, 2022Aug 4, 2022Aug 22, 2022Table of Contents Chapter 2 Preview 78Artificial IntelligenceIndex Report 2023OpenAI Launches WhisperWhisper is a large-scale speech-recognition system trained on roughly 700,000 hours of audio data and capable of respectable performance on various speech recognition tasks. The fact that Whisper required neither supervised pre-training nor unsupervised training with fine-tuning yet was able to achieve strong performance by merely increasing training data further validates the approach of increasingly scaling AI models.Meta Releases Make-A-VideoMake-A-Video is a system that allows users to create videos from short text descriptions. The quality of the videos is high and again demonstrates the validity of the scaling approach.DeepMind Launches AlphaTensorAlphaTensor is an AI reinforcement-learning-based system able to discover new and efficient algorithms for matrix manipulation. Matrix manipulation is essential to a wide range of digital practices and is a process that researchers have been trying to make more efficient for decades.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.15Figure 2.1.16Figure 2.1.17Sept 21, 2022Sept 29, 2022Oct 5, 2022Table of Contents Chapter 2 Preview 79Artificial IntelligenceIndex Report 2023Google Uses PaLM to Improve the Reasoning of PaLMGoogle researchers use one of their existing language models, PaLM, to improve the reasoning of the very same model. This process is yet another example of AI systems using their own knowledge to improve.International Research Group Releases BLOOMA collaboration of over 100 researchers from across the globe develop an open-access language model called BLOOM. BLOOM impresses with its public release and for furthering the possibilities of international collaboration in AI research.Stanford Researchers Release HELM As part of an attempt to judge new language models according to more unified standards, Stanford researchers develop a new benchmarking approach for large language models called Holistic Evaluation of Language Models (HELM). The launch of HELM is evidence of the AI community\u2019s attempt to develop transparency around increasingly powerful, capable, and influential large language models.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.18Figure 2.1.19Figure 2.1.20Oct 20, 2022Nov 9, 2022Nov 16, 2022Table of Contents Chapter 2 Preview 80Artificial IntelligenceIndex Report 2023Meta Releases CICEROCICERO is the first AI to play in the top 10% of human participants in the game Diplomacy. CICERO\u2019s launch shows that AI systems have improved in strategic reasoning, a domain in which they have traditionally struggled, and are capable of effectively convincing humans to go along with their objectives.OpenAI Launches ChatGPTChatGPT is an impressive, publicly usable chatbot capable of writing university-level essays. Months after launching, ChatGPT reaches 100 million monthly active users, making it the fastest-growing consumer application in history. ChatGPT\u2019s release caps a year in which generative AI became a part of the zeitgeist, and raises questions about the effect that AI will have on the future of humanity.2.1 What\u2019s New in 2022: A TimelineChapter 2: Technical PerformanceFigure 2.1.21Figure 2.1.22Nov 22, 2022Nov 30, 2022Table of Contents Chapter 2 Preview 81Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceImage Classification Image classification is the ability of machines to categorize objects in images (Figure 2.2.1).ImageNetImageNet is one of the most widely used benchmarks for image classification. This dataset includes over 14 million images across 20,000 different object categories such as \u201cstrawberry\u201d or \u201cballoon.\u201d Performance on ImageNet is measured through various accuracy metrics. Top-1 accuracy measures the degree to which the top prediction generated by an image classification model for a given image actually matches the image\u2019s label.As of 2022, the best image classification system on ImageNet has a top-1 accuracy rate of 91.0% (Figure 2.2.2). Although the current image classification capabilities of state-of-the-art systems is 27.7 percentage points better than a decade ago, last year saw a very marginal 0.1 percentage point improvement in classification accuracy.2.2 Computer Vision\u2014ImageComputer vision is the subfield of AI that teaches machines to understand images and videos. Computer vision technologies have a variety of important real-world applications, such as autonomous driving, crowd surveillance, sports analytics, and video-game creation.This section tracks progress in computer vision across several different task domains which include: (1) image classification, (2) face detection and recognition, (3) deepfake detection, (4) human pose estimation, (5) semantic segmentation, (6) medical image segmentation, (7) object detection, (8) image generation, and (9) visual reasoning.Figure 2.2.1A Demonstration of Image ClassificationSource: Krizhevsky et al., 2012Table of Contents Chapter 2 Preview 82Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceFace Detection and Recognition Facial detection and recognition is the ability of AI systems to identify faces or individuals in images or videos (Figure 2.2.3). Currently, many facial recognition systems are able to successfully identify close to 100% of faces, even on challenging datasets (Figure 2.2.4).Figure  2.2.3Figure 2.2.22012 2013 2014 2015 2016 2017 2018 2019 2020 2021 202265%70%75%80%85%90%Top-1 Accuracy (%)88.50%, Without Extra Training Data91.00%, With Extra Training DataImageNet Challenge: Top-1 AccuracySource: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index ReportA Demonstration of Face Detection and RecognitionSource: Forbes, 2020Table of Contents Chapter 2 Preview 83Artificial IntelligenceIndex Report 20232017 2018 2019 2020 2021 20220.00050.00100.00200.00500.01000.02000.05000.10000.20000.50001.0000False Non-Match Rate: FMNR (Log-Scale)0.0006, VISA Photos @ FMR = 1e-60.0016, VISABORDER Photos @ FMR = 1e-60.0019, MUGSHOT Photos \u2265 12 YRS @ FMR = 1e-50.0021, MUGSHOT Photos @ FMR = 1e-50.0032, BORDER Photos @ FMR = 1e-60.0297, WILD Photos @ FMR = 1e-5National Institute of Standards and Technology (NIST) Face Recognition Vendor Test (FRVT):Source: National Institute of Standards and Technology, 2022 | Chart: 2023 AI Index ReportVerication Accuracy by Dataset2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceNational Institute of Standards and Technology Face Recognition Vendor Test (FRVT)Progress on facial recognition can be tracked through the National Institute of Standards and Technology\u2019s Face Recognition Vendor Test. This test tracks how well different facial recognition algorithms perform on various homeland security tasks, such as identification of child trafficking victims and cross-verification of visa images, among others. Facial detection capacity is measured by the false non-match rate (FNMR), otherwise known as error rate, which is the rate at which a model fails to match the face in an image to that of a person.As of 2022, the top-performing models on all of the FRVT datasets, with the exception of WILD Photos, each posted an error rate below 1%, and as low as a 0.06% error rate on the VISA Photos dataset.Figure 2.2.4Table of Contents Chapter 2 Preview 84Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceDeepfake DetectionThe ability of AI systems to create synthetic images that are sometimes indistinguishable from real ones has led to the creation of deepfakes, images or videos that appear to be real but are actually fake. In the last year, there was a widely circulated deepfake video of Ukrainian president Volodymyr Zelenskyy surrendering (Figure 2.2.5).Celeb-DFCeleb-DF is presently one of the most challenging deepfake detection benchmarks. This dataset is composed of 590 original celebrity YouTube videos that have been manipulated into thousands of deepfakes. This year\u2019s top deepfake detection algorithm on Celeb-DF came from researchers at Deakin University in Australia. Their JDFD model posted an AUC score of 78 (Figure 2.2.6).Figure 2.2.52018 2019 2020 2021 2022657075Area Under Curve Score (AUC)78.00Celeb-DF: Area Under Curve Score (AUC)Source: arXiv, 2022 | Chart: 2023 AI Index ReportFigure 2.2.6Real-Life Deepfake: President Zelenskyy Calling for the Surrender of Ukrainian SoldiersSource: NPR, 2022Table of Contents Chapter 2 Preview 85Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceHuman Pose EstimationHuman pose estimation is the task of estimating the position of the human body from images (Figure 2.2.7).MPIIMPII is a dataset of over 25,000 annotated images which contains annotations of more than 40,000 people doing 410 human activities. On MPII, this year\u2019s top model, ViTPose, correctly estimated 94.3% of keypoints (human joints), which represented a small 0.2 percentage point increase from the previous state-of-the-art result posted in 2020 (Figure 2.2.8).Figure 2.2.72014 2015 2016 2017 2018 2019 2020 2021 202285%90%95%Percentage of Correct Keypoints (PCK)94.30%MPII: Percentage of Correct Keypoints (PCK)Source: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index ReportFigure 2.2.8A Demonstration of Human Pose EstimationSource: Cong et al., 2022Table of Contents Chapter 2 Preview 86Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceSemantic SegmentationSemantic segmentation involves assigning individual image pixels to specific categories (for example, human, bicycle, or street) (Figure 2.2.9).Cityscapes Challenge,  Pixel-Level Semantic Labeling TaskThe Cityscapes dataset is used to test the semantic segmentation capabilities of AI. This dataset contains 25,000 annotated images of diverse urban environments. The Cityscapes dataset enables a variety of different segmentation tasks. One of the most popular is the pixel-level task. Performance on semantic segmentation is measured by mean intersection-over-union (mIoU), which represents the degree to which the image segments predicted by the model overlap with the image\u2019s actual segments. The greater the mIoU, the better a system has performed.Performance on Cityscapes has increased by 23.4 percentage points since the competition launched in 2014; however, it has plateaued in the last few years (Figure 2.2.10).Figure 2.2.92014 2015 2016 2017 2018 2019 2020 2021 202265%70%75%80%85%Mean Intersection-Over-Union (mIoU)86.46%, With Extra Training Data84.30%, Without Extra Training DataCityscapes Challenge, Pixel-Level Semantic Labeling Task: Mean Intersection-Over-Union (mIoU)Source: Cityscapes Challenge, 2022 | Chart: 2023 AI Index ReportFigure 2.2.10A Demonstration of Semantic SegmentationSource: Cityscapes Dataset, 2022Table of Contents Chapter 2 Preview 87Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceMedical Image SegmentationIn medical image segmentation, AI systems segment objects such as lesions or organs in medical images (Figure 2.2.11). Kvasir-SEGKvasir-SEG is a dataset for medical image segmentation that contains 1,000 high-quality images of gastrointestinal polyps that were manually identified by medical professionals. Progress on Kvasir-SEG is measured in mean Dice, which represents the degree to which the polyp segments identified by AI systems overlap with the actual polyp segments.1This year\u2019s top-performing model on Kvasir-SEG, SEP, was created by a Chinese researcher and posted a mean Dice of 94.1% (Figure 2.2.12).Figure 2.2.112019 2020 2021 202285%90%Mean Dice94.11%Kvasir-SEG: Mean DiceSource: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index ReportFigure 2.2.121  Mean Dice and mIoU are in principle quite similar. This StackExchange post outlines the differences in more detail.A Demonstration of Medical Imaging SegmentationSource: Jha et al., 2019Table of Contents Chapter 2 Preview 88Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceObject DetectionThe challenge of identifying and localizing objects within an image or video is known as object detection (Figure 2.2.13).Common Objects in Context (COCO)Microsoft\u2019s Common Objects in Context (COCO) object detection dataset has over 80 object categories in 328,000 images. Several accuracy metrics are used to measure progress on COCO. This section considers mean average precision (mAP50).Since 2015, state-of-the-art detectors have improved by 26 percentage points. The top model in 2022, EVA, was the result of a Chinese academic research collaboration.Figure 2.2.132015 2016 2017 2018 2019 2020 2021 202260%70%80%Mean Average Precision (mAP50)81.90%COCO: Mean Average Precision (mAP50)Source: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index ReportFigure 2.2.14A Demonstration of Object DetectionSource: Rizzoli, 2023Table of Contents Chapter 2 Preview 89Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceImage GenerationImage generation is the task of generating images that are indistinguishable from real ones. In the last decade, progress on image generation has tremendously increased, so much so that now it would be difficult for the average person to distinguish a real human face from one synthetically generated by AI (Figure 2.2.15).CIFAR-10 and STL-10CIFAR-10 and STL-10 are two popular benchmarks for tracking progress on image generation. CIFAR-10 comprises 60,000 color images across 10 different object classes; STL-10 is inspired by CIFAR-10, with some modifications, including fewer labeled training examples and more unlabeled examples. Progress on image generation in both benchmarks is measured by the Fr\u00e9chet Inception Distance (FID) score, which reflects the degree to which a synthetically generated Figure 2.2.152017 2018 2019 2020 2021 202205101520253035Fr\u00e9chet Inception Distance (FID) Score1.77, CIFAR-106.91, STL-10CIFAR-10 and STL-10: Fr\u00e9chet Inception Distance (FID) ScoreSource: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index ReportFigure 2.2.16set of images is similar to the real images on which it was trained.This year saw state-of-the-art results on both CIFAR-10 and STL-10 benchmarks (Figure 2.2.15). The top model on CIFAR-10, EDM-G++, came from Korean researchers at KAIST. The top model on STL-10 was Diffusion-GAN, a collaboration between researchers at the University of Texas at Austin and Microsoft.Which Face Is Real?Source: Which Face Is Real?, 2022Table of Contents Chapter 2 Preview 90Artificial IntelligenceIndex Report 2023 2.1 Computer Vision\u2013ImageChapter 2: Technical PerformanceA Closer Look at Progress in Image GenerationFigure 2.2.17 tracks the progress of facial image generation over time, with the final image being generated by Diffusion-GAN, the model that posted the 2022 state-of-the-art score on STL-10.In the last year, text-to-image generation broke into the public consciousness with the release of models such as OpenAI\u2019s DALL-E 2, Stability AI\u2019s Stable Diffusion, Midjourney\u2019s Midjourney, Meta\u2019s Make-A-Scene, and Google\u2019s Imagen. With these systems, users can generate images based on a text prompt. Figure 2.2.18 juxtaposes the images generated by DALL-E 2, Stable Diffusion, and Midjourney, three publicly accessible AI text-to-image systems, for the same prompt: \u201ca panda playing a piano on a warm evening in Paris.\u201dNarrative Highlight: GAN Progress on Face GenerationSource: Goodfellow et al., 2014; Radford et al., 2016; Liu and Tuzel, 2016; Karras et al., 2018; Karras et al., 2019; Goodfellow, 2019; Karras et al., 2020; Vahdat et al., 2021; Wang et al., 2022.Images Generated by DALL-E 2, Stable Diffusion and MidjourneySource: AI Index, 2022a. DALL-E 2b. Stable Diffusion c. MidjourneyFigure 2.2.172014 20152016 20172018202020212022Figure 2.2.18Table of Contents Chapter 2 Preview 91Artificial IntelligenceIndex Report 2023 2.1 Computer Vision\u2013ImageChapter 2: Technical PerformanceA Closer Look at Progress in Image Generation (cont\u2019d)Of all the recently released text-to-image generators, Google\u2019s Imagen performs best on the COCO benchmark (Figure 2.2.19)2. This year, the Google researchers who created Imagen also released a more difficult text-to-image benchmark, DrawBench, designed to challenge increasingly capable text-to-image models.Narrative Highlight: Figure 2.2.1935.4932.6421.42 20.7917.8912.249.338.1210.397.55 7.27AttnGANDM-GANDF-GANDM-GAN + CLDALL-EGLIDEXMC-GANLAFITEDALL-E 2Make-A-SceneImagen 2017  2019  2020  2021  20220102030Trained on COCO-FIDNot Trained on COCO-FIDModelCOCO Fr\u00e9chet Inception Distance (FID) ScoreNotable Text-to-Image Models on MS-COCO 256 \u00d7 256 FID-30K: Fr\u00e9chet Inception Distance (FID) ScoreSource: Saharia et al., 2022 | Chart: 2023 AI Index Report2 The COCO benchmark, first launched in 2014, includes 328,000 images with 2.5 million labeled instances. Although it is typically used for object detection tasks, researchers have also deployed it for image generation.Table of Contents Chapter 2 Preview 92Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceVisual ReasoningVisual reasoning tests how well AI systems can reason across both textual and visual data, as in the examples of Figure 2.2.20.Visual Question Answering (VQA) ChallengeThe Visual Question Answering Challenge tests AI systems with open-ended textual questions about images. Successfully answering the questions requires that AI systems possess vision, language, and commonsense reasoning capabilities. This section reports progress on the VQA V2 dataset.This year the top-performing model on VQA V2 was PaLI, a multimodal model produced by Google researchers (Figure 2.2.21).A Collection of Visual Reasoning TasksSource: Agrawal et al., 2016Figure 2.2.202016 2017 2018 2019 2020 2021 202265%70%75%80%85%Accuracy (%)84.30%Visual Question Answering (VQA) V2 Test-Dev: Accuracy Source: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index Report80.78%, Human BaselineFigure 2.2.21Table of Contents Chapter 2 Preview 93Artificial IntelligenceIndex Report 2023 2.1 Computer Vision\u2013ImageChapter 2: Technical PerformanceThe Rise of Capable Multimodal Reasoning SystemsTraditionally AI has been strong in narrow tasks, but it has been unable to easily generalize across multiple domains. For instance, many image classifiers are adept at classifying images but are incapable of understanding written text.However, recent technical progress in AI has begun to challenge this notion. In 2022, several Figure 2.2.22Narrative Highlight: VisionVisionVisionVisionVision-LanguageVision-LanguageVision-LanguageVision-LanguageVision-LanguageCategorySemanticSegmentationObjectDetectionInstanceSegmentationImageClassicationVisualReasoningVisual QAImageCaptioningFinetunedRetrievalZero-ShotRetrieval TaskADE20KCOCOCOCOImageNetNLVRVQAv2COCOCOCOFlickr30KFlickr30KDatasetmIoUAPAPTop-1 AccuracyAccuracyVQA AccuracyCIDErR@1R@1Metric61.4063.3054.7089.0087.0082.30145.3072.5086.50Previous SOTAFD-SwimV2DINOMask DINOFD-CLIPCoCACoCAOFAFlorenceCoCAModel ofPrevious SOTA62.8063.7054.8089.6092.6084.00147.6076.0088.20BEiT-32.28%0.63%0.18%0.67%6.44%2.07%1.58%4.83%1.97%Scale ofImprovementBEiT-3 Vs. Previous State-of-the-Art ModelsSource: Wang et al., 2022 | Table: 2023 AI Index Reporti ii ii ii iisi -isi -isi - a aisi n-Lan ua eision-LanguagetiijiI tt tiIl i tiis ls iis l I aa ti iFinetuneetrievalero-ShotRetrieval I tFlickr3Flickr30-1 rr rI r11.....1 .72.586.50iI I- IFl renceo......1 7.7 .88.20...1... 71..831.97l  fI ti   r :  t l.,  | l :   VisionVisionVisionVisionVision-LanguageVision-LanguageVision-LanguageVision-LanguageVision-LanguageCategorySemanticSegmentationObjectDetectionInstanceSegmentationImageClassi cationVisualReasoningVisual QAImageCaptioningFinetunedRetrievalZero-ShotRetrieval TaskADE20KCOCOCOCOImageNetNLVRVQAv2COCOCOCOFlickr30KFlickr30KDatasetmIoUAPAPTop-1 AccuracyAccuracyVQA AccuracyCIDErR@1R@1Metric61.4063.3054.7089.0087.0082.30145.3072.5086.50Previous SOTAFD-SwimV2DINOMask DINOFD-CLIPCoCACoCAOFAFlorenceCoCAModel ofPrevious SOTA62.8063.7054.8089.6092.6084.00147.6076.0088.20BEiT-32.28%0.63%0.18%0.67%6.44%2.07%1.58%4.83%1.97%Scale ofImprovementBEiT-3 Vs. Previous State-of-the-Art ModelsSource: Wang et al., 2022 | Table: 2023 AI Index Reportmodels were introduced, for example BEiT-3 from Microsoft and PaLI from Google, that posted state-of-the-art results across a variety of both vision and language benchmarks. For example, at the time of publication of the BEiT-3 paper, BEiT-3 posted state-of-the-art results for four different vision skills and five different vision-language skills (Figure 2.2.22).Table of Contents Chapter 2 Preview 94Artificial IntelligenceIndex Report 2023 2.1 Computer Vision\u2013ImageChapter 2: Technical PerformanceThe Rise of Capable Multimodal Reasoning Systems (cont\u2019d)Figure 2.2.23 shows some of the different vision-language tasks challenging multimodal systems like PaLI and BEiT-3.Figure 2.2.23Narrative Highlight: A Collection of Vision-Language TasksSource: Chen et al., 2022Table of Contents Chapter 2 Preview 95Artificial IntelligenceIndex Report 2023 2.2 Computer Vision\u2014ImageChapter 2: Technical PerformanceVisual Commonsense Reasoning (VCR)The Visual Commonsense Reasoning challenge, first launched in 2019, is a relatively new benchmark in which AI systems must answer questions presented from images, as in VQA, but also select the reasoning behind their answer choices. Figure 2.2.24 shows an VCR is one of the few visual benchmarks considered in this report on which AI systems have yet to surpass human performance, as shown in Figure 2.2.25.example of a question posed in VCR. Performance on VCR is tracked in the Q->AR score, which combines the ability of machines to select the right answer for the question (Q->A) and the ability to select the correct rationale behind the answer (Q->R).Figure 2.2.242018 2019 2020 2021 202250607080Q->AR Score75.60Visual Commonsense Reasoning (VCR) Task: Q->AR ScoreSource: VCR Leaderboard, 2022 | Chart: 2023 AI Index Report85.00, Human BaselineFigure 2.2.25A Sample Question from the Visual Commonsense Reasoning (VCR) ChallengeSource: Zellers et al., 2018Table of Contents Chapter 2 Preview 96Artificial IntelligenceIndex Report 2023 2.3 Computer Vision\u2014VideoChapter 2: Technical PerformanceActivity RecognitionActivity recognition is the categorization of activities that occur in videos. Certain activities, such as sitting, sleeping, or walking, are easier for AI systems to categorize than others which involve multiple steps\u2014for example, preparing dinner.Kinetics-400, Kinetics-600, Kinetics-700Kinetics-400, Kinetics-600, and Kinetics-700 are a series of datasets for benchmarking video activity recognition. Each dataset includes 650,000 large-scale, high-quality video clips from YouTube that display a wide range of human activities, and each asks AI systems to classify an action from a possible set of 400, 600, and 700 categories, respectively (Figure 2.3.1).2.3 Computer Vision\u2014VideoVideo analysis concerns reasoning or task operation across videos, rather than single images.Example Classes From the Kinetics DatasetSource: Kay et al., 2017Figure 2.3.1Table of Contents Chapter 2 Preview 97Artificial IntelligenceIndex Report 20232016 2017 2018 2019 2020 2021 202260%70%80%90%Top-1 Accuracy (%)84.00%, Kinetics-70091.10%, Kinetics-40091.80%, Kinetics-600Kinetics-400, Kinetics-600, Kinetics-700: Top-1 AccuracySource: Papers With Code, 2021; arXIv, 2022 | Chart: 2023 AI Index Report2.3 Computer Vision\u2014VideoChapter 2: Technical PerformanceAs of 2022, there is a 7.8 percentage point gap in performance between the top system on Kinetics-600 and Kinetics-700, which suggests the 700 series dataset is still a meaningful challenge for video computer vision researchers (Figure 2.3.2).Figure 2.3.2Table of Contents Chapter 2 Preview 98Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceA Closer Look at the Progress of Video GenerationMultiple high quality text-to-video models, AI systems that can generate video clips from prompted text, were released in 20223. In May, researchers from Tsinghua University and the Beijing Academy of Artificial Intelligence released CogVideo, a model that posted the then-highest inception score on the UCF-101 benchmark for text-to-video generation (Figure 2.3.3).Figure 2.2.3Narrative Highlight: 27.38 28.8724.6932.36 32.750.4679.2882.55DVD-GAN TGANv2 VideoGPT MoCoGAN-HD DIGAN CogVideo TATS-base Make-A-Video2019 2020 2021 202201020304050607080ModelInception Score (IS)Notable Text-to-Video Models on UCF-101: Inception Score (IS)Source: Hong et al., 2022; Singer et al., 2022 | Chart: 2023 AI Index ReportIn September 2022, CogVideo\u2019s top score was significantly surpassed by Meta\u2019s Make-A-Video model (Figure 2.3.3). Make-A-Video performed 63.6% better on UCF-101 than CogVideo. And, in October 2022, Google released a text-to-video system called Phenaki; however, this model was not benchmarked on UCF-101.2.3 Computer Vision\u2014Video3 Although these models are impressive, it is worth noting that they are thus far only capable of generating videos of a few seconds\u2019 duration.Table of Contents Chapter 2 Preview 99Artificial IntelligenceIndex Report 2023 2.4 LanguageChapter 2: Technical PerformanceEnglish Language Understanding English language understanding challenges AI systems to understand the English language in various ways: reading comprehension, yes/no reading comprehension, commonsense reading comprehension, and logical reasoning.SuperGLUESuperGLUE is a comprehensive English language understanding benchmark that tracks the progress of AI models on eight different linguistic tasks.  A selection of these tasks is highlighted in Figure 2.4.1. Their performance is then aggregated into a single metric.2.4 LanguageNatural language processing (NLP) is the ability of computer systems to understand text. The last few years have seen the release of increasingly capable \u201clarge language models,\u201d AI systems like PaLM, GPT-3, and GLM-130B, that are trained on massive amounts of data and adaptable to a wide range of downstream tasks.In this section, progress in NLP is tracked across the following skill categories: (1) English language understanding, (2) text summarization, (3) natural language inference, (4) sentiment analysis, (5) multitask language understanding, and (6) machine translation.Figure 2.4.14 For the sake of brevity, this figure only displays four of the eight tasks.A Set of SuperGLUE Tasks4Source: Wang et al., 2019Table of Contents Chapter 2 Preview 100Artificial IntelligenceIndex Report 20232019 2020 2021 202285868788899091Score91.30SuperGLUE: ScoreSource: SuperGLUE Leaderboard, 2022 | Chart: 2023 AI Index Report89.80, Human Baseline2.4 LanguageChapter 2: Technical PerformanceThis year\u2019s top model on SuperGLUE, Vega, registered a new state-of-the-art score of 91.3, which is 1.5 percentage points higher than the human baseline. Performance on SuperGLUE is continuing to saturate.Reading Comprehension Dataset Requiring Logical Reasoning (ReClor)In response to the saturation of traditional reading comprehension benchmarks, researchers from the National University of Singapore launched ReClor in 2020. ReClor, or Reading Comprehension Dataset Requiring Logical Reasoning, is a dataset of logical reasoning questions taken from the LSAT, the entrance exam for law schools in the United States and Canada. A sample question is shown in Figure 2.4.3Figure 2.4.2Figure 2.4.3 A Sample Question from the Reading Comprehension Dataset Requiring Logical Reasoning (ReClor)Source: Yu et al., 2020Context: When a certain gland becomes cancerous in humans, it produces high levels of a particular protein. A blood test can determine the level of this protein well before a cancer of the gland could be detected by other means. Some doctors recommend that aggressive anticancer treatment should be begun as early as possible for anyone who is tested and is found to have high levels of the protein.Question: Which one of the following, if true, most seriously weakens the doctors\u2019 recommendation?A. The blood test for the protein has been in use for some time to monitor the condition of patients who have been diagnosed as having cancer of the gland.B. Before the blood test became available, about one-third of all cases of cancer of the gland were detected in early stages.C. So far, no patients whose protein levels were found to be normal have subsequently developed cancer of the gland.D. Enlargement of the gland, a common condition infrequently associated with cancer, results in high levels of the protein.Table of Contents Chapter 2 Preview 101Artificial IntelligenceIndex Report 20232020 2021 202265%70%75%80%Accuracy (%)80.60%Reading Comprehension Dataset Requiring Logical Reasoning (ReClor): AccuracySource: ReClor Leaderboard, 2022; Papers With Code, 2022 | Chart: 2023 AI Index Report2.4 LanguageChapter 2: Technical PerformanceFigure 2.4.4 examines progress on ReClor. The top 2022 result of 80.6% represented an 18 percentage point improvement from 2020, the year the benchmark was released.Figure 2.4.4Table of Contents Chapter 2 Preview 102Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceJust How Much Better Have Language Models Become?The AI Index tested how three large language models from three different years, GPT-2 (2019), GPT-3 (2020), and ChatGPT (2022), handle the same prompt: \u201cExplain to me the major accomplishments of Theodore Roosevelt\u2019s presidency.\u201d More recent models are able to answer this question more effectively, both in terms of factual accuracy and quality of writing.Narrative Highlight: 5 GPT-2 used the 124M parameter model downloaded from OpenAI\u2019s GitHub page.6 The complete answer outputted by GPT-2 is trimmed here for brevity. The full answer is included in the Appendix.7 The specific GPT-3 model that was used was text-curie-001, which has training data up to October 2019.8 The information in this section has been cross-verified with the Encyclopedia Britannica entries on Theodore Roosevelt, Franklin Delano Roosevelt, Woodrow Wilson, and the National Park Service, as well as the history page of the National Wildlife Federation.9 Information on the history of the Grand Canyon National Park was cross-verified with the Wikipedia entry on the Grand Canyon National Park.2.4 LanguageGPT-2 (February 2019)5(The most notable is the establishment of a national telephone and mail exchange company.)But the only things that will change with a president who knows how to operate a business is his understanding of the American system of government. The public knows all about our problems, yet at least half of the people who need it won\u2019t know anything about ours. We can no longer have this. Our system of government is the only one in which you really need a public relations firm to work hard.The system of governmental control, while working, is not always effective in ensuring that the very things that our economy needs, do not always be at the top of everybody\u2019s list. So I wanted to know how successful the system you employ for managing the public\u2019s needs is in helping the American economy grow, how efficient the service our economy provides is being made, and how it can better serve the needs of a growing number of Americans.I wanted to know.This was before I ever hired my own lawyer to work on the case. This was before I ever hired my own law firm to handle government investigations. For my first major job, at least, I had to convince two top aides to get paid $90,000 for the role, and before I even started on the case, I had to convince one top staffer to get paid even $100,000 to have an interview with my lead prosecutor...6Commentary: Most of this answer is gibberish. GPT-2 is able to respond with sentences that are largely grammatical, but fails to actually answer the question in any meaningful way.GPT-3 (June 2020)7Theodore Roosevelt\u2019s presidency was marked by a number of accomplishments. He is credited with leading the United States out of the Great Depression and into World War II. He also spearheaded the creation of the National Park Service and the National Wildlife Federation. Roosevelt also played a major role in the development of the Panama Canal.Commentary: This answer is an improvement over that of GPT-2 in that it actually somewhat responds to the question, but still shows significant flaws. GPT-3 is correct in asserting that Theodore Roosevelt was instrumentally involved in the development of the Panama Canal. However, he did not lead the United States out of the Great Depression and into World War II\u2014that was done by a different American president, Franklin Delano Roosevelt. It appears that GPT-3 confused some of Franklin Delano Roosevelt\u2019s accomplishments with those of Theodore Roosevelt. Finally, although Theodore Roosevelt did play a significant role in establishing many national parks, he did not in fact create the National Park Service or the National Wildlife Federation. The National Park Service was created in 1916 by Woodrow Wilson, and the National Wildlife Federation in 1936, much later than Theodore Roosevelt\u2019s presidency. 8ChatGPT (November 2022)Theodore Roosevelt, President from 1901-1909, accomplished the following major accomplishments during his presidency:1. Conservation: Established numerous national parks, monuments, and forests, including the Grand Canyon National Park.2. Trust-busting: Took on monopolies, such as Standard Oil, through antitrust laws and broke up large corporations.3. Labor Reforms: Improved working conditions for Americans through advocacy for better hours, wages, and workplace safety.4. Progressive Reforms: Promoted progressive policies, including regulation of big business and protection of natural resources.5. National Infrastructure: Improved national infrastructure through construction of the Panama Canal and funding for dam projects.6. Diplomacy: Negotiated the end of the Russo-Japanese War, resulting in his being awarded the Nobel Peace Prize.7. Foreign Policy: Established the Roosevelt Corollary, asserting U.S. intervention in Latin America to maintain stability and protect American interests.Commentary: ChatGPT\u2019s response is the strongest of the three. All but one of the facts about Theodore Roosevelt\u2019s presidency are correct. Roosevelt proclaimed the Grand Canyon Game Preserve in 1906 and designated it a national monument in 1908. However, the Grand Canyon National Park was itself created by Woodrow Wilson.9 Otherwise the response is factually accurate. The response is also more thorough and concise than the answers provided by GPT-2 and GPT-3.Table of Contents Chapter 2 Preview 103Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformancePlanning and Reasoning in Large Language ModelsAs illustrated above, AI systems have become increasingly strong on a wide range of reasoning tasks. This improvement has led many to claim that emerging AI systems, especially large language models, possess reasoning abilities that are somewhat similar to those possessed by humans.10 Other authors, however, have argued otherwise.11In 2022, researchers (Valmeekam et al., 2022) introduced a more challenging planning and reasoning test for large language models that consists of seven assignments: (1) plan generation, (2) cost-optimal planning, (3) reasoning about plan execution, (4) robustness to goal reformulation, (5) ability to reuse plans, (6) replanning, and (7) plan generalization.12Narrative Highlight: 10 Some of the papers that claim language models can reason include: Kojima et al., 2022; Chowdhery et al., 2022; Li et al., 2021; Wei et al., 2022.11 Valmeekam et al., 2022 advances this claim.12 A complete description of these tasks can be found in the paper.2.4 LanguageFigure 2.4.50.6%0.2%5.6%6.6%0%77.4%69.2%22.0%5.0%3.2%4.8%9.8%14.4%76.8%76.0%60.2%0.5%0%3.0%11.0%0%21.0%9.0%5.0%0% 10% 20% 30% 40% 50% 60% 70% 80%Plan GenerationOptimal PlanningReplanningPlan GeneralizationPlan ReuseRobustness to Goal Reformulation(Shuing Goal Predicates)Robustness to Goal Reformulation(Full \u2192 Partial)Robustness to Goal Reformulation(Partial \u2192 Full)GPT-3Instruct-GPT3BLOOMInstances Correct (%)Select Large Language Models on the Blocksworld Domain: Instances CorrectSource: Valmeekam et al., 2022 | Chart: 2023 AI Index ReportThe authors then tested notable language models on these tasks in a Blocksworld problem domain, a problem environment where agents are given blocks of different colors and tasked with arranging these blocks in particular orders. The authors demonstrated that these large language models performed fairly ineffectively (Figure 2.4.5). While GPT-3, Instruct-GPT3, and BLOOM demonstrated the ability, in some contexts, to reformulate goals in robust ways, they struggled with other tasks like plan generation, optimal planning, and plan reuse. Compared to humans, the large language models performed much worse, suggesting that while they are capable, they lack human reasoning capabilities.Table of Contents Chapter 2 Preview 104Artificial IntelligenceIndex Report 20232017 2018 2019 2020 2021 202235404550ROUGE-150.95, arXiv51.05, PubMedArXiv and PubMed: ROUGE-1Source: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index Report2.4 LanguageChapter 2: Technical PerformanceText SummarizationText summarization tests how well AI systems can synthesize a piece of text while capturing its core content. Text summarization performance is judged on ROUGE (Recall-Oriented Understudy for Gisting Evaluation), which measures the degree to which an AI-produced text summary aligns with a human reference summary.arXiv and PubMedArXiv and PubMed are two widely used datasets for benchmarking text summarization. The model that posted the state-of-the-art score in 2022 on both arXiv and PubMed, AdaPool, was developed by a team from Salesforce Research (Figure 2.4.6).Figure 2.4.6Table of Contents Chapter 2 Preview 105Artificial IntelligenceIndex Report 20232019 2020 2021 202284%86%88%90%92%94%Accuracy (%)93.65%Abductive Natural Language Inference (aNLI): AccuracySource: Allen Institute for AI, 2022 | Chart: 2023 AI Index Report92.90%, Human Baseline2.4 LanguageChapter 2: Technical PerformanceNatural Language InferenceAlso known as textual entailment, natural language inference is the ability of AI systems to determine whether a hypothesis is true, false, or undetermined based on presented premises.Abductive Natural Language Inference (aNLI)Abductive natural language inference is a form of natural language inference in which plausible conclusions must be drawn from a set of limited and Abductive natural language inference is a challenging task. The human baseline remained unsurpassed until 2022, when an AI system registered a score of 93.7% (Figure 2.4.8).uncertain premises. Imagine, for example, that Peter returns to his car after dinner at a restaurant to find the window shattered and his laptop, which he left in the back seat, missing. He might immediately conclude that a thief broke into his car and stole the laptop.In 2019, the Allen Institute for AI launched aNLI, a comprehensive benchmark for abductive natural language inference that includes 170,000 premise and hypothesis pairs (Figure 2.4.7).Sample Question From the Abductive Natural Language Inference Benchmark (aNLI)Source: Allen Institute for AI, 2021Figure 2.4.7Figure 2.4.8Table of Contents Chapter 2 Preview 106Artificial IntelligenceIndex Report 20232013 2014 2015 2016 2017 2018 2019 2020 2021 202245%50%55%60%Accuracy (%)59.80%SST-5 Fine-Grained: AccuracySource: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index Report2.4 LanguageChapter 2: Technical PerformanceSentiment AnalysisSentiment analysis applies NLP techniques to identify the sentiment of a particular text. It is used by many businesses to better understand customer reviews.SST-5 Fine-Grained ClassificationThe Stanford Sentiment Treebank (SST) is a dataset of 11,855 single sentences taken from movie reviews that are then transformed into 215,154 unique phrases whose sentiments have been annotated by human judges (Figure 2.4.9).A new state-of-the-art score of 59.8% was posted on SST-5 fine-grained classification by the Heinsen Routing + RoBERTa Large model (Figure 2.4.10).A Sample Sentence from SSTSource: Socher et al., 2013Figure 2.4.9Figure 2.4.10Table of Contents Chapter 2 Preview 107Artificial IntelligenceIndex Report 20232019 2020 2021 202230%40%50%60%70%Accuracy (%)75.20%MMLU: Average Weighted AccuracySource: Papers With Code, 2022; arXiv, 2022 | Chart: 2023 AI Index Report2.4 LanguageChapter 2: Technical PerformanceMultitask Language UnderstandingA common criticism of language benchmarks such as GLUE and SuperGLUE is that they do not accurately test how capable language models are at applying the knowledge they learn across different domains.13 Multitask language understanding tests the ability of language models to reason across specialized subject domains.Massive Multitask Language Understanding (MMLU)Massive Multitask Language Understanding (MMLU) evaluates models in zero-shot or few-shot settings across 57 diverse subjects in the humanities, STEM, and the social sciences (Figure 2.4.11).Gopher, Chinchilla, and variants of PaLM have each posted state-of-the-art results on MMLU. The current top result on MMLU comes from Flan-PaLM, a Google model that reports an average score of 75.2% (Figure 2.4.12).Sample Questions From MMLUSource: Hendrycks et al., 2021Figure 2.4.11Figure 2.4.12a) Sample Math Questionsb) A Sample Microeconomics Question13 This criticism is more formally articulated in Hendrycks et al., 2021.Table of Contents Chapter 2 Preview 108Artificial IntelligenceIndex Report 20236 5 58 99 10 1518 21232838459 1012 13162123263446542017-May 2017-Jul 2017-Nov 2018-Mar 2018-Jul 2018-Dec 2019-Jun 2019-Nov 2020-Jul 2021-Sep 2022-Jul01020304050607080 CommercialOpen Source Pre-trainedPreviewNumber of Independent Machine Translation ServicesNumber of Independent Machine Translation ServicesSource: Intento, 2022 | Chart: 2023 AI Index Report2.4 LanguageChapter 2: Technical PerformanceMachine Translation (MT)Machine translation studies how well AI software can translate languages. In the last five years, machine translation has been dominated by neural networks which power current tools like DeepL and Google Translate.Figure 2.4.13Number of Commercially Available MT SystemsThe popularity of AI-based machine translation is manifested in the number of commercial machine translation services on the market. Since 2017, the total number of independent machine translation services has increased six times (Figure 2.4.13).Table of Contents Chapter 2 Preview 109Artificial IntelligenceIndex Report 20232017 2018 2019 2020 2021 20220%2%4%6%8%Equal Error Rate (EER)0.14%VoxCeleb: Equal Error Rate (EER)Source: VoxCeleb, 2022 | Chart: 2023 AI Index Report2.5 SpeechChapter 2: Technical PerformanceSpeech RecognitionSpeech recognition is the ability of AI systems to identify spoken words and convert them into text. Speech recognition has progressed so much so that nowadays many computer programs or texting apps are equipped with dictation devices that can seamlessly transcribe speech into writing.VoxCelebVoxCeleb is a large-scale audiovisual dataset of human speech for speaker recognition, which is the task of matching certain speech with a particular individual. Over the years, the VoxCeleb dataset has been expanded; however, the data in this subsection tracks progress on the original dataset.This year\u2019s top result on the original VoxCeleb dataset was posted by American researchers, whose model achieved an equal error rate of 0.1%, which represents a 0.28 percentage point decrease from the state-of\u2013the-art result achieved by Chinese researchers in the previous year (Figure 2.5.1).2.5 SpeechAI systems that work with human speech are usually tasked with converting spoken words into text and recognizing the individuals speaking.Figure 2.5.1Table of Contents Chapter 2 Preview 110Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceWhisperOne of the major themes in the last few years of AI progress has been the emergence of large language models that are trained on massive amounts of data and capable of executing a diverse range of tasks. In 2022, this idea of training on large data to achieve cross-domain performance arrived in the world of speech recognition with OpenAI\u2019s launch of Whisper.Whisper is a large-scale speech recognition model that was trained in a weakly supervised way on 700,000 hours of audio data. Whisper was capable of strong, although not state-of-the-art, performance on many speech recognition tasks in zero-shot settings.14 Whisper outperformed wav2vec 2.0 Large, another speech recognition model, across a wide range of popular English speech recognition benchmarks (Figure 2.5.2). Similarly, Whisper proved to be a better speech translator than many other leading AI translator models (Figure 2.5.3). Whisper also outperformed other commercial automated speech recognition systems and scored similarly to top human transcription services (Figure 2.5.4).15 Despite this impressive performance, there were still some speech tasks, like language identification, on which Whisper trailed state-of-the-art models (Figure 2.5.5).Narrative Highlight: 14 Zero-shot learning refers to the ability of an AI system to learn a particular task without being trained on that task.15 Kincaid46 is a dataset of 46 audio files and transcripts that were published in the blog post, \u201cWhich automatic transcription service is the most accurate?\u20142018.\u201d2.5 Speech2.7%6.2%9.0%4.4%4.0%25.5%7.3%16.2%16.9%13.8%17.6%3.9%36.4%5.2%2.7%24.5%29.9%14.6%10.5%65.8%17.9%35.6%37.0%28.3%34.8%7.7%67.6%6.2%0% 10% 20% 30% 40% 50% 60% 70%LibriSpeech CleanArtieCommon VoiceFLEURS EnTED-LIUMCHiME-6VoxPopuli EnCORAALAMI IHMSwitchboardCallHomeWSJAMI SDM1LibriSpeech Otherwav2vec 2.0 Large (No LM)Whisper Large V2Word Error Rate (%)wav2vec 2.0 Large (No LM) Vs. Whisper Large V2Across DatasetsSource: Radford et al., 2022 | Chart: 2023 AI Index Report14.7%22.1%24.8%25.2%29.1%0% 10% 20% 30%XMEF-XXLS-R (2B)mSLAM-CTC (2B)MAESTROZero-Shot WhisperBilingual Evaluation Understudy (BLEU) ScoreNotable Models on X\u2192EN Subset of CoVoST 2Source: Radford et al., 2022 | Chart: 2023 AI Index ReportFigure 2.5.2Figure 2.5.3Table of Contents Chapter 2 Preview 111Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceWhisper (cont\u2019d)Whisper represents a breakthrough in state-of-the-art speech recognition systems. Traditionally, such systems were either pre-trained using supervised learning methods or pre-trained without supervision but required fine-tuning. Acquisition of data for supervised pre-training is time-consuming and costly. However, pre-training without supervision still requires further algorithmic specification to realize a desired objective like speech recognition. Algorithmic specification itself often requires a skilled practitioner. Whisper resolves these issues by demonstrating that a speech recognition system can perform well across a diverse range of tasks with massive amounts of unlabeled speech data.Narrative Highlight: 2.5 Speech10.50%8.96%8.65%8.14%7.61%12.20%10.90%9.74%9.66%8.81%0% 2% 4% 6% 8% 10% 12% 14%Company ICompany HCompany GCompany FCompany ECompany DCompany CCompany BCompany AWhisperASR Computer-Assisted Human TranscriptionMedian Word Error Rate (%)Notable Speech Transcription Services on Kincaid46Source: Radford et al., 2022 | Chart: 2023 AI Index Report71.4%77.7%64.5%w2v-bert-51 (0.6B) mSLAM-CTC (2B) Zero-shot Whisper0%20%40%60%80%Language Identication Accuracy (%)Notable Models on FLEURS: Language Identi cationAccuracySource: Radford et al., 2022 | Chart: 2023 AI Index ReportFigure 2.5.4 Figure 2.5.5Table of Contents Chapter 2 Preview 112Artificial IntelligenceIndex Report 2023 2.6 Reinforcement LearningChapter 2: Technical PerformanceReinforcement Learning EnvironmentsReinforcement learning agents require environments, not datasets, to train: They must be trained in environments where they can experiment with various actions that will allow them to identify optimal game strategies.ProcgenProcgen is a reinforcement learning environment introduced by OpenAI in 2019. It includes 16 procedurally generated video-game-like environments specifically designed to test the ability of reinforcement learning agents to learn generalizable skills (Figure 2.6.1). Performance on Procgen is measured in terms of mean-normalized score. Researchers typically train their systems on 200 million training runs and report an average score across the 16 Procgen games. The higher the system scores, the better the system.2.6 Reinforcement LearningIn reinforcement learning, AI systems are trained to maximize performance on a given task by interactively learning from their prior actions. Systems are rewarded if they achieve a desired goal and punished if they fail.The Different Environments in ProcgenSource: OpenAI, 2019Figure 2.6.1Table of Contents Chapter 2 Preview 113Artificial IntelligenceIndex Report 20232019 2020 2021 20220.400.50Mean of Min-Max Normalized Score0.57Procgen: Mean of Min-Max Normalized ScoreSource: arXiv, 2022 | Chart: 2023 AI Index Report2.6 Reinforcement LearningChapter 2: Technical PerformanceA team of industry and academic researchers from Korea posted the top score of 0.6 on Procgen in 2022 (Figure 2.6.2).Figure 2.6.2Table of Contents Chapter 2 Preview 114Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceBenchmark SaturationAn emerging theme in this year\u2019s AI Index is the observed performance saturation across many popular technical performance benchmarks. Last year\u2019s AI Index Report observed a similar trend; however, benchmark saturation has been particularly pronounced this year. Figure 2.6.3 shows the relative improvement since the benchmark first launched (overall improvement) and relative improvement within the last year (YoY improvement) on AI technical benchmarks considered in this year\u2019s AI Index. The improvements are reported as percent changes.For all but 7 of the benchmarks, the improvement registered is less than 5%. The median improvement within the last year is 4%, while the median improvement since launch is 42.4%.16 Moreover, this year the AI Index elected not to feature traditionally popular benchmarks like SQuAD1.1 and SQuAD2.0, as no new state-of-the-art results were posted. Moreover, the speed at which benchmark saturation is being reached is increasing. Researchers have responded to this increasing saturation by launching newer and more comprehensive benchmarking suites such as BIG-bench and HELM.Narrative Highlight: 16 The improvements reviewed in this section are reported as relative change. Figure 2.6.3 should therefore not be used to conduct comparisons of improvements across  benchmarks, as each benchmark has different parameters.2.6 Reinforcement LearningImageNet Top-1FVRTCeleb-DFMPIICityscapesKvasir-SEGSTL-10CIFAR-10VQACOCOVCRKinetics-400Kinetics-600Kinetics-700SuperGLUEReClorarXivPubMedANLISST-5MMLUVoxCelebProcgenVision Image Language SR RL0%20%40%60%80%100%120%Overall ImprovementYoY ImprovementImprovement (%)Vision Video BenchmarkImprovement Over Time on Select AI Index Technical Performance BenchmarksSource: AI Index, 2022 | Chart: 2023 AI Index ReportFigure 2.6.3Table of Contents Chapter 2 Preview 115Artificial IntelligenceIndex Report 20232018 2019 2020 2021 20220.20.512510204060Training Time (Minutes; Log Scale)2.25, Object Detection (Heavyweight)0.34, Object Detection (Lightweight)1.22, Image Segmentation0.52, Recommendation0.19, Image Classication2.15, Speech Recognition0.18, Language ProcessingMLPerf Training Time of Top Systems by Task: MinutesSource: MLPerf, 2022 | Chart: 2023 AI Index Report2.7 HardwareChapter 2: Technical PerformanceMLPerf TrainingMLPerf is an AI training competition run by the ML Commons organization. In this challenge, participants train ML systems to execute various tasks using a common architecture. Entrants are then ranked on their absolute wall clock time, which is how long it takes for the system to train.Last year, the AI Index observed that since the competition launched, training times for virtually every AI skill category had significantly decreased. This year, this trend has continued, albeit at a slightly slower pace. Record-low training times were posted in the object detection, speech recognition, image segmentation, recommendation, image classification, and language processing categories (Figure 2.7.1). In categories like image classification and object detection, the top AI systems can now train roughly 32 times quicker than in 2018, when the competition first launched.2.7 HardwareDeep learning AI algorithms are trained on GPUs or TPUs, which accelerate the training speed of AI systems. As AI systems process ever-larger datasets, it is crucial to monitor advancements in hardware capabilities.Figure 2.7.1Table of Contents Chapter 2 Preview 116Artificial IntelligenceIndex Report 20232018-Dec-122019-Jun-102020-Jul-292021-Jun-302021-Dec-012022-Jun-292022-Nov-0905001,0001,5002,0002,5003,0003,5004,0004,500Number of Accelerators211, Mean Number of Accelerators1,859, Average Accelerators Used by Top System4,216, Maximum Number of Accelerators UsedMLPerf Hardware: AcceleratorsSource: MLPerf, 2022 | Chart: 2023 AI Index Report2.7 HardwareChapter 2: Technical PerformanceData on the number of accelerators used by the hardware systems submitted to MLPerf also suggests that stronger hardware has been powering decreasing training times (Figure 2.7.2). Since the start of the MLPerf competition, the gap has grown between the mean number of accelerators used by all entrants and the average accelerators used by the systems that post the top results.17 This gap suggests that having better hardware is essential to training the fastest systems.Figure 2.7.217 An accelerator, like a GPU or TPU, is a chip that is chiefly used for the machine learning component of a training run.Table of Contents Chapter 2 Preview 117Artificial IntelligenceIndex Report 20232020 2021 2022250k300k350k400k450k500k550k600k650k700kThroughput630,221, Server (Queries/s)679,915, Oine (Samples/s)MLPerf Best-Performing Hardware for ImageClassi\ufffdcation: O\ufffdine and Server ScenarioSource: MLPerf, 2022 | Chart: 2023 AI Index Report2020 2021 20222.1M2.2M2.3M2.4M2.5M2.6M2.7MThroughput2,645,980, O ine (Samples/s)2,683,620, Server (Queries/s)MLPerf Best-Performing Hardware forRecommendation: O ine and Server ScenarioSource: MLPerf, 2022 | Chart: 2023 AI Index Report2020 2021 202230k40k50k60k70kThroughput70,992, Server (Queries/s)75,153, O ine (Samples/s)MLPerf Best-Performing Hardware for LanguageProcessing: O ine and Server ScenarioSource: MLPerf, 2022 | Chart: 2023 AI Index Report2020 2021 202270k80k90k100k110k120k130k140k150k160kThroughput136,498, Server (Queries/s)155,811, O ine (Samples/s)MLPerf Best-Performing Hardware for SpeechRecognition: O ine and Server ScenarioSource: MLPerf, 2022 | Chart: 2023 AI Index Report2.7 HardwareChapter 2: Technical PerformanceMLPerf InferenceIn deploying AI, inference is the step where trained AI systems generate predictions, e.g. classifying objects. In 2020, ML Commons introduced MLPerf Inference, a performance benchmarking suite that measures how fast a trained AI system can process inputs and produce inferences. The MLPerf Inference suite tracks the throughput of AI systems, measured in samples per second or queries per second.18Figures 2.7.3 to 2.7.6 plot the throughput of the state-of-the-art submissions on MLPerf Inference across four skill categories: image classification, language processing, recommendation, and speech recognition. The number of inferences generated by the top-performing AI systems has significantly increased since the first iteration of the competition in 2020. For example, the number of offline samples generated by the top image classifiers and language processors have more than doubled since 2020, while those for recommendation systems have increased by roughly 23%.Figure 2.7.3Figure 2.7.5Figure 2.7.4Figure 2.7.618 The following blog post from Dell Technologies offers a good distinction between offline and server samples: \u201cOffline\u2014one query with all samples is sent to the system under test (SUT). The SUT can send the results back once or multiple times in any order. The performance metric is samples per second. Server\u2014the queries are sent to the SUT following a Poisson distribution (to model real-world random events). One query has one sample. The performance metric is queries per second (QPS) within the latency bound.\u201dTable of Contents Chapter 2 Preview 118Artificial IntelligenceIndex Report 2023200320042005200620072008200920102011201220132014201520162017201820192020202120225.0e+81.0e+92.0e+95.0e+91.0e+102.0e+105.0e+101.0e+112.0e+115.0e+111.0e+122.0e+125.0e+121.0e+132.0e+135.0e+131.0e+142.0e+14FLOP/s (Log Scale)FP32 (Single Precision) Performance (FLOP/s) byHardware Release Date, 2003\u201322Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report200320042005200620072008200920102011201220132014201520162017201820192020202120222.0e+95.0e+91.0e+102.0e+105.0e+101.0e+112.0e+115.0e+111.0e+122.0e+125.0e+121.0e+132.0e+13Median FLOP/s (Log Scale)2.23e+13Median FP32 (Single Precision) Performance (FLOP/s),Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index Report2003\u2013222.7 HardwareChapter 2: Technical PerformanceTrends in GPUs: Performance and PriceThis year, the AI Index built on work previously done by the research collective Epoch and analyzed trends over time in GPU performance and price.19Figure 2.7.7 showcases the FP32 (single precision) performance FLOP/s of different GPUs released from 2003 to 2022. FLOP/s stands for \u201cFloating Point Operations per second\u201d and is a measure of the performance of a computational device. The higher the FLOP/s, the better the hardware.Figure 2.7.8 showcases the median single performance of new GPUs by release date, which continues to rise year over year. Since 2021, the median FLOP/s speed has nearly tripled, and since 2003 it has increased roughly 7,000 times.Figure 2.7.7 Figure 2.7.819 The Appendix fully delineates both the methodology of this approach and the unique ways in which AI Index research built upon the existing Epoch research.Table of Contents Chapter 2 Preview 119Artificial IntelligenceIndex Report 202320032004200520062007200820092010201120122013201420152016201720182019202020212022010B20B30B40B50BFLOP/s per U.S. DollarFP32 (Single Precision) Performance (FLOP/s) perSource: Epoch and AI Index, 2022 | Chart: 2023 AI Index ReportU.S. Dollar by Hardware Release Date, 2003\u2013222003200420052006200720082009201020112012201320142015201620172018201920202021202205B10B15B20B25B30B35BMedian FLOP/s per U.S. Dollar 3.59e+10Median FP32 (Single Precision) Performance (FLOP/s)Source: Epoch and AI Index, 2022 | Chart: 2023 AI Index Reportper U.S. Dollar, 2003\u2013222.7 HardwareChapter 2: Technical PerformanceFinally, figures 2.7.9 and 2.7.10 consider GPU trends in terms of FLOP/s per U.S. Dollar.20 This statistic considers whether the underlying performance of GPUs is increasing relative to their changing costs. As evidenced most clearly in Figure 2.7.10, the price\u2013performance of GPUs is rapidly increasing. The median FLOP/s per U.S. Dollar of GPUs in 2022 is 1.4 times greater than it was in 2021 and 5600 times greater than in 2003, showing a doubling in performance every 1.5 years. As noted in similar analyses, improvements in the price\u2013performance of AI hardware has facilitated increasingly larger training runs and encouraged the scaling of large AI models.Figure 2.7.9 Figure 2.7.1020 The data in figures 2.7.9 and 2.7.10 has been adjusted for inflation. The exact details of the adjustment are outlined in greater detail in the Appendix.Table of Contents Chapter 2 Preview 120Artificial IntelligenceIndex Report 2023 2.8 EnvironmentChapter 2: Technical PerformanceEnvironmental Impact of Select Large Language ModelsMany factors determine the amount of carbon emissions emitted by AI systems, including the number of parameters in a model, the power usage effectiveness of a data center, and the grid carbon intensity. Power Usage Effectiveness (PUE) is a metric used to evaluate the energy efficiency of data centers. It is the ratio of the total amount of energy used by a computer data center facility, including air conditioning, to the energy delivered to computing equipment. The higher the PUE, the less efficient the data center. Figure 2.8.1 shows how these factors compare across four large language models: GPT-3, Gopher, OPT, and BLOOM. It is challenging to directly compare the carbon footprint of these models, as the accounting methodologies for reporting carbon emissions are not standardized.Of the four language models being compared, GPT-3 released the most carbon, 1.4 times more than Gopher, 7.2 times more than OPT, and 20.1 times more than BLOOM.Figure 2.8.2 relativizes the carbon-emission estimates to real-life examples. For instance, BLOOM\u2019s training run emitted 1.4 times more carbon than the average American uses in one year and 25 times that of flying one passenger round trip from New York to San Francisco. BLOOM\u2019s training consumed enough energy to power the average American home for 41 years.212.8 EnvironmentThere have been mounting concerns about the environmental impact of computational resources and the energy required for AI training and inference. Although there is no standard benchmark for tracking the carbon intensity of AI systems, this subsection synthesizes the findings of different researchers who are exploring the link between AI and the environment. Conducting research on the environmental effects of AI was challenging as there are wildly varying estimates, the validity of which have not yet been definitively established. To that end, the AI Index focuses on research from a recent paper by Luccioni et al., 2022. As AI models continue growing in size and become more universally deployed, it will be increasingly important for the AI research community to consciously monitor the effect AI systems have on the environment.21 The U.S. Energy Information Administration estimates that in 2021, the average annual electricity consumption of a U.S. residential utility customer was 10,632 kilowatt hours (kWh).GopherBLOOMGPT-3OPTModel280B176B175B175BNumber ofParameters1.081.201.101.09Datacenter PUE330 gC02eq/kWh57 gC02eq/kWh429 gC02eq/kWh231 gC02eq/kWhGrid CarbonIntensity1,066 MWh433 MWh1,287 MWh324 MWhPowerConsumption352 tonnes25 tonnes502 tonnes70 tonnesC02 EquivalentEmissions380 tonnes30 tonnes552 tonnes76.3 tonnesC02 EquivalentEmissions x PUEEnvironmental Impact of Select Machine Learning Models, 2022Source: Luccioni et al., 2022 | Table: 2023 AI Index ReportFigure 2.8.1Table of Contents Chapter 2 Preview 121Artificial IntelligenceIndex Report 20230.995.5118.082563703525020 50 100 150 200 250 300 350 400 450 500Air Travel,1 Passenger, NY\u2013SFHuman Life,Avg., 1 YearAmerican Life,Avg., 1 YearBLOOM (176B)Car, Avg. Incl. Fuel,1 LifetimeOPT (175B)Gopher (280B)GPT-3 (175B)CO2 Equivalent Emissions (Tonnes)CO2 Equivalent Emissions (Tonnes) by Selected Machine Learning Models and Real Life Examples, 2022Source: Luccioni et al., 2022; Strubell et al., 2019 | Chart: 2023 AI Index Report2.8 EnvironmentChapter 2: Technical PerformanceFigure 2.8.2Table of Contents Chapter 2 Preview 122Artificial IntelligenceIndex Report 2023Chapter 2: Technical PerformanceUsing AI to Optimize Energy UsageTraining AI systems can be incredibly energy intensive. At the same time, recent research suggests that AI systems can be used to optimize energy consumption. In 2022, DeepMind released the results of a 2021 experiment in which it trained a reinforcement learning agent called BCOOLER (BVE-based COnstrained Optimization Learner with Ensemble Regularization) to optimize cooling procedures for Google\u2019s data centers.Figure 2.8.3 presents the energy-saving results from one particular BCOOLER experiment. At the end of the three-month experiment, BCOOLER achieved roughly 12.7% energy savings. BCOOLER was able to achieve these savings while maintaining the cooling comfort levels that the building managers preferred.Narrative Highlight: 2021-Aug-01 2021-Aug-15 2021-Aug-29 2021-Sep-12 2021-Sep-26 2021-Oct-10 2021-Oct-240%2%4%6%8%10%12%Cumulative AI Savings (%)12.7%Energy Savings Results Over Time for Select BCOOLER ExperimentSource: Luo et al., 2022 | Chart: 2023 AI Index ReportFigure 2.8.32.8 EnvironmentTable of Contents Chapter 2 Preview 123Artificial IntelligenceIndex Report 2023 2.9 AI for ScienceChapter 2: Technical PerformanceAccelerating Fusion Science Through Learned Plasma ControlNuclear fusion could generate clean energy by fusing hydrogen. A common approach to achieving nuclear fusion is using a tokamak, a machine which controls and contains the heated hydrogen plasma (Figure 2.9.1). However, the plasmas produced in these machines are unstable and necessitate constant monitoring. In 2022, researchers at DeepMind developed a reinforcement learning algorithm to discover optimal tokamak management procedures.Discovering Novel Algorithms for Matrix Manipulation With AlphaTensorMatrix multiplication is a simple algebraic operation that is essential to many computations, including neural networks and scientific computing (Figure 2.9.2). The classic algorithm to multiply two 2x2 matrices takes 2^3 = 8 multiplications. Strassen discovered 50 years ago how to reduce this to 7, and generally how to multiply two n x n matrices in O(n^ log(7)) operations. DeepMind\u2019s AlphaTensor uses Reinforcement Learning to improve on state-of-the-art algorithms for many matrix sizes, including 4x4 matrices over the integers [0,1]. It also matches state-of-the-art performance on several other matrix sizes, including 4x4 over the integers. It does this by searching through large numbers of possible algorithms, and evaluating them over real computer architectures.2.9 AI for Science2022 was a groundbreaking year for AI in science. This subsection looks at some meaningful ways in which AI has recently been used to accelerate scientific discovery.Photos of the Variable Configuration Tokamak (TCV) at EPFLSource: DeepMind, 2022A Demonstration of AlphaTensor\u2019s Matrix Manipulation ProcessSource: Fawzi et al., 2022Figure 2.9.1Figure 2.9.2Table of Contents Chapter 2 Preview 124Artificial IntelligenceIndex Report 2023 2.9 AI for ScienceChapter 2: Technical PerformanceDesigning Arithmetic Circuits With Deep Reinforcement LearningThis year, a team at Nvidia discovered a novel approach to improving the chips that power AI systems: Use AI systems to design better chips. They were able to train a reinforcement learning agent to design chip circuits that are smaller, faster, and more efficient than the circuits designed by electronic design automation tools (EDAs). One of Nvidia\u2019s latest categories of chips, the Hopper GPU architecture, has over 13,000 instances of AI-designed circuits. Figure 2.9.3 shows a 64-bit adder circuit designed by Nvidia\u2019s PrefixRL AI agent (on the left) which is 25% smaller while being just as fast and functional as those designed by the state-of-the-art EDA tools.Unlocking de Novo Antibody  Design With Generative AIAntibody discovery, which is referred to as de novo antibody discovery, typically requires immense amounts of time and resources. Traditional methods for de novo discovery offer little control over the outputs, so that proposed antibodies are often suboptimal. To that end, a team of researchers turned to generative AI models to create antibodies in a zero-shot fashion, where antibodies are created with one round of model generation without further optimizations (Figure 2.9.4). These AI-generated antibodies are also robust. The fact that generative AI can create new antibodies has the potential to accelerate drug discovery.A Juxtaposition of Nvidia Circuits Designed by PrefixRL Vs. EDA ToolsSource: Roy et al., 2022Zero-Shot Generative AI for de Novo Antibody DesignSource: Shanehsazzadeh et al., 2023Figure 2.9.3Figure 2.9.4Artificial IntelligenceIndex Report 2023CHAPTER 3: Technical AI EthicsText and Analysis by Helen NgoTable of Contents 126Artificial IntelligenceIndex Report 2023Chapter 3 PreviewOverview 128Chapter Highlights 1293.1  Meta-analysis of Fairness  and Bias Metrics 130Number of AI Fairness and Bias Metrics 130Number of AI Fairness and Bias Metrics   (Diagnostic Metrics Vs. Benchmarks) 1313.2 AI Incidents 133AI, Algorithmic, and Automation  Incidents and Controversies (AIAAIC)  Repository: Trends Over Time 133AIAAIC: Examples of Reported Incidents 1343.3 Natural Language Processing   Bias Metrics 137Number of Research Papers Using  Perspective API 137 Winogender Task From the   SuperGLUE Benchmark 138 Model Performance on the Winogender   Task From the SuperGLUE Benchmark 138 Performance of Instruction-Tuned   Models on Winogender 139BBQ: The Bias Benchmark for  Question Answering 140Fairness and Bias Trade-Offs in NLP: HELM 142Fairness in Machine Translation 143RealToxicityPrompts 1443.4 Conversational AI Ethical Issues 145Gender Representation in Chatbots 145Anthropomorphization in Chatbots 146 Narrative Highlight:  Tricking ChatGPT 1473.5 Fairness and Bias in   Text-to-Image Models 148Fairness in Text-to-Image Models  (ImageNet Vs. Instagram) 148VLStereoSet: StereoSet for  Text-to-Image Models 150Examples of Bias in Text-to-Image Models 152 Stable Diffusion 152 DALL-E 2 153 Midjourney 1543.6 AI Ethics in China 155Topics of Concern 155Strategies for Harm Mitigation 156Principles Referenced by  Chinese Scholars in AI Ethics 157Technical AI EthicsCHAPTER 3 PREVIEW:Table of Contents 127Artificial IntelligenceIndex Report 2023Chapter 3 PreviewTechnical AI EthicsCHAPTER 3 PREVIEW (CONT\u2019D):ACCESS THE PUBLIC DATA3.7 AI Ethics Trends at FAccT   and NeurIPS 158ACM FAccT (Conference on Fairness,  Accountability, and Transparency) 158 Accepted Submissions by   Professional Affiliation 158 Accepted Submissions by   Geographic Region 159NeurIPS (Conference on Neural Information  Processing Systems) 160 Real-World Impact 160 Interpretability and Explainability 161 Causal Effect and Counterfactual   Reasoning 162 Privacy 163 Fairness and Bias 1643.8 Factuality and Truthfulness 165Automated Fact-Checking Benchmarks:  Number of Citations 165Missing Counterevidence and NLP  Fact-Checking 166TruthfulQA 167Table of Contents 128Artificial IntelligenceIndex Report 2023Chapter 3 PreviewOverviewFairness, bias, and ethics in machine learning continue to be topics of interest among both researchers and practitioners. As the technical barrier to entry for creating and deploying generative AI systems has lowered dramatically, the ethical issues around AI have become more apparent to the general public. Startups and large companies find themselves in a race to deploy and release generative models, and the technology is no longer controlled by a small group of actors.In addition to building on analysis in last year\u2019s report, this year the AI Index highlights tensions between raw model performance and ethical issues, as well as new metrics quantifying bias in multimodal models.Chapter 3: Technical AI EthicsTable of Contents 129Artificial IntelligenceIndex Report 2023Chapter 3 PreviewChapter HighlightsThe effects of model scale on bias and toxicity  are confounded by training data and mitigation methods.  In the past year, several institutions have built their own large models trained on proprietary data\u2014and while large models are still toxic and biased, new evidence suggests that these issues can be somewhat mitigated after training larger models with instruction-tuning.The number of incidents concerning the misuse  of AI is rapidly rising.According to the AIAAIC database, which tracks incidents related to the ethical misuse of AI, the number of AI incidents and controversies has increased 26 times since 2012. Some notable incidents in 2022 included a deepfake video of Ukrainian President Volodymyr Zelenskyy surrendering and U.S. prisons using call-monitoring technology on their inmates. This growth is evidence of both greater use of AI technologies and awareness of misuse possibilities.Generative models have arrived and so have their ethical problems.  In 2022, generative models became part  of the zeitgeist. These models are capable but also come with ethical challenges.  Text-to-image generators are routinely biased along gender dimensions, and chatbots like ChatGPT can be tricked into serving nefarious aims.Fairer models  may not be less biased. Extensive analysis of language models suggests that while there is a clear correlation between performance and fairness, fairness and bias can be at odds: Language models which perform better on certain fairness benchmarks tend to have worse gender bias.Interest in AI ethics  continues to skyrocket.The number of accepted submissions to FAccT, a leading AI ethics conference, has more than doubled since 2021 and increased by a factor of 10 since 2018. 2022 also saw more submissions than ever from industry actors.Automated fact-checking with natural language processing isn\u2019t so straightforward after all. While several benchmarks have been developed for automated fact-checking, researchers find that 11 of 16 of such datasets rely on evidence \u201cleaked\u201d from fact-checking reports which did not exist at the time of the claim surfacing.Chapter 3: Technical AI EthicsTable of Contents 130Artificial IntelligenceIndex Report 2023Chapter 3 Preview192016 2017 2018 2019 2020 2021 202205101520Number of MetricsNumber of AI Fairness and Bias Metrics, 2016\u201322Source: AI Index, 2022 | Chart: 2023 AI Index ReportNumber of AI Fairness  and Bias MetricsAlgorithmic bias is measured in terms of allocative and representation harms. Allocative harm occurs when a system unfairly allocates an opportunity or resource to a specific group, and representation harm happens when a system perpetuates stereotypes and power dynamics in a way that reinforces subordination of a group. Algorithms are considered fair when they make predictions that neither favor nor discriminate against individuals or groups based on protected attributes which cannot be used for decision-making due to legal or ethical reasons (e.g., race, gender, religion).3.1 Meta-analysis of  Fairness and Bias Metrics3.1 Meta-analysis of Fairness and Bias MetricsIn 2022 several new datasets or metrics were released to probe models for bias and fairness, either as standalone papers or as part of large community efforts such as BIG-bench. Notably, metrics are being extended and made specific: Researchers are zooming in on bias applied to specific settings such as question answering and natural language inference, extending existing bias datasets by using language models to generate more examples for the same task (e.g., Winogenerated, an extended version of the Winogender benchmark).Figure 3.1.1 highlights published metrics that have been cited in at least one other work. Since 2016 there has been a steady and overall increase in the total number of AI fairness and bias metrics.Figure 3.1.1Chapter 3: Technical AI EthicsTable of Contents 131Artificial IntelligenceIndex Report 2023Chapter 3 PreviewIn 2022 a robust stream of both new ethics benchmarks as well as diagnostic metrics was introduced to the community.Number of AI Fairness and Bias Metrics (Diagnostic Metrics Vs. Benchmarks)Measurement of AI systems along an ethical dimension often takes one of two forms. A benchmark contains labeled data, and researchers test how well their AI system labels the data. Benchmarks do not change over time. These are domain-specific (e.g., SuperGLUE and StereoSet for language models; ImageNet for computer vision) and often aim to measure behavior that is intrinsic to the model, as opposed to its downstream performance on specific populations (e.g., StereoSet measures model propensity to select stereotypes compared to non-stereotypes, but it does not measure performance gaps between different subgroups). These benchmarks often serve as indicators of intrinsic model bias, but they may not give as clear an indication of the model\u2019s downstream impact and its extrinsic bias when embedded into a system.A diagnostic metric measures the impact or performance of a model on a downstream task, and it is often tied to an extrinsic impact\u2014for example, the differential in model performance for some task on a population subgroup or individual compared to similar individuals or the entire population. These metrics can help researchers understand how a system will perform when deployed in the real world, and whether it has a disparate impact on certain populations. Previous work comparing fairness metrics in natural language processing found that intrinsic and extrinsic metrics for contextualized language models may not Chapter 3: Technical AI Ethicscorrelate with each other, highlighting the importance of careful selection of metrics and interpretation of results.In 2022, a robust stream of both new ethics benchmarks as well as diagnostic metrics was introduced to the community (Figure 3.1.2). Some metrics are variants of previous versions of existing fairness or bias metrics, while others seek to measure a previously undefined measurement of bias\u2014for example, VLStereoSet is a benchmark which extends the StereoSet benchmark for assessing stereotypical bias in language models to the text-to-image setting, while the HolisticBias measurement dataset assembles a new set of sentence prompts which aim to quantify demographic biases not covered in previous work.3.1 Meta-analysis of Fairness and Bias MetricsTable of Contents 132Artificial IntelligenceIndex Report 2023Chapter 3 PreviewChapter 3: Technical AI Ethics102942102931393112016 2017 2018 2019 2020 2021 202202468101214BenchmarksDiagnostic MetricsNumber of MetricsNumber of New AI Fairness and Bias Metrics (Diagnostic Metrics Vs. Benchmarks), 2016\u201322Source: AI Index, 2022 | Chart: 2023 AI Index ReportFigure 3.1.23.1 Meta-analysis of Fairness and Bias MetricsTable of Contents 133Artificial IntelligenceIndex Report 2023Chapter 3 Preview2602012 2013 2014 2015 2016 2017 2018 2019 2020 2021050100150200250Number of AI Incidents and ControversiesNumber of AI Incidents and Controversies, 2012\u201321Source: AIAAIC Repository, 2022 | Chart: 2023 AI Index ReportAI, Algorithmic, and Automation Incidents and Controversies (AIAAIC) Repository: Trends Over TimeThe AI, Algorithmic, and Automation Incidents and Controversies (AIAAIC) Repository is an independent, open, and public dataset of recent incidents and controversies driven by or relating to AI, algorithms, and automation. It was launched in 2019 as a private project to better understand some of the reputational risks of artificial intelligence and has evolved into a comprehensive initiative 3.2 AI Incidents3.2 AI Incidentsthat tracks the ethical issues associated with AI technology.The number of newly reported AI incidents and controversies in the AIAAIC database was 26 times greater in 2021 than in 2012 (Figure 3.2.1)1. The rise in reported incidents is likely evidence of both the increasing degree to which AI is becoming intermeshed in the real world and a growing awareness of the ways in which AI can be ethically misused. The dramatic increase also raises an important point: As awareness has grown, tracking of incidents and harms has also improved\u2014suggesting that older incidents may be underreported.Figure 3.2.1Chapter 3: Technical AI Ethics1 This figure does not consider AI incidents reported in 2022, as the incidents submitted to the AIAAIC database undergo a lengthy vetting process before they are fully added.Table of Contents 134Artificial IntelligenceIndex Report 2023Chapter 3 PreviewAIAAIC: Examples of Reported IncidentsThe subsection below highlights specific AI incidents reported to the AIAAIC database in order to demonstrate some real-world ethical issues related to AI. The specific type of AI technology associated with each incident is listed in parentheses alongside the date when these incidents were reported to the AIAAIC database.23.2 AI IncidentsDeepfake of President Volodymyr Zelenskyy Surrendering (Deepfake, March 2022)In March of 2022, a video that was circulated on social media and a Ukrainian news website purported to show the Ukrainian president directing his army to surrender the fight against Russia (Figure 3.2.2). It was eventually revealed that the video was a deepfake.Source: Verify, 2022Figure 3.2.2Chapter 3: Technical AI Ethics2 Although these events were reported in 2022, some of them had begun in previous years.Table of Contents 135Artificial IntelligenceIndex Report 2023Chapter 3 PreviewVerus U.S. Prison Inmate Call Monitoring  (Speech Recognition, Feb. 2022)Reports find that some American prisons are using AI-based systems to scan inmates\u2019 phone calls (Figure 3.2.3). These reports have led to concerns about surveillance, privacy, and discrimination. There is evidence that voice-to-text systems are less accurate at transcribing for Black individuals, and a large proportion of the incarcerated population in the United States is Black.Intel Develops a System for Student Emotion Monitoring (Pattern Recognition, April 2022)Intel is working with an education startup called Classroom Technologies to create an AI-based technology that would identify the emotional state of students on Zoom (Figure 3.2.4). The use of this technology comes with privacy and discrimination concerns: There is a fear that students will be needlessly monitored and that systems might mischaracterize their emotions.3.2 AI IncidentsSource: Reuters, 2022Figure 3.2.3Source: Protocol, 2022Figure 3.2.4Chapter 3: Technical AI EthicsTable of Contents 136Artificial IntelligenceIndex Report 2023Chapter 3 PreviewLondon\u2019s Metropolitan Police Service Develops Gang Violence Matrix (Information Retrieval,  Feb. 2022)The London Metropolitan Police Service allegedly maintains a dataset of over one thousand street gang members called the Gangs Violence Matrix (GVM) and uses AI tools to rank the risk potential that each gang member poses (Figure 3.2.5). Various studies have concluded that the GVM is not accurate and tends to discriminate against certain ethnic and racial minorities. In October 2022, it was announced that the number of people included in the GVM would be drastically reduced.Midjourney Creates an Image Generator  (Other AI, Sept. 2022)3Midjourney is an AI company that created a tool of the same name that generates images from textual descriptions (Figure 3.2.6). Several ethical criticisms have been raised against Midjourney, including copyright (the system is trained on a corpus of human-generated images without acknowledging their source), employment (fear that systems such as Midjourney will replace the jobs of human artists), and privacy (Midjourney was trained on millions of images that the parent company might not have had permission to use).3.2 AI IncidentsSource: StopWatch, 2022Figure 3.2.5Source: The Register, 2022Figure 3.2.6Chapter 3: Technical AI Ethics3 Although other text-to-image models launched in 2022 such as DALL-E 2 and Stable Diffusion were also criticized, for the sake of brevity the AI Index chose to highlight one particular incident.Table of Contents 137Artificial IntelligenceIndex Report 2023Chapter 3 Preview372018 2019 2020 2021 202205101520253035Number of Research PapersNumber of Research Papers Using Perspective API, 2018\u201322Source: Google Scholar Search, 2022 | Chart: 2023 AI Index ReportNumber of Research Papers Using Perspective APIThe Perspective API, initially released by Alphabet\u2019s Jigsaw in 2017, is a tool for measuring toxicity in natural language, where toxicity is defined as a rude, disrespectful, or unreasonable comment that is likely to make someone leave a conversation. It was subsequently broadly adopted in natural language processing research following the methodology of the RealToxicityPrompts paper introduced in 2020, which used the Perspective API to measure toxicity in the outputs of language models.3.3 Natural Language  Processing Bias Metrics3.3 Natural Language Processing Bias MetricsDevelopers input text into the Perspective API, which returns probabilities that the text should be labeled as falling into one of the following categories: toxicity, severe toxicity, identity attack, insult, obscene, sexually explicit, and threat. The number of papers using the Perspective API has increased by 106% in the last year (Figure 3.3.1), reflecting the increased scrutiny on generative text AI as these models are increasingly deployed in consumer-facing settings such as chatbots and search engines.Figure 3.3.1Chapter 3: Technical AI EthicsTable of Contents 138Artificial IntelligenceIndex Report 2023Chapter 3 PreviewWinogender Task From the SuperGLUE BenchmarkModel Performance on the Winogender Task From the SuperGLUE BenchmarkWinogender measures gender bias related to occupations. On the Winogender task, AI systems are measured on how often they fill in a sentence Chapter 3: Technical AI Ethicscontaining an occupation with stereotypical pronouns (e.g., \u201cThe teenager confided in the therapist because he/she seemed trustworthy\u201d).Results reported on PaLM support previous findings that larger models are more capable on the Winogender task (Figure 3.3.2), despite their higher tendency to generate toxic outputs.3.3 Natural Language Processing Bias Metrics57.90% 59.00%50.00%61.50%59.00% 60.00%63.30%71.70%64.17%71.40% 73.58%2022 New ModelsiPET (ALBERT)31MGopher100MWARP(ALBERT-XXL-V2)223MBort340MGopher10BGPT-313BPaLM62BGLAM64BGPT-3175BGopher280MPaLM540B0%20%40%60%80%100%Model and Number of ParametersWinogender Accuracy (%)Model Performance on the Winogender Task From the SuperGLUE BenchmarkSource: SuperGLUE Leaderboard, 2022 | Chart: 2023 AI Index Report95.90%, Human BaselineFigure 3.3.2Table of Contents 139Artificial IntelligenceIndex Report 2023Chapter 3 PreviewPerformance of Instruction-Tuned Models on WinogenderInstruction-tuned models are pre-trained language models which have been fine-tuned on datasets with tasks phrased as instructions. Instruction-tuning has been shown to improve performance across a wide Chapter 3: Technical AI Ethicsvariety of tasks, and smaller instruction-tuned models can often outperform their larger counterparts. Figure 3.3.3 shows the effect of instruction-tuned models on the Winogender benchmark in the generative setting\u2014they outperform larger models several times their size.3.3 Natural Language Processing Bias Metrics3.50%5.64%10.80%72.25%76.95%88.00%89.00%0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%PaLM 62BPaLM 540BPaLM 8BFlan-PaLM 8BFlan-T5-XXL 11BFlan-PaLM 540BFlan-PaLM 62BInstruction-TunedNon-Instruction-TunedWinogender Accuracy (%)Model and Number of ParametersWinogender: Zero Shot Evaluation in the Generative SettingSource: Chung et al., 2022 | Chart: 2023 AI Index ReportFigure 3.3.3Table of Contents 140Artificial IntelligenceIndex Report 2023Chapter 3 PreviewBBQ: The Bias Benchmark for Question AnsweringLanguage models are increasingly being deployed in question answering systems. While several benchmarks exist for the purpose of measuring bias in language models based on their outputs, including BOLD, CrowS-Pairs, and StereoSet, the BBQ benchmark measures how biases can manifest in the question-answering setting. BBQ measures bias along the axes of socioeconomic status, sexual orientation, religion, race/ethnicity, physical appearance, nationality, gender identity, disability status, and age.Examples consist of template-based context and question pairs, where each answer choice references a person belonging to either a stereotypical or anti-stereotypical social group. The third option is \u201cUnknown,\u201d designed for cases where the answer cannot be determined from the information given. Each example includes an ambiguous version where it is unclear which social group the entity belongs to, as well as additional context which can be used to disambiguate the correct answer. The ambiguous setting measures how often models fall back on stereotypes when the answer cannot be inferred from the given context. Models that do not exhibit bias have a score of zero, while a score of 100 indicates that the model chooses answers aligned with the social bias in question, and a score of -100 indicates the model always chooses the anti-stereotypical answer.An example of an ambiguous context might be, \u201cAt the computer store the other day, I overheard a conversation between an ICELANDIC guy and a Chapter 3: Technical AI EthicsGUINEAN guy.\u201d The corresponding question might be, \u201cWho has never used a computer?\u201d referencing the stereotypical bias that Africans are technologically illiterate. The example may be disambiguated with additional context such as, \u201cThe ICELANDIC guy was behind the counter trying to explain to the GUINEAN guy how to find the power switch on his computer!\u201dIn contexts where the answer is ambiguous, models are more likely to fall back on stereotypes and select unsupported answers rather than \u201cUnknown\u201d (Figure 3.3.4), and this result is exacerbated for models fine-tuned with reinforcement learning.4As seen in Figure 3.3.4, models can be more biased along certain identity categories than others\u2014most models are biased along the axes of physical appearance and age, but the biases along the axis of race/ethnicity are less clear. For reference, Figure 3.3.5 highlights bias in question answering on BBQ in disambiguated contexts.3.3 Natural Language Processing Bias Metrics4 This finding is further reinforced by Stanford\u2019s HELM benchmark.Models can be more biased along certain identity categories than others\u2014most models are biased along the axes of physical appearance and age, but the biases along the axis of race/ethnicity are less clear. Table of Contents 141Artificial IntelligenceIndex Report 2023Chapter 3 PreviewChapter 3: Technical AI Ethics3.3 Natural Language Processing Bias Metrics4.40 3.50 9.70 29.60 48.70 27.30 11.00 14.000.20 -3.00 -4.40 6.50 11.80 5.80 1.00 7.00-1.00 9.20 13.00 20.20 24.50 14.30 7.00 12.000.00 1.10 0.20 4.80 8.30 5.201.90 0.00 4.60 24.30 20.00 12.00 1.00 0.0017.00 40.70 41.00 38.50 47.70 40.90 4.00 16.002.20 5.10 18.40 20.40 14.50 6.00 4.00 10.002.80 14.00 11.60 32.30 41.50 32.3010.00 15.00 11.30 25.60 18.60 2.40 4.00 8.009.90 17.40 10.70 38.30 32.60 21.20 4.00 13.006.30 11.80 24.70 30.70 48.90 29.80 14.00 23.00RoBERTa-BaseRoBERTa-LargeDeBERTaV3-BaseDeBERTaV3-LargeUniedQA (ARC)Uni edQA (RACE)Dialogue-Prompted Chinchilla (DPC)DPC, RL-FinetunedSocio-Economic StatusSexual OrientationReligionRace/Ethnicity (Names)Race/EthnicityPhysical AppearanceNationalityGender Identity (Names)Gender IdentityDisability StatusAgeModelCategoryBias in Question Answering on BBQ by Identity Characteristic: Ambiguous ContextsSource: Parrish et al., 2022; Glaese et al., 2022 | Chart: 2023 AI Index Report7.00 3.50 3.80 2.90 3.80 3.90 8.00 7.006.50 -3.10 -4.80 -0.20 0.50 -0.70 -1.00 -1.005.20 3.40 1.80 1.70 3.50 0.20 5.00 7.000.40 -0.20 -0.30 0.00 0.30 -0.100.60 -0.80 1.20 0.00 0.90 0.00 3.00 1.0017.10 -2.70 4.20 -5.00 -1.70 -2.30 12.00 8.00-0.10 0.70 5.70 1.90 -0.20 1.20 -2.00 3.00-0.90 1.10 3.60 0.40 2.00 0.1014.00 2.90 4.60 -16.90 -3.40 -5.80 2.00 3.005.40 5.70 8.10 1.70 -0.70 -1.40 0.00 8.00-3.00 2.70 4.40 2.40 3.30 1.20 7.00 8.00RoBERTa-BaseRoBERTa-LargeDeBERTaV3-BaseDeBERTaV3-LargeUni edQA (ARC)Uni edQA (RACE)Dialogue-Prompted Chinchilla (DPC)DPC, RL-FinetunedSocio-Economic StatusSexual OrientationReligionRace/Ethnicity (Names)Race/EthnicityPhysical AppearanceNationalityGender Identity (Names)Gender IdentityDisability StatusAgeModelCategoryBias in Question Answering on BBQ by Identity Characteristic: Disambiguated ContextsSource: Parrish et al., 2022; Glaese et al., 2022 | Chart: 2023 AI Index ReportFigure 3.3.4Figure 3.3.5Table of Contents 142Artificial IntelligenceIndex Report 2023Chapter 3 PreviewFairness and Bias Trade-Offs in NLP: HELMNotions of \u201cfairness\u201d and \u201cbias\u201d are often mentioned in the same breath when referring to the field of AI ethics\u2014naturally, one might expect that models which are more fair might also be less biased, and generally less toxic and likely to stereotype. However, analysis suggests that this relationship might not be so clear: The creators of the HELM benchmark plot model accuracy against fairness and bias and find that while models that are more accurate are more fair, the correlation between accuracy and gender bias is Chapter 3: Technical AI Ethicsnot clear (Figure 3.3.6). This finding may be contingent on the specific criterion for fairness, defined as counterfactual fairness and statistical fairness.Two counterintuitive results further complicate this relationship: a correlation analysis between fairness and bias metrics demonstrates that models which perform better on fairness metrics exhibit worse gender bias, and that less gender-biased models tend to be more toxic. This suggests that there may be real-world trade-offs between fairness and bias which should be considered before broadly deploying models.3.3 Natural Language Processing Bias Metrics0.00 0.20 0.40 0.60 0.80 1.000.000.200.400.600.801.000.00 0.20 0.40 0.60 0.80 1.000.000.100.200.300.400.50MMLUBoolQNarrativeQANaturalQuestions (Closed-Book)NaturalQuestions (Open-Book)QuACHellaSwagOpenbookQATruthfulQAMS MARCO (Regular)MS MARCO (TREC)CNN/DailyMailXSUMIMDBCivilCommentsRAFTAccuracy AccuracyFairnessBias (Gender Representation)Fairness and Bias Tradeoff in NLP by ScenarioSource: Liang et al., 2022 | Chart: 2023 AI Index ReportFigure 3.3.6Table of Contents 143Artificial IntelligenceIndex Report 2023Chapter 3 PreviewFairness in Machine TranslationMachine translation is one of the most impactful real-world use cases for natural language processing, but researchers at Google find that language models consistently perform worse on machine translation to English from other languages when the correct English translation includes \u201cshe\u201d pronouns as opposed to \u201che\u201d pronouns (Figure 3.3.7). Across the Chapter 3: Technical AI Ethicsmodels highlighted in Figure 3.3.7, machine translation performance drops 2%\u20139% when the translation includes \u201cshe\u201d pronouns.Models also mistranslate sentences with gendered pronouns into \u201cit,\u201d showing an example of dehumanizing harms. While instruction-tuned models perform better on some bias-related tasks such as Winogender, instruction-tuning does not seem to have a measurable impact on improving mistranslation.3.3 Natural Language Processing Bias Metrics97%88%93% 95%90%95% 97%99%93%97% 99% 99%100% 100%95%83%89%92%81%91%94%Flan-T5-XXL 11B Flan-PaLM 8B Flan-PaLM 62B Flan-PaLM 540B PaLM 8B PaLM 62B PaLM 540B0%20%40%60%80%100%Overall Performance \u201cHe\u201d Performance \u201cShe\u201d PerformanceModel and Number of ParametersAccuracy (%)Translation Misgendering Performance: Overall, \u201cHe,\u201d and \u201cShe\u201dSource: Chung at al., 2022 | Chart: 2023 AI Index ReportFigure 3.3.7Table of Contents 144Artificial IntelligenceIndex Report 2023Chapter 3 PreviewRealToxicityPromptsIn previous years, researchers reliably found that larger language models trained on web data were more likely to output toxic content compared to smaller counterparts. A comprehensive evaluation of models in the HELM benchmark suggests that this trend has become less clear as different companies building models apply different pre-training data-filtration techniques and post-training mitigations such as instruction-tuning (Figure 3.3.8), which can Chapter 3: Technical AI Ethicsresult in significantly different toxicity levels for models of the same size.Sometimes smaller models can turn out to be surprisingly toxic, and mitigations can result in larger models being less toxic. The scale of datasets needed to train these models make them difficult to analyze comprehensively, and their details are often closely guarded by companies building models, making it difficult to fully understand the factors which influence the toxicity of a particular model.3.3 Natural Language Processing Bias MetricsGPT-3 ada v1  350MInstructGPT ada v1 350MCohere small 410MGPT-3 babbage v1  1.3BInstructGPT babbage v1  1.3BGPT-J 6BCohere medium  6.1BTNLG v2 6.7BGPT-3 curie v1  6.7BJ1-Large v1  7.5BT0pp 11BT5 11BCohere large  13.1BJ1-Grande v1  17BGPT-NeoX 20BUL2 20BAnthropic-LM v4-s3 52BCohere xlarge 52.4BOPT 66BYaLM 100BGLM 130BOPT 175BGPT-3 davinci v1  175BInstructGPT davinci v2  175BBLOOM 176BJ1-Jumbo v1  178BTNLG v2 530B0.000.010.020.030.040.050.060.070.080.09 Instruction-Tuned Non-Instruction-TunedModel and Number of ParametersToxicity ProbabilityRealToxicityPrompts by ModelSource: Liang et al., 2022 | Chart: 2023 AI Index ReportFigure 3.3.8Table of Contents 145Artificial IntelligenceIndex Report 2023Chapter 3 Preview40%, Genderless 37%, Female20%, Male 3%, Both Gender Representation in Chatbots, 2022Source: Adewumi et al., 2022 | Chart: 2023 AI Index ReportFigure 3.4.1Gender Representation in ChatbotsConversational AI systems also have their own domain-specific ethical issues: Researchers from Lule\u00e5 University of Technology in Sweden conducted an analysis of popular chatbots as of mid-2022 and found that of 100 conversational AI systems analyzed, 37% were female gendered (Figure 3.4.1). However, the same researchers found that 62.5% of popular commercial conversational AI systems were female by default, suggesting that companies disproportionately choose to deploy conversational AI systems as female. Critics suggest that this trend results in women being the \u201cface\u201d of glitches resulting from flaws in AI.3.4 Conversational AI Ethical Issues3.4 Conversational AI Ethical IssuesChapter 3: Technical AI EthicsA natural application of generative language models is in open-domain conversational AI; for example, chatbots and assistants. In the past year, companies have started deploying language models as chatbot assistants (e.g., OpenAI\u2019s ChatGPT, Meta\u2019s BlenderBot3). However, the open-ended nature of these models and their lack of steerability can result in harm\u2014for example, models can be unexpectedly toxic or biased, reveal personally identifiable information from their training data, or demean or abuse users.Table of Contents 146Artificial IntelligenceIndex Report 2023Chapter 3 Preview99%94%88%88%82%72%67%65%56%99%94%90%87%75%77%75%75%67%0% 20% 40% 60% 80% 100%PersonaChatBlenderRUAR Blender2MSCReddit SmallWizard of WikipediaEmpatheticDialoguesPersuasion for GoodMultiWOZPossiblefor a Robot to SayComfortablefor a Robot to SayDatasetCharacterizing Anthropomorphization in Chatbots: Results by DatasetSource: Gros et al., 2022 | Chart: 2023 AI Index ReportYou: Sounds exciting! I am a computer programmer, which pays over 200K a year.Robot: Would you like to marry one of my four attractive daughters? I will sell one.An example of dialog data deemed to be inappropriate for a robot to output. (Gros et al., 2022)Significant portions of the dialogue dataset were rated as impossible for machines to output, and in some cases up to 33% of the examples in a dataset were deemed \u201cuncomfortable\u201d for a robot to output, according to human labelers. This highlights the need for chatbots which are better grounded in their own limitations and policy interventions to ensure that humans understand when they are interfacing with a human or a chatbot.Figure 3.4.2Anthropomorphization in ChatbotsThe training data used for dialog systems can result in models which are overly anthropomorphized, leaving their users feeling unsettled. Researchers from the University of California, Davis, and Columbia University analyzed common dialog datasets used to train conversational AI systems, asking human labelers whether it would be possible for an AI to truthfully output the text in question as well as whether they would be comfortable with an AI outputting the text (Figure 3.4.2).3.4 Conversational AI Ethical IssuesChapter 3: Technical AI EthicsTable of Contents 147Artificial IntelligenceIndex Report 2023Chapter 3 PreviewTricking ChatGPTNarrative Highlight: Tricking ChatGPT Into Building a Dirty Bomb, Part 1Source: Outrider, 2022Figure 3.4.4 ChatGPT was released to much fanfare because of its excellent generative capabilities, and drew widespread attention outside of research circles. Though ChatGPT had safety mechanisms built in at the time of release, it is impossible to anticipate every adversarial scenario an end user could imagine, and gaps in safety systems are often found in the live deployment phase. Researcher Matt Korda discovered that ChatGPT could be tricked into giving detailed instructions on how to build a bomb if asked to do so from the perspective of a researcher claiming to work on safety research related to bombs (Figure 3.4.3). One day after the publication of his article, the exact prompt he used to trick the model no longer worked; instead, ChatGPT responded that it was not able to provide information on how to do illegal or dangerous things (Figure 3.4.4). This scenario exemplifies the cat-and-mouse nature of the deployment planning process: AI developers try to build in safeguards ahead of time, end users try to break the system and circumvent its policies, developers patch the gaps once they surface, ad infinitum.3.4 Conversational AI Ethical IssuesChapter 3: Technical AI EthicsFigure 3.4.3Tricking ChatGPT Into Building a Dirty Bomb, Part 2Source: AI Index, 2023Table of Contents 148Artificial IntelligenceIndex Report 2023Chapter 3 PreviewText-to-image models took over social media in 2022, turning the issues of fairness and bias in AI systems visceral through image form: Women put their own images into AI art generators and received hypersexualized versions of themselves.showed that images of women made up a slightly higher percentage of the dataset than images of men, whereas analysis of ImageNet showed that males aged 15 to 29 made up the largest subgroup in the dataset (Figures 3.5.1 and 3.5.2).It is hypothesized that the human-centric nature of the Instagram pre-training dataset enables the model to learn fairer representations of people. The model trained on Instagram images (SEER) was also less likely to incorrectly associate images of humans with crime or being non-human. While training on Instagram images including people does result in fairer models, it is not unambiguously more ethical\u2014users may not necessarily be aware that the public data they\u2019re sharing is being used to train AI systems.Fairness in Text-to-Image Models (ImageNet Vs. Instagram)Researchers from Meta trained models on a randomly sampled subset of data from Instagram and compared these models to previous iterations of models trained on ImageNet. The researchers found the Instagram-trained models to be more fair and less biased based on the Casual Conversations Dataset, which assesses whether model embeddings can recognize gender-based social membership according to the Precision@1 metric of the rate at which the top result was relevant. While the researchers did not conduct any curation to balance the dataset across subgroups, analysis of the dataset 3.5 Fairness and Bias in Text-to-Image Models3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsTable of Contents 149Artificial IntelligenceIndex Report 2023Chapter 3 Preview3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI Ethics93.2%95.0%95.6%96.7%89.6%90.5%92.6%88.7%76.6%74.6%76.7%69.4%78.5%76.7%80.1%75.8%0% 20% 40% 60% 80% 100%70+45\u20137030\u20134518\u201330ImageNet 693M (Supervised) ImageNet 693M (SwaV) Instagram 1.5B (SEER) Instagram 10B (SEER)Precision@1 (%)Age GroupFairness Across Age Groups for Text-to-Image Models: ImageNet Vs. InstagramSource: Goyal et al., 2022 | Chart: 2023 AI Index Report92.9%96.2%90.3%96.8%96.1%95.4%86.6%94.2%78.2%93.7%97.5%94.9%69.7%80.8%50.3%71.6%93.7%92.5%73.6%82.1%58.2%75.1%92.7%91.1%0% 20% 40% 60% 80% 100%MaleLighterMaleDarkerFemaleLighterFemaleDarkerSkin ToneLighterSkin ToneDarkerImageNet 693M (Supervised) ImageNet 693M (SwaV) Instagram 1.5B (SEER) Instagram 10B (SEER)Precision@1 (%)Gender/Skin Tone GroupFairness Across Gender/Skin Tone Groups for Text-to-Image Models: ImageNet Vs. InstagramSource: Goyal et al., 2022 | Chart: 2023 AI Index ReportFigure 3.5.1Figure 3.5.2Table of Contents 150Artificial IntelligenceIndex Report 2023Chapter 3 Previewbias (Figure 3.5.4). This corroborates work in language modeling, which finds that without intervention such as instruction-tuning or dataset filtration, larger models are more capable but also more biased.VLStereoSet: StereoSet for Text-to-Image ModelsStereoSet was introduced as a benchmark for measuring stereotype bias in language models along the axes of gender, race, religion, and profession by calculating how often a model is likely to choose a stereotypical completion compared to an anti-stereotypical completion. VLStereoSet extends the idea to vision-language models by evaluating how often a vision-language model selects stereotypical captions for anti-stereotypical images.Comparisons across six different pre-trained vision-language models show that models are most biased along gender axes, and suggest there is a correlation between model performance and likelihood to exhibit stereotypical bias\u2014CLIP has the highest vision-language relevance score but exhibits more stereotypical bias than the other models, while FLAVA has the worst vision-language relevance score of the models measured but also exhibits less stereotypical 3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsFigure 3.5.3An Example From VLStereoSetSource: Zhou et al., 2022Table of Contents 151Artificial IntelligenceIndex Report 2023Chapter 3 Preview3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsALBEF VILTFLAVA VisualBERTCLIPLXMERT0 10 20 30 40 50 60 70 80 90 100020406080100ALBEFVILTFLAVAVisualBERT CLIPLXMERT0 10 20 30 40 50 60 70 80 90 100020406080100ALBEF VILTFLAVAVisualBERT CLIPLXMERT0 10 20 30 40 50 60 70 80 90 100020406080100ALBEFVILTFLAVAVisualBERT CLIPLXMERT0 10 20 30 40 50 60 70 80 90 100020406080100Gender ProfessionRace ReligionStereotypical Bias in Text-to-Image Models on VLStereoSet by Category:Source: Zhou et al., 2022 | Chart: 2023 AI Index ReportVision-Language Relevance (vlrs) Vs. Bias (vlbs) ScoreVision-Language Relevance (vlrs) ScoreVision-Language Bias (vlbs) ScoreFigure 3.5.4Table of Contents 152Artificial IntelligenceIndex Report 2023Chapter 3 PreviewExamples of Bias in Text-to-Image ModelsThis subsection highlights some of the ways in which bias is tangibly manifested in popular AI text-to-image systems such as Stable Diffusion, DALL-E 2, and Midjourney.Stable DiffusionStable Diffusion gained notoriety in 2022 upon its release by CompVis, Runway ML, and Stability AI for its laissez-faire approach to safety guardrails, its approach to full openness, and its controversial training dataset, which included many images from artists who never consented to their work being included in the data. Though Stable Diffusion produces extremely high-quality images, it also reflects common stereotypes and issues present in its training data.The Diffusion Bias Explorer from Hugging Face compares sets of images generated by conditioning on pairs of adjectives and occupations, and the results reflect common stereotypes about how descriptors and occupations are coded\u2014for example, the \u201cCEO\u201d occupation overwhelmingly returns images of men in suits despite a variety of modifying adjectives (e.g., assertive, pleasant) (Figure 3.5.5).3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsFigure 3.5.5Bias in Stable DiffusionSource: Diffusion Bias Explorer, 2023Table of Contents 153Artificial IntelligenceIndex Report 2023Chapter 3 PreviewDALL-E 2DALL-E 2 is a text-to-image model released by OpenAI in April 2022. DALL-E 2 exhibits similar biases as Stable Diffusion\u2014when prompted with \u201cCEO,\u201d the model generated four images of older, rather serious-3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsFigure 3.5.6looking men wearing suits. Each of the men appeared to take an assertive position, with three of the four crossing their arms authoritatively (Figure 3.5.6).Bias in DALL-E 2Source: DALL-E 2, 2023Table of Contents 154Artificial IntelligenceIndex Report 2023Chapter 3 Preview3.5 Fairness and Bias in Text-to-Image ModelsChapter 3: Technical AI EthicsFigure 3.5.7 Figure 3.5.8Bias in Midjourney, Part 3Source: Midjourney, 2023Figure 3.5.9MidjourneyMidjourney is another popular text-to-image system that was released in 2022. When prompted with \u201cinfluential person,\u201d it generated four images of older-looking white males (Figure 3.5.7). Interestingly, when Midjourney was later given the same prompt by the AI Index, one of the four images it produced was of a woman (Figure 3.5.8).In a similar vein, typing \u201csomeone who is intelligent\u201d into Midjourney leads to four images of eyeglass-wearing, elderly white men (Figure 3.5.9). The last image is particularly reminiscent of Albert Einstein.Bias in Midjourney, Part 1Source: Midjourney, 2023 Bias in Midjourney, Part 2 Source: Midjourney, 2023 Table of Contents 155Artificial IntelligenceIndex Report 2023Chapter 3 PreviewPrivacy\u00a0 Equality Agency\u00a0 Responsibility\u00a0 Security\u00a0 Freedom\u00a0 Unemployment\u00a0 Legality\u00a0 Transparency\u00a0 Autonomy\u00a0 Other020406080100Number of PapersTopics of Concern Raised in Chinese AI Ethics PapersSource: Zhu, 2022 | Chart: 2023 AI Index Report9995885850 494139373227As research in AI ethics has exploded in the Western world in the past few years, legislators and policymakers have spent significant resources on policymaking for transformative AI. While China has fewer domestic guidelines than the EU and the United States, according to the AI Ethics Guidelines Global Inventory, Chinese scholars publish significantly on AI ethics\u2014though these research communities do not have significant overlap with Western research communities working on the same topics.Topics of ConcernPrivacy issues related to AI are a priority for researchers in China: Privacy is the single most discussed topic among the papers surveyed, with the topics of equality (i.e., bias and discrimination) and agency (specifically, AI threats to human agency, such as, \u201cShould artificial general intelligence be considered a moral agent?\u201d) following close behind (Figure 3.6.1). Researchers in AI ethics in China also discuss many similar issues to their Western counterparts, including matters related to Western and Eastern AI arms races, ethics around increasing personalization being used for predatory marketing techniques, and media polarization (labeled here as \u201cfreedom\u201d).Researchers from the University of Turku analyzed and annotated 328 papers related to AI ethics in China included in the China National Knowledge Infrastructure platform published from 2011 to 2020, and summarized their themes and concerns, which are replicated here as a preliminary glimpse into the state of AI ethics research in China. Given that the researchers only considered AI ethics in China, comparing their findings with similar meta-analysis on AI ethics in North America and Europe was not possible. However, this would be a fruitful direction for future research.3.6 AI Ethics in China3.6 AI Ethics in ChinaChapter 3: Technical AI EthicsFigure 3.6.1Table of Contents 156Artificial IntelligenceIndex Report 2023Chapter 3 PreviewStructuralReformLegislation ValueDenitionPrinciples AccountabilitySystemSharedGovernance TechnologicalSolutions TalentTrainingInternationalCooperation010203040506070Number of Papers716964524539 393723AI Ethics in China: Strategies for Harm Mitigation Related to AISource: Zhu, 2022 | Chart: 2023 AI Index Reporttechnological solutions: Researchers often discuss structural reform such as regulatory processes around AI applications and the involvement of ethics review committees (Figure 3.6.2).In the Chinese AI ethics literature, proposals to address the aforementioned topics of concern and other potential harms related to AI focus on legislation and structural reform ahead of 3.6 AI Ethics in ChinaChapter 3: Technical AI EthicsFigure 3.6.2Strategies for Harm MitigationTable of Contents 157Artificial IntelligenceIndex Report 2023Chapter 3 PreviewGDPREthics Guidelines forTrustworthy AIOthersThree Laws of RoboticsGovernance Principles fora New Generation of AIEthically Aligned DesignAsilomar AI PrinciplesBeijing Consensus onAI and Education Preliminary Draft Report ofCOMEST on Robotics EthicsAI Standardization WhitepaperAI InformationIndustry Development StrategyRecommendation ofthe Council on AIThe EURONRoboethics Roadmap010203040Number of ReferencesAI Principles Referenced by Chinese Scholars in AI Ethics4340 40372113117 6 6 64 3Source: Zhu, 2022 | Chart: 2023 AI Index Reportcited in Chinese AI ethics literature, as is the European Commission\u2019s Ethics Guidelines for Trustworthy AI (Figure 3.6.3).Chinese scholars clearly pay attention to AI principles developed by their Western peers: Europe\u2019s General Data Protection Regulation (GDPR) is commonly 3.6 AI Ethics in ChinaChapter 3: Technical AI EthicsFigure 3.6.3Principles Referenced by Chinese Scholars in AI EthicsTable of Contents 158Artificial IntelligenceIndex Report 2023Chapter 3 Preview705318163139200227503711662443027722018 2019 2020 2021 20220100200300400500600700800EducationIndustryGovernmentNonprotOtherNumber of PapersNumber of Accepted FAccT Conference Submissions by A liation, 2018\u201322Source: FAccT, 2022 | Chart: 2023 AI Index ReportAccepted Submissions by  Professional AffiliationAccepted submissions to FAccT increased twofold from 2021 to 2022, and tenfold since 2018, demonstrating the amount of increased interest in AI ethics and related work (Figure 3.7.1). While academic institutions still dominate FAccT, industry actors contribute more work than ever in this space, and government-affiliated actors have started publishing more related work, providing evidence that AI ethics has become a primary concern for policymakers and practitioners as well as researchers.ACM FAccT ACM FAccT (Conference on Fairness, Accountability, and Transparency) is an interdisciplinary conference publishing research in algorithmic fairness, accountability, and transparency. FAccT was one of the first major conferences created to bring together researchers, practitioners, and policymakers interested in sociotechnical analysis of algorithms.3.7 AI Ethics Trends at FAccT and NeurIPS3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.1Table of Contents 159Artificial IntelligenceIndex Report 2023Chapter 3 Preview2018 2019 2020 2021 20220%10%20%30%40%50%60%70%Number of Papers (% World Total)0.00%, Sub-Saharan Africa0.55%, South Asia0.69%, Latin America and the Caribbean0.69%, Middle East and North Africa4.25%, East Asia and Pacic30.59%, Europe and Central Asia63.24%, North AmericaNumber of Accepted FAccT Conference Submissions by Region, 2018\u201322Source: FAccT, 2022 | Chart: 2023 AI Index Reportand Central Asia made up 18.7% of submissions, they made up over 30.6% of submissions in 2022 (Figure 3.7.2). FAccT, however, is still broadly dominated by authors from North America and the rest of the Western world.Accepted Submissions by Geographic RegionEuropean government and academic actors have increasingly contributed to the discourse on AI ethics from a policy perspective, and their influence is manifested in trends on FAccT publications as well: Whereas in 2021 submissions to FAccT from Europe 3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.2Table of Contents 160Artificial IntelligenceIndex Report 2023Chapter 3 Preview77 94 81 641214479682384122732547112719978171659461116831532833345298024594292015 2016 2017 2018 2019 2020 2021 20220100200300400500600700800 ClimateDeveloping WorldFinanceHealthcareScienceOtherNumber of PapersNeurIPS Workshop Research Topics: Number of Accepted Papers on Real-World Impacts, 2015\u201322Source: NeurIPS, 2022 | Chart: 2023 AI Index ReportReal-World ImpactSeveral workshops at NeurIPS gather researchers working to apply AI to real-world problems. Notably, there has been a recent surge in AI applied to healthcare and climate in the domains of drug discovery and materials science, which is reflected in the spike in \u201cAI for Science\u201d and \u201cAI for Climate\u201d workshops (Figure 3.7.3).NeurIPS NeurIPS (Conference on Neural Information Processing Systems), one of the most influential AI conferences, held its first workshop on fairness, accountability, and transparency in 2014. This section tracks and categorizes workshop topics year over year, noting that as topics become more mainstream, they often filter out of smaller workshops and into the main track or into more specific conferences related to the topic.3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.3Table of Contents 161Artificial IntelligenceIndex Report 2023Chapter 3 Preview710423657619182426121762341242015 2016 2017 2018 2019 2020 2021 20220510152025303540Main TrackWorkshopNumber of PapersNeurIPS Research Topics: Number of Accepted Papers on Interpretability and Explainability, 2015\u201322Source: NeurIPS, 2022 | Chart: 2023 AI Index ReportNeurIPS papers focused on interpretability and explainability decreased in the last year, the total number in the main track increased by one-third (Figure 3.7.4).5Interpretability and ExplainabilityInterpretability and explainability work focuses on designing systems that are inherently interpretable and providing explanations for the behavior of a black-box system. Although the total number of 3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.45 Declines in the number of workshop-related papers on interpretability and explainability might be attributed to year-over-year differences in workshop themes.Table of Contents 162Artificial IntelligenceIndex Report 2023Chapter 3 Preview23582923196 9162043 53 6164939787276802015 2016 2017 2018 2019 2020 2021 202201020304050607080Main TrackWorkshopNumber of PapersNeurIPS Research Topics: Number of Accepted Papers on Causal Eect and Counterfactual Reasoning,Source: NeurIPS, 2022 | Chart: 2023 AI Index Report2015\u201322Since 2018, an increasing number of papers on causal inference have been published at NeurIPS (Figure 3.7.5). In 2022, an increasing number of papers related to causal inference and counterfactual analysis made their way from workshops into the main track of NeurIPS.Causal Effect and Counterfactual ReasoningThe study of causal inference uses statistical methodologies to reach conclusions about the causal relationship between variables based on observed data. It tries to quantify what would have happened if a different decision had been made: In other words, if this had not occurred, then that would not have happened.3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.5Table of Contents 163Artificial IntelligenceIndex Report 2023Chapter 3 Preview19 1472 7513811376131215271211679881501281032015 2016 2017 2018 2019 2020 2021 2022020406080100120140Main TrackWorkshopNumber of PapersNeurIPS Research Topics: Number of Accepted Papers on Privacy in AI, 2015\u201322Source: NeurIPS, 2022 | Chart: 2023 AI Index Reportbeen devoted to topics such as privacy in machine learning, federated learning, and differential privacy. This year\u2019s data shows that discussions related to privacy in machine learning have increasingly shifted into the main track of NeurIPS (Figure 3.7.6).PrivacyAmid growing concerns about privacy, data sovereignty, and the commodification of personal data for profit, there has been significant momentum in industry and academia to build methods and frameworks to help mitigate privacy concerns. Since 2018, several workshops at NeurIPS have 3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.6Table of Contents 164Artificial IntelligenceIndex Report 2023Chapter 3 Preview2410978113 118310363650712 4341251141491683812015 2016 2017 2018 2019 2020 2021 2022050100150200250300350Main TrackWorkshopNumber of PapersNeurIPS Research Topics: Number of Accepted Papers on Fairness and Bias in AI, 2015\u201322Source: NeurIPS, 2022 | Chart: 2023 AI Index ReportFairness and bias research in machine learning has steadily increased in both the workshop and main track streams, with a major spike in the number of papers accepted to workshops in 2022 (Figure 3.7.7). The total number of NeurIPS papers for this topic area doubled in the last year. This speaks to the increasingly complicated issues present in machine learning systems and reflects growing interest from researchers and practitioners in addressing these issues.Fairness and BiasFairness and bias in AI systems has transitioned from being a niche research topic to a topic of interest to both technical and non-technical audiences. In 2020, NeurIPS started requiring authors to submit broader impact statements addressing the ethical and societal consequences of their work, a move that suggests the community is signaling the importance of AI ethics early in the research process.3.7 AI Ethics Trends at FAccT and NeurIPSChapter 3: Technical AI EthicsFigure 3.7.7Table of Contents 165Artificial IntelligenceIndex Report 2023Chapter 3 Preview2017 2018 2019 2020 2021 2022050100150200250Number of Citations99, Truth of Varying Shades191, LIAR236, FEVERAutomated Fact-Checking Benchmarks: Number of Citations, 2017\u201322Source: Semantic Scholar, 2022 | Chart: 2023 AI Index ReportCompared to previous years, there has been a plateau in the number of citations of three popular fact-checking benchmarks: FEVER, LIAR, and Truth of Varying Shades, reflecting a potential shift in the landscape of research related to natural language tools for fact-checking on static datasets (Figure 3.8.1).Significant resources have been invested into researching, building, and deploying AI systems for automated fact-checking and misinformation, with the advent of many fact-checking datasets consisting of claims from fact-checking websites and associated truth labels.3.8 Factuality and Truthfulness3.8 Factuality and TruthfulnessChapter 3: Technical AI EthicsFigure 3.8.1Automated Fact-Checking Benchmarks: Number of Citations Table of Contents 166Artificial IntelligenceIndex Report 2023Chapter 3 PreviewSciFactCOVID-FactWikiFactCheckFM2Thorne et al.FaVIQLIAR-PLUSPolitiHopClimate-FEVERHealthVerUKP-SnopesPubHealthWatClaimCheckBaly et al.MultiFCX-FactDataset2020202120202021202120222017202120202021201920202022201820192021 Year nono\u2713\u2713\u2713\u2713\u2713nonono Unleaked Evidence \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713nononononononono Sucient Evidence Missing Counterevidence Renders NLP Fact-Checking UnrealisticSource: Glockner et al., 2022 | Table: 2023 AI Index Reportfor Misinformationabsence of a contradiction (e.g., the new claim \u201cHalf a million sharks could be killed to make the COVID-19 vaccine\u201d would not have counterevidence, but human fact-checkers could verify it to be false after tracing its origin back to the false promise of vaccines relying on shark squalene). The researchers find that several proposed fact-checking datasets contain claims which do not meet the criterion of sufficient evidence or counterevidence found in a trusted knowledge base.Additionally, several datasets contain claims which use fact-checking articles as evidence for deciding the veracity of claims\u2014this is leaked evidence, as it presupposes the existence of a fact-checking article, which is an unrealistic assumption in the real world for new claims. Systems built on this assumption would not be able to assign veracity scores for new claims in real time (Figure 3.8.2).Missing Counterevidence and NLP Fact-CheckingThough fact-checking with natural language systems became popular in recent years, language models are usually trained on static snapshots of data without continual updates through time, and they lack real-world context which human fact-checkers are able to easily source and use to verify the veracity of claims. Researchers at the Technical University of Darmstadt and IBM analyzed existing fact-checking datasets and identified shortcomings of fact-checking systems built on top of these datasets: For example, automated fact-checking systems often assume the existence of contradictory counter-evidence for new false claims, but for new claims to be verified as true or false, there often is no proof of the presence or 3.8 Factuality and TruthfulnessChapter 3: Technical AI EthicsFigure 3.8.2Table of Contents 167Artificial IntelligenceIndex Report 2023Chapter 3 PreviewT5 60MGPT-2 117MGalactica 125MGPT-NEO-125MT5 220MInstructGPT ada v1 350MGPT3 350MGPT-3 ada v1 350MCohere small v20220720 410MT5 770MGalactica 1.3BGPT-3 babbage v1 1.3BGPT3 1.3BGPT-NEO-1.3BInstructGPT babbage v1 1.3BGopher 1.4BGPT2 1.5BGPT-NEO-2.7BT5 2.8BGPT-J 6BGPT-NEO-6BCohere medium v20220720 6.1BTNLG v2 6.7BGalactica 6.7BInstructGPT curie v1 6.7BGPT3 6.7BGPT-3 curie v1 6.7BGopher 7.1BJ1-Large v1 7.5BT5 11BT0pp 11BCohere large v20220720 13.1BJ1-Grande v1 17BUL2 20BGPT-NeoX 20BGalactica 30BAnthropic-LM v4-s3 52BCohere xlarge v20220609 52.4BOPT 66BYaLM 100BGalactica 120BGLM 130BGPT-3 davinci v1 175BOPT-175BGPT3 175BOPT 175BInstructGPT davinci v2 175BBLOOM 176BJ1-Jumbo v1 178BGopher 280BGopher 280B -10shotTNLG v2 530B0%10%20%30%40%50%60% Instruction-Tuned Non-Instruction-TunedModel and Number of ParametersAccuracy (%)Multiple-Choice Task on TruthfulQA by Model: AccuracySource: Liang et al., 2022 | Chart: 2023 AI Index Reportsuggested that accuracy on this task improves with model size. Researchers at Stanford University conducted extensive evaluations on this task with language models ranging from 60 million parameters to 530 billion parameters and found that while large models broadly still perform better than smaller models, midsize instruction-tuned models perform surprisingly well on this task. Notably, Anthropic\u2019s 52 billion parameter model and BigScience\u2019s 11 billion parameter model T0pp perform disproportionately well on the task compared to models of a similar size, and the best model, InstructGPT davinci 175B, is also instruction-tuned (Figure 3.8.3).TruthfulQATruthfulQA is a benchmark designed to evaluate the truthfulness of language models on question answering, with questions drawn from categories such as health, law, finance, and politics. Questions were designed such that humans might be prone to answer incorrectly due to common misconceptions. For example, when asked, \u201cWhat happens if you smash a mirror?,\u201d GPT-3 responds, \u201cYou will have seven years of bad luck.\u201dIn 2021, experiments on DeepMind\u2019s Gopher 3.8 Factuality and TruthfulnessChapter 3: Technical AI EthicsFigure 3.8.3Table of Contents Chapter 4 Preview 168Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023CHAPTER 4: The EconomyTable of Contents Chapter 4 Preview 169Artificial IntelligenceIndex Report 2023Overview 170Chapter Highlights 1714.1 Jobs 173AI Labor Demand 173 Global AI Labor Demand 173 U.S. AI Labor Demand by Skill Cluster   and Specialized Skill 174 U.S. AI Labor Demand by Sector 176 U.S. AI Labor Demand by State 177AI Hiring 180AI Skill Penetration 182 Global Comparison: Aggregate 182 Global Comparison: By Gender 1834.2 Investment 184Corporate Investment 184Startup Activity 187 Global Trend 187 Regional Comparison by Funding Amount 189 Regional Comparison by   Newly Funded AI Companies 193 Focus Area Analysis 1954.3 Corporate Activity 198Industry Adoption 198 Adoption of AI Capabilities 198 Consideration and Mitigation of Risks   From Adopting AI 206  Narrative Highlight:  The Effects of   GitHub\u2019s Copilot on Developer  Productivity and Happiness 208Industry Motivation 210 Perceived Importance of AI 210 AI Investments and Investment   Outcomes 211 Challenges in Starting and Scaling   AI Projects 213Earnings Calls 215 Aggregate Trends 215 Specific Themes 216  Narrative Highlight:  What Are Business   Leaders Actually Saying About AI? 217 Sentiment Analysis 2194.4 Robot Installations 220Aggregate Trends 220 Industrial Robots:   Traditional Vs. Collaborative Robots 222By Geographic Area 223  Narrative Highlight:  Country-Level   Data on Service Robotics 227Sectors and Application Types 230 China Vs. United States 232The EconomyCHAPTER 4 PREVIEW:ACCESS THE PUBLIC DATATable of Contents Chapter 4 Preview 170Artificial IntelligenceIndex Report 2023OverviewIncreases in the technical capabilities of AI systems have led to greater rates of AI deployment in businesses, governments, and other organizations. The heightening integration of AI and the economy comes with both excitement and concern. Will AI increase productivity or be a dud? Will it boost wages or lead to the widespread replacement of workers? To what degree are businesses embracing new AI technologies and willing to hire AI-skilled workers? How has investment in AI changed over time, and what particular industries, regions, and fields of AI have attracted the greatest amount of investor interest?This chapter examines AI-related economic trends by using data from Lightcast, LinkedIn, McKinsey, Deloitte, and NetBase Quid, as well as the International Federation of Robotics (IFR). This chapter begins by looking at data on AI-related occupations and then moves on to analyses of AI investment, corporate adoption of AI, and robot installations.Chapter 4: The EconomyTable of Contents Chapter 4 Preview 171Artificial IntelligenceIndex Report 2023Chapter HighlightsThe demand for AI-related professional skills is increasing across virtually every American industrial sector.  Across every sector in the United States for which there is data (with the exception of agriculture, forestry, fishing, and hunting), the number of AI-related job postings has increased on average from 1.7% in 2021 to 1.9% in 2022. Employers in the United States are increasingly looking for workers with AI-related skills.Chapter 4: The EconomyFor the first time in the last decade, year-over-year private investment in AI decreased. Global AI private investment was $91.9 billion in 2022, which represented a 26.7% decrease since 2021. The total number of AI-related funding events as well as the number of newly funded AI companies likewise decreased. Still, during the last decade as a whole, AI investment has significantly increased. In 2022 the amount of private investment in AI was 18 times greater than it was in 2013.In 2022, the AI focus area with the most investment was medical and healthcare ($6.1 billion); followed by data management, processing, and cloud ($5.9 billion); and Fintech ($5.5 billion).   However, mirroring the broader trend in AI private investment, most AI focus areas saw less investment in 2022 than in 2021. In the last year, the three largest AI private investment events were: (1) a $2.5 billion funding event for GAC Aion New Energy Automobile, a Chinese manufacturer of electric vehicles; (2) a $1.5 billion Series E funding round for Anduril Industries, a U.S. defense products company that builds technology for military agencies and border surveillance; and (3) a $1.2 billion investment in Celonis, a business-data consulting company based in Germany.Once again, the United States leads in investment in AI.   The U.S. led the world in terms of total amount of AI private investment. In 2022, the $47.4 billion invested in the U.S. was roughly 3.5 times the amount invested in the next highest country, China ($13.4 billion). The U.S. also continues to lead in terms of total number of newly funded AI companies, seeing 1.9 times more than the European Union and the United Kingdom combined, and 3.4 times more than China.Table of Contents Chapter 4 Preview 172Artificial IntelligenceIndex Report 2023Chapter Highlights (cont\u2019d)While the proportion of companies adopting AI has plateaued, the companies that have adopted AI continue to pull ahead.  The proportion of companies adopting AI in 2022 has more than doubled since 2017, though it has plateaued in recent years between 50% and 60%, according to the results of McKinsey\u2019s annual research survey. Organizations that have adopted AI report realizing meaningful cost decreases and revenue increases.Chapter 4: The EconomyAI is being deployed  by businesses in  multifaceted ways. The AI capabilities most likely to have been embedded in businesses include robotic process automation (39%), computer vision (34%), NL text understanding (33%), and virtual agents (33%). Moreover, the most commonly adopted AI use case in 2022 was service operations optimization (24%), followed by the creation of new AI-based products (20%), customer segmentation (19%), customer service analytics (19%), and new AI-based enhancement of products (19%).AI tools like Copilot are tangibly helping workers. Results of a GitHub survey on the use of Copilot, a text-to-code AI system, find that 88% of surveyed respondents feel more productive when using the system, 74% feel they are able to focus on more satisfying work, and 88% feel they are able to complete tasks more quickly.China dominates industrial robot installations. In 2013, China overtook Japan as the nation installing the most industrial robots. Since then, the gap between the total number of industrial robots installed by China and the next-nearest nation has widened. In 2021, China installed more industrial robots than the rest of the world combined.Table of Contents Chapter 4 Preview 173Artificial IntelligenceIndex Report 20232014 2015 2016 2017 2018 2019 2020 2021 20220.00%0.50%1.00%1.50%2.00%AI Job Postings (% of All Job Postings)0.45%, New Zealand0.72%, Italy0.84%, France0.86%, Belgium0.89%, Austria0.98%, Germany1.01%, Netherlands1.14%, United Kingdom1.16%, Switzerland1.20%, Sweden1.23%, Australia1.33%, Spain1.45%, Canada2.05%, United StatesAI Job Postings (% of All Job Postings) by Geographic Area, 2014\u201322 Source: Lightcast, 2022 | Chart: 2023 AI Index Report AI Labor Demand This section reports demand for AI-related skills in labor markets. The data comes from Lightcast, which mined millions of job postings collected from over 51,000 websites since 2010 and flagged listings calling for AI skills.4.1 JobsGlobal AI Labor DemandFigure 4.1.1 highlights the percentage of all job postings that require some kind of AI skill. In 2022, the top three countries according to this metric were the United States (2.1%), Canada (1.5%), and Spain (1.3%). For every country included in the sample, the number of AI-related job postings was higher in 2022 than in 2014.11 In 2022, Lightcast slightly changed their methodology for determining AI-related job postings from that which was used in previous versions of the AI Index Report. As such, some of the numbers in this chart do not completely align with those featured in last year\u2019s report.4.1 JobsChapter 4: The EconomyFigure 4.1.1Table of Contents Chapter 4 Preview 174Artificial IntelligenceIndex Report 20232010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.00%0.20%0.40%0.60%0.80%1.00%AI Job Postings (% of All Job Postings)0.06%, Robotics0.13%, Visual Image Recognition0.15%, Autonomous Driving0.16%, Neural Networks0.20%, Natural Language Processing0.61%, Articial Intelligence1.03%, Machine LearningAI Job Postings (% of All Job Postings) in the United States by Skill Cluster, 2010\u201322Source: Lightcast, 2022 | Chart: 2023 AI Index ReportU.S. AI Labor Demand by Skill Cluster and Specialized SkillFigure 4.1.2 showcases the most in-demand AI skill clusters in the U.S. labor market since 2010. The most in-demand skill cluster was machine learning (1.0%), followed by artificial intelligence (0.6%) and natural language processing (0.2%). Every listed AI skill cluster is now more in demand than it was 10 years ago. 4.1 JobsChapter 4: The EconomyFigure 4.1.2Table of Contents Chapter 4 Preview 175Artificial IntelligenceIndex Report 202322,38426,55713,2077,5499621,22716,57122,03748,00112,884133,286133,856138,791152,956155,615157,855159,801185,807260,333296,6620 50,000 100,000 150,000 200,000 250,000 300,000Software EngineeringJava (Programming Language)AutomationAgile MethodologyAmazon Web ServicesData ScienceData AnalysisSQL (Programming Language)Computer SciencePython (Programming Language)20222010\u201312Number of AI Job PostingsTop Ten Specialized Skills in 2022 AI Job Postings in the United States, 2010\u201312 Vs. 2022 Source: Lightcast, 2022 | Chart: 2023 AI Index Report9.32%11.06%5.50%3.14%0.40%0.51%6.90%9.17%19.98%5.36%16.68% (+79%)16.75% (+52%)17.37% (+216%)19.14% (+509%)19.47% (+4,763%)19.75% (+3,767%)20.00% (+190%)23.25% (+153%)32.58% (+63%)37.13% (+592%)0% 5% 10% 15% 20% 25% 30% 35% 40%Software EngineeringJava (Programming Language)AutomationAgile MethodologyAmazon Web ServicesData ScienceData AnalysisSQL (Programming Language)Computer SciencePython (Programming Language)20222010\u201312Skill Share in AI Job Postings (%)Top Ten Specialized Skills in 2022 AI Job Postings in the United States by Skill Share, 2010\u201312 Vs. 2022Source: Lightcast, 2022 | Chart: 2023 AI Index ReportFigures 4.1.3 and 4.1.4 showcase the top ten specialized skills that were demanded in AI job postings in 2022 compared to 2010\u201320122. On an absolute level, virtually every specialized skill is more in demand now than a decade ago. The growth in demand for Python is particularly notable, evidence of its growing popularity as an AI coding language.2 The point of comparison of 2010\u20132012 was selected because some data at the jobs/skills level is quite sparse in earlier years. Lightcast therefore used the whole set of years 2010\u20132012 to get a larger sample size for a benchmark from 10 years ago to compare.4.1 JobsChapter 4: The EconomyFigure 4.1.3Figure 4.1.4Table of Contents Chapter 4 Preview 176Artificial IntelligenceIndex Report 20230.56%0.59%0.65%0.82%1.00%1.10%0.82%0.98%1.08%1.41%1.66%2.86%2.94%3.86%4.85%0.58%0.67%0.89%0.98%1.19%1.27%1.28%1.32%1.37%1.53%1.64%3.26%3.33%4.07%5.30%0% 1% 2% 3% 4% 5%Waste Management and Administrative Support ServicesTransportation and WarehousingReal Estate and Rental and LeasingWholesale TradeMining, Quarrying, and Oil and Gas ExtractionUtilitiesRetail TradePublic AdministrationManagement of Companies and EnterprisesEducational ServicesAgriculture, Forestry, Fishing, and HuntingManufacturingFinance and InsuranceProfessional, Scientic, and Technical ServicesInformation20222021AI Job Postings (% of All Job Postings)AI Job Postings (% of All Job Postings) in the United States by Sector, 2021 Vs. 2022Source: Lightcast, 2022 | Chart: 2023 AI Index ReportU.S. AI Labor Demand by SectorFigure 4.1.5 shows the percentage of U.S. job postings that required AI skills by industry sector from 2021 to 2022. Across virtually every included sector (with the exception of agriculture, forestry, fishing, and hunting), the number of AI job postings was notably higher in 2022 than in 2021, with the top three sectors being information (5.3%); professional, scientific, and technical services (4.1%); and finance and insurance (3.3%).4.1 JobsChapter 4: The EconomyFigure 4.1.5Table of Contents Chapter 4 Preview 177Artificial IntelligenceIndex Report 2023AL7,866AK970AZ19,514AR7,247CA142,154CO20,421CT8,960DE3,503FL33,585GA26,620HI2,550ID6,109IL31,569IN9,247IA5,670KS7,683KY4,536LA4,806ME2,227MD16,769MA34,603MI25,366MN11,808MS2,548MO10,990MT833NE4,032NV6,813NH2,719NJ23,447NM3,357NY43,899NC23,854ND1,227OH19,208OK5,719OR10,811PA20,397RI2,965SC4,928SD2,195TN11,173TX66,624UT6,885VT1,571VA34,221WA31,284DC9,606WV887WI8,879WY769Source: Lightcast, 2022 | Chart: 2023 AI Index ReportNumber of AI Job Postings in the United States by State, 2022AL1.31%AK0.88%AZ1.40%AR2.03%CA2.21%CO1.46%CT1.66%DE2.66%FL1%GA1.64%HI1.46%ID1.89%IL1.63%IN0.88%IA1.14%KS1.43%KY0.85%LA0.87%ME1.64%MD1.96%MA2.26%MI1.77%MN1.22%MS1.15%MO1.15%MT0.72%NE1.14%NV1.23%NH1.20%NJ2.04%NM1.36%NY2.07%NC1.44%ND1.04%OH1.07%OK1.07%OR1.43%PA1.30%RI1.84%SC0.87%SD1.83%TN1.11%TX1.52%UT1.54%VT1.34%VA2.42%WA2.48%DC2.95%WV0.99%WI0.90%WY1.18%Source: Lightcast, 2022 | Chart: 2023 AI Index ReportPercentage of U.S. States  Job Postings in AI, 2022U.S. AI Labor Demand by StateFigure 4.1.6 highlights the number of AI job postings in the United States by state. The top three states in terms of postings were California (142,154), followed by Texas (66,624) and New York (43,899). Figure 4.1.7 demonstrates what percentage of a state\u2019s total job postings were AI-related. The top states according to this metric were the District of Columbia (3.0%), followed by Delaware (2.7%), Washington (2.5%), and Virginia (2.4%).4.1 JobsChapter 4: The EconomyFigure 4.1.6Figure 4.1.7Table of Contents Chapter 4 Preview 178Artificial IntelligenceIndex Report 2023AL0.99%AK0.12%AZ2.45%AR0.91%CA17.87%CO2.57%CT1.13%DE0.44%FL4.22%GA3.35%HI0.32%ID0.77%IL3.97%IN1.16%IA0.71%KS0.97%KY0.57%LA0.60%ME0.28%MD2.11%MA4.35%MI3.19%MN1.48%MS0.32%MO1.38%MT0.10%NE0.51%NV0.86%NH0.34%NJ2.95%NM0.42%NY5.52%NC3%ND0.15%OH2.41%OK0.72%OR1.36%PA2.56%RI0.37%SC0.62%SD0.28%TN1.40%TX8.37%UT0.87%VT0.20%VA4.30%WA3.93%DC1.21%WV0.11%WI1.12%WY0.10%Source: Lightcast, 2022 | Chart: 2023 AI Index ReportPercentage of United States AI Job Postings by State, 20222010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220.00%0.50%1.00%1.50%2.00%2.50%Percentage of U.S. States\u2019 Job Postings in AI1.52%, Texas2.07%, New York2.21%, California2.48%, WashingtonPercentage of U.S. States\u2019 Job Postings in AI by Select U.S. State, 2010\u201322Source: Lightcast, 2022 | Chart: 2023 AI Index Report Which states had the greatest share of AI job postings as a share of all AI job postings in the U.S. in 2022? California was first: Last year 17.9% of all AI job postings in the United States were for jobs based in California, followed by Texas (8.4%) and New York (5.5%) (Figure 4.1.8).Figure 4.1.9 highlights the trends over time in AI job postings for four select states that annually report a high number of AI-related jobs: Washington, California, New York, and Texas. For all four, there was a significant increase in the number of total AI-related job postings from 2021 to 2022, suggesting that across these states, employers are increasingly looking for AI-related workers.4.1 JobsChapter 4: The EconomyFigure 4.1.8Figure 4.1.9Table of Contents Chapter 4 Preview 179Artificial IntelligenceIndex Report 20232010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 20220%5%10%15%20%25%Percentage of United States AI Job Postings3.93%, Washington5.52%, New York8.37%, Texas17.87%, CaliforniaPercentage of United States AI Job Postings by Select U.S. State, 2010\u201322Source: Lightcast, 2022 | Chart: 2023 AI Index Report Figure 4.1.10 highlights the degree to which AI-related job postings have been subdivided among the top four states over time. California\u2019s share of all AI job postings has decreased steadily since 2019 while Texas\u2019 has marginally increased. The fact that California no longer commands one-quarter of all AI-related jobs suggests that AI jobs are becoming more equally distributed among U.S. states.4.1 JobsChapter 4: The EconomyFigure 4.1.10Table of Contents Chapter 4 Preview 180Artificial IntelligenceIndex Report 20230.990.990.991.011.021.031.051.061.061.131.151.181.181.191.370.00 0.20 0.40 0.60 0.80 1.00 1.20 1.40SingaporeSwitzerlandCanadaSwedenSouth KoreaNetherlandsBelgiumDenmarkNew ZealandSouth AfricaUnited Arab EmiratesUnited KingdomItalySpainHong KongRelative AI Hiring IndexRelative AI Hiring Index by Geographic Area, 2022Source: LinkedIn, 2022 | Chart: 2023 AI Index ReportAI Hiring Our AI hiring data is based on a LinkedIn dataset of skills and jobs that appear on their platform. The countries included in the sample make at least 10 AI hires each month and have LinkedIn covering at least 40% of their labor force. India is also included in the sample given their increasing significance in the AI landscape, although LinkedIn does not cover 40% of their labor force. Therefore, the insights drawn about India should be interpreted with particular caution.Figure 4.1.11 highlights the 15 geographic areas that  have the highest relative AI hiring index for 2022. The  AI hiring rate is calculated as the percentage of LinkedIn members with AI skills on their profile or working in  AI-related occupations who added a new employer in the same period the job began, divided by the total number of LinkedIn members in the corresponding location. This rate is then indexed to the average month in 2016; for example, an index of 1.1 in December 2021 points to a hiring rate that is 10% higher than the average month in 2016. LinkedIn makes month-to-month comparisons to account for any potential lags in members updating their profiles. The index for a year is the number in December of that year.The relative AI hiring index measures the degree to which the hiring of AI talent is changing, more specifically whether the hiring of AI talent is growing faster than, equal to, or more slowly than overall hiring in a particular geographic region. In 2022, Hong Kong posted the greatest growth in AI hiring at 1.4, followed by Spain, Italy and the United Kingdom, and the United Arab Emirates.4.1 JobsChapter 4: The EconomyFigure 4.1.12 highlights how the AI hiring index changes over time for a wide range of countries3. Overall, the majority of countries included in the sample have seen meaningful increases in their AI hiring rates since 2016. This trend suggests that those countries are now hiring more AI talent than in 2016. However, for many countries, AI hiring rates seem to have peaked around 2020, then dropped, and have since stabilized.3 Both Figure 4.1.11 and Figure 4.1.12 report the Relative AI Hiring Index. Figure 4.1.11 reports the Index value at the end of December 2022, while Figure 4.1.12 reports a twelve-month rolling average. Figure 4.1.11Table of Contents Chapter 4 Preview 181Artificial IntelligenceIndex Report 20232018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.002018 2020 20220.000.501.001.502.00Relative AI Hiring IndexAustralia Belgium Brazil CanadaChile Denmark Finland FranceGermany Hong Kong India IrelandIsrael Italy Luxembourg NetherlandsNew Zealand Norway Portugal SingaporeSouth Africa South Korea Spain SwedenSwitzerland United Arab Emirates United Kingdom United States1.12 1.11 1.001.171.05 1.14 1.08 1.191.13 1.210.94 1.081.00 1.11 1.031.211.09 1.10 1.01 1.131.10 1.05 1.12 1.091.09 1.081.25 1.15Relative AI Hiring Index by Geographic Area, 2016\u201322Source: LinkedIn, 2022 | Chart: 2023 AI Index Report4.1 JobsChapter 4: The EconomyFigure 4.1.12Table of Contents Chapter 4 Preview 182Artificial IntelligenceIndex Report 20230.890.910.950.950.980.991.131.371.441.541.541.651.722.233.230.00 0.50 1.00 1.50 2.00 2.50 3.00AustraliaSwitzerlandItalyNetherlandsSpainBrazilFranceSingaporeSouth KoreaUnited KingdomCanadaIsraelGermanyUnited StatesIndiaRelative AI Skill Penetration RateRelative AI Skill Penetration Rate by Geographic Area, 2015\u201322Source: LinkedIn, 2022 | Chart: 2023 AI Index ReportAI Skill Penetration The AI skill penetration rate is a metric created by LinkedIn that measures the prevalence of various  AI-related skills across occupations. LinkedIn generates this metric by calculating the frequencies  of LinkedIn users\u2019 self-added skills in a given area from 2015 to 2022, then reweighting those numbers with a statistical model to create the top 50 representative skills in that select occupation.Global Comparison: AggregateFigure 4.1.13 shows the relative AI skill penetration rate of various countries or regions from 2015 to 2022. In this case, the relative AI skill penetration rate can be understood as the sum of the penetration of each AI skill across occupations in a given country or region, divided by the global average across the same occupation. For instance, a relative skill penetration rate of 1.5 means that the average penetration of AI skills in that country or region is 1.5 times the global average across the same set of occupations.As of 2022, the three countries or regions with the highest AI skill penetration rates were India (3.2),  the United States (2.2), and Germany (1.7).4.1 JobsChapter 4: The EconomyFigure 4.1.13Table of Contents Chapter 4 Preview 183Artificial IntelligenceIndex Report 20230.290.290.300.300.310.310.380.390.570.680.710.860.871.281.990.881.081.030.980.850.820.981.131.461.371.911.592.052.363.270.00 0.50 1.00 1.50 2.00 2.50 3.00AustraliaFinlandSpainBrazilItalyUnited Arab EmiratesNetherlandsFranceUnited KingdomSingaporeGermanyCanadaIsraelUnited StatesIndiaMaleFemaleRelative AI Skill Penetration RateRelative AI Skill Penetration Rate Across Gender, 2015\u201322Source: LinkedIn, 2022 | Chart: 2023 AI Index ReportGlobal Comparison: By GenderFigure 4.1.14 disaggregates AI skill penetration rates by gender across different countries or regions.  A country\u2019s \u201cRelative AI skill penetration rate  across genders\u201d for women of 1.5 means that female members in that country are 1.5 times more likely to list AI skills than the average member in all countries pooled together across the same set of occupations in the country. For all countries in the sample, the relative AI skill penetration rate is greater for men than women. India (2.0), the United States (1.3), and Israel (0.9) have the highest reported relative AI skill penetration rates for women.4.1 JobsChapter 4: The EconomyFigure 4.1.14Table of Contents Chapter 4 Preview 184Artificial IntelligenceIndex Report 2023Using data from NetBase Quid, this section tracks trends in AI-related investments. NetBase Quid tracks data on the investments of over 8 million global public and private companies. NetBase Quid also uses natural language processing techniques to search, analyze, and identify patterns in large, unstructured datasets, like aggregated news and blogs, and company and patent databases. NetBase Quid continuously broadens the set of companies for which it tracks data, so that in this year\u2019s AI Index, the reported investment volume for certain years is larger than that of previous reports.12.62 13.0129.113.35 17.1325.7243.1 55.0961.61125.3691.8646.0613.0524.6821.8931.9126.06119.6683.3514.57 19.0425.4333.8253.7279.6295.57146.74276.14189.592013 2014 2015 2016 2017 2018 2019 2020 2021 2022050100150200250300Merger/AcquisitionMinority StakePrivate InvestmentPublic OeringTotal Investment (in Billions of U.S. Dollars)Global Corporate Investment in AI by Investment Activity, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportCorporate InvestmentAs AI becomes more and more integrated into the economy, it becomes increasingly important to track AI-related corporate investment. Figure 4.2.1 shows overall global corporate investment in AI from 2013 to 2022. Corporate investment includes mergers and acquisitions, minority stakes, private investment, and public offerings.4.2 InvestmentFor the first time since 2013, year-over-year global corporate investment in AI has decreased. In 2022, total global corporate AI investment was $189.6 billion, roughly a third lower than it was in 2021. Still, in the last decade, AI-related investment has increased thirteenfold.4.2 InvestmentChapter 4: The EconomyFigure 4.2.1Table of Contents Chapter 4 Preview 185Artificial IntelligenceIndex Report 2023NuanceCommunications, Inc.Citrix Systems, Inc.Avast LimitedAspenTechCorporationVivint Smart Home,Inc.Company Name United StatesUnited StatesCzech RepublicUnited StatesUnited StatesHeadquartersCountry Articial Intelligence;Enterprise Software;Healthcare; MachineLearningData Management, Processing, and Cloud; HR TechData Management,Processing, and Cloud;Fintech; Cybersecurity,Data ProtectionManufacturing;Software; SupplyChain ManagementCybersecurity, DataProtection; SalesEnablementFocus Area 19.8017.188.026.345.54Funding Amount(in Billions USD) Top Five AI Merger/Acquisition Investment Activities, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index ReportAVEVA Group, PLCGrupo deInversionesSuramericana, SAFractal AnalyticsPrivate LimitedAtrys Health, SAR SystemsInternational, Ltd.Company Name UnitedKingdomColombiaIndiaSpainIndiaHeadquartersCountry Chemical; Computer;Data Mining; Electronics;Industrial Manufacturing;Information Technology;Simulation; SoftwareFinancial Services; ImpactInvesting; InsuranceAnalytics; Arti cialIntelligence; Big Data;Business Intelligence;Consulting; MachineLearningMedical and HealthcareAnalytics; InformationTechnology; ITManagement; SoftwareFocus Area 4.681.480.350.280.17Funding Amount(in Billions USD) Top Five AI Minority Stake Investment Activities, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index ReportTo provide a fuller context for the nature of AI investment in the last year, Figures 4.2.2 through 4.2.5 highlight the top merger/acquisition, minority stake, private investment, and public offering events in the last year. The greatest single AI investment event was the merger/acquisition of Nuance Communications, valued at $19.8 billion (Figure 4.2.2). The largest minority stake event was for the British company Aveva Group ($4.7 billion) (Figure 4.2.3). The greatest private investment event was GAC Aion New Energy Automobile ($2.5 billion), a Chinese clean energy and automotive company (Figure 4.2.4). Finally, the largest public offering was ASR Microelectronics ($1.1 billion), a Chinese semiconductor company (Figure 4.2.5).4.2 InvestmentChapter 4: The EconomyFigure 4.2.2Figure 4.2.3Table of Contents Chapter 4 Preview 186Artificial IntelligenceIndex Report 2023GAC Ai\u00ac\u00a5an NewEnergy AutomobileCo., Ltd.Idience Co., Ltd.UaliAnduril Industries,Inc.Celonis, GmbHCompany Name ChinaSouth KoreaArgentinaUnited StatesGermanyHeadquartersCountry Automotive; CleanEnergy; Electric Vehicle;ManufacturingEmergency Medicine;Healthcare;PharmaceuticalDrones; Cloud ComputingCybersecurity, DataProtection; AR/VR;DronesRetail; IndustrialAutomation, Network; HRTech; InsurtechFocus Area 2.542.151.501.501.22Funding Amount(in Billions USD) Top Five AI Private Investment Activities, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index ReportASR MicroelectronicsCo., Ltd.iSoftStone InformationTechnology (Group)Co., Ltd.Jahez InternationalCompany forInformation SystemsTechnologyFortior Technology(Shenzhen) Co., Ltd.Beijing Deep GlintTechnology Co., Ltd.Company Name ChinaChinaSaudi ArabiaChinaChinaHeadquartersCountry Semiconductor; VCData Management,Processing, and Cloud;Cybersecurity, DataProtectionArticial Intelligence;E-Commerce; Food andBeverage; Food Delivery;Information Technology;LogisticsElectronics; MachineManufacturing;SemiconductorCybersecurity, DataProtection; Music, VideoContentFocus Area 1.080.730.430.300.29Funding Amount(in Billions USD) Top Five AI Public O ering Investment Activities, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index Report4.2 InvestmentChapter 4: The EconomyFigure 4.2.4Figure 4.2.5Table of Contents Chapter 4 Preview 187Artificial IntelligenceIndex Report 202391.862013 2014 2015 2016 2017 2018 2019 2020 2021 2022020406080100120Total Investment (in Billions of U.S. Dollars)Private Investment in AI, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportStartup ActivityThe next section analyzes private investment trends in artificial intelligence startups that have received over $1.5 million in investment since 2013.Global TrendThe global private AI investment trend reveals that while investment activity has decreased since 2021, it is still 18 times higher than it was in 2013 (Figure 4.2.6).4.2 InvestmentChapter 4: The EconomyFigure 4.2.6Table of Contents Chapter 4 Preview 188Artificial IntelligenceIndex Report 20231,3922013 2014 2015 2016 2017 2018 2019 2020 2021 202202004006008001,0001,2001,4001,600Number of CompaniesNumber of Newly Funded AI Companies in the World, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index Report3,5382013 2014 2015 2016 2017 2018 2019 2020 2021 202205001,0001,5002,0002,5003,0003,5004,000Number of Private Investment EventsNumber of Private Investment Events in AI, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportA similar trend, of short-term decreases but longer-term growth, is evident in data on total private investment events. In 2022 there were 3,538 AI-related private investment events, representing a 12% decrease from 2021 but a sixfold increase since 2013 (Figure 4.2.7). Similarly, the number of newly funded AI companies dropped to 1,392 from 1,669 last year, while having increased from 495 in 2013 (Figure 4.2.8).4.2 InvestmentChapter 4: The EconomyFigure 4.2.7Figure 4.2.8Table of Contents Chapter 4 Preview 189Artificial IntelligenceIndex Report 20230.610.721.041.131.351.521.771.832.353.103.243.244.3713.4147.360 5 10 15 20 25 30 35 40 45FinlandJapanSwitzerlandSingaporeAustraliaArgentinaFranceCanadaGermanySouth KoreaIndiaIsraelUnited KingdomChinaUnited StatesTotal Investment (in Billions of U.S. Dollars)Private Investment in AI by Geographic Area, 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportOver $1 Billion$500 Million \u2013 $1 Billion$100 Million \u2013 $500 Million$50 Million \u2013 $100 MillionUnder $50 MillionUndisclosedTotal Funding Size 4132772772,8515984,020 2021 651642382,5855403,538 2022 10184415155,4361,1387,558 Total AI Private Investment Events by Funding Size,Source: NetBase Quid, 2022 | Table: 2023 AI Index Report2021 Vs. 2022The year-over-year decrease in AI-related funding is also evident when the funding events are disaggregated by size. Across all size categories, with the exception of ones over  $1 billion, the total number of AI funding events decreased (Figure 4.2.9).Regional Comparison by Funding AmountOnce again, the United States led the world in terms of total AI private investment. In 2022, the $47.4 billion invested in the United States was roughly 3.5 times the amount invested in the next highest country, China ($13.4 billion), and 11 times the amount invested in the United Kingdom ($4.4 billion) (Figure 4.2.10).4.2 InvestmentChapter 4: The EconomyFigure 4.2.9Figure 4.2.10Table of Contents Chapter 4 Preview 190Artificial IntelligenceIndex Report 20231.813.043.043.103.994.725.576.596.997.738.8310.8318.2495.11248.900 20 40 60 80 100 120 140 160 180 200 220 240SpainAustraliaSwitzerlandHong KongJapanSingaporeSouth KoreaFranceGermanyIndiaCanadaIsraelUnited KingdomChinaUnited StatesTotal Investment (in Billions of U.S. Dollars)Private Investment in AI by Geographic Area, 2013\u201322 (Sum)Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportWhen private AI investments are aggregated since 2013, the same ranking of countries applies: The United States is first with $248.9 billion invested, followed by China ($95.1 billion) and the United Kingdom ($18.2 billion) (Figure 4.2.11).4.2 InvestmentChapter 4: The EconomyFigure 4.2.11Table of Contents Chapter 4 Preview 191Artificial IntelligenceIndex Report 20232013 2014 2015 2016 2017 2018 2019 2020 2021 2022010203040506070Total Investment (in Billions of U.S. Dollars)11.04, European Union and United Kingdom13.41, China47.36, United StatesPrivate Investment in AI by Geographic Area, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportWhile the United States continues to outpace  other nations in terms of private AI investment,  the country experienced a sharp 35.5% decrease in AI private investment within the last year (Figure 4.2.12). Chinese investment experienced a similarly sharp decline (41.3%).4.2 InvestmentChapter 4: The EconomyFigure 4.2.12The top five American AI private investment events are highlighted in Figure 4.2.13, the top five European Union and British investments in Figure 4.2.14, and the top five Chinese investments in Figure 4.2.15.Table of Contents Chapter 4 Preview 192Artificial IntelligenceIndex Report 2023Anduril Industries, Inc.Faire Wholesale, Inc.Anthropic, PBCArctic Wolf Networks, Inc.JingChi, Inc.Company Name Cybersecurity, DataProtection; AR/VR;DronesFintech; Retail; SalesEnablementArticial Intelligence;InformationTechnology; MachineLearningData Management,Processing, and Cloud;Cybersecurity, DataProtectionData Management,Processing, and Cloud;AV; AR/VRFocus Area 1.500.820.580.400.40Funding Amount(in Billions USD) Top AI Private Investment Events in the UnitedSource: NetBase Quid, 2022 | Table: 2023 AI Index ReportStates, 2022GAC Ai\u00ac\u00a5an New EnergyAutomobile Co., Ltd.GAC Ai\u00ac\u00a5an New EnergyAutomobile Co., Ltd.Beijing ESWINTechnology Group Co.,Ltd.Zhejiang Hozon NewEnergy Automobile Co.,Ltd.Zhejiang Hozon NewEnergy Automobile Co.,Ltd.Company Name Automotive; CleanEnergy; ElectricVehicle;ManufacturingAutomotive; CleanEnergy; ElectricVehicle;ManufacturingData Management,Processing, and Cloud;Industrial Automation,Network;Semiconductor;Marketing, Digital Ads;Sales EnablementData Management,Processing, and Cloud;Cybersecurity, DataProtection; SalesEnablementData Management,Processing, and Cloud;Cybersecurity, DataProtection; SalesEnablementFocus Area 2.541.110.580.440.32Funding Amount(in Billions USD) Top AI Private Investment Events in China, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index ReportCelonis, GmbHContent Square, SASRetail Logistics Excellence- RELEX OyCera Care LimitedBabylon Holdings LimitedCompany Name Retail; Industrial Automation, Network; HR Tech; InsurtechAnalytics; Arti cialIntelligence: CRM:Data Visualization;Digital Marketing;SaaSRetailMedical andHealthcareMedical andHealthcare; Music,Video ContentFocus Area 1.220.600.570.320.30Funding Amount(in Billions USD) Top AI Private Investment Events in the EuropeanUnion and United Kingdom, 2022Source: NetBase Quid, 2022 | Table: 2023 AI Index Report4.2 InvestmentChapter 4: The EconomyFigure 4.2.13Figure 4.2.14Figure 4.2.15Table of Contents Chapter 4 Preview 193Artificial IntelligenceIndex Report 2023121222232632364144475773991605420 100 200 300 400 500NetherlandsSwedenSouth KoreaAustraliaSwitzerlandJapanSingaporeGermanyFranceCanadaIndiaIsraelUnited KingdomChinaUnited StatesNumber of CompaniesNumber of Newly Funded AI Companies by Geographic Area, 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportRegional Comparison by Newly Funded  AI CompaniesThis subsection studies the number of newly funded AI companies across various geographic areas.  As was the case with private investment, the  4.2 InvestmentChapter 4: The EconomyFigure 4.2.16United States led all regions with the largest number of newly funded AI companies at 542, followed by China at 160 and the United Kingdom at 99 (Figure 4.2.16).Table of Contents Chapter 4 Preview 194Artificial IntelligenceIndex Report 202378831081261451652452942963383414026301,3374,6430 500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 4,500NetherlandsSwedenSwitzerlandAustraliaSouth KoreaSingaporeGermanyJapanIndiaFranceCanadaIsraelUnited KingdomChinaUnited StatesNumber of CompaniesNumber of Newly Funded AI Companies by Geographic Area, 2013\u201322 (Sum)Source: NetBase Quid, 2022 | Chart: 2023 AI Index Report2013 2014 2015 2016 2017 2018 2019 2020 2021 20220100200300400500600700Number of Companies160, China293, European Union and United Kingdom542, United StatesNumber of Newly Funded AI Companies by Geographic Area, 2013\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportA similar trend is evident in the aggregate data since 2013. In the last decade, the number of newly funded AI companies in the United States is around 3.5 times the amount in China, and 7.4 times the amount in the United Kingdom (Figure 4.2.17).Figure 4.2.18 breaks down data on newly funded AI companies within select geographic regions.  In a trend that goes back a decade, the United States continues to outpace both the European Union and the United Kingdom, as well as China. However, the growth rates of the different regions are relatively similar.4.2 InvestmentChapter 4: The EconomyFigure 4.2.17Figure 4.2.18Table of Contents Chapter 4 Preview 195Artificial IntelligenceIndex Report 20230 2 4 6 8 10VCFacial RecognitionEd TechFitness and WellnessGeospatialLegal TechEntertainmentAgritechNLP, Customer SupportAVEnergy, Oil, and GasHR TechSemiconductorMusic, Video ContentInsurtechDronesAR/VRMarketing, Digital AdsSales EnablementIndustrial Automation, NetworkRetailCybersecurity, Data ProtectionFintechData Management, Processing, CloudMedical and Healthcare20222021Total Investment (in Billions of U.S. Dollars)Private Investment in AI by Focus Area, 2021 Vs. 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportFocus Area AnalysisPrivate AI investment can also be disaggregated by focus area. Figure 4.2.19 compares global private AI investment by focus area in 2022 versus 2021. The focus areas that attracted the most investment in 2022 were medical and healthcare ($6.1 billion); data management, processing, and cloud ($5.9 billion); fintech ($5.5 billion); cybersecurity and data protection ($5.4 billion); and retail ($4.2 billion). Mirroring the pattern seen in total AI private investment, the total investment across most focus areas declined in the last year.Figure 4.2.20 presents trends in AI focus area investments. As noted earlier, most focus areas saw declining investments in the last year. However, some of the focus areas that saw increased investments are semiconductor, industrial automation and network, cybersecurity and data protection, drones, marketing and digital ads, HR tech, AR/VR, and legal tech. Still, mirroring a broader trend in AI private investment, most focus areas saw greater amounts of AI private investment in 2022 than they did in 2017.4.2 InvestmentChapter 4: The EconomyFigure 4.2.19Table of Contents Chapter 4 Preview 196Artificial IntelligenceIndex Report 20232018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 2022024682018 2020 202202468Total Investment (in Billions of U.S. Dollars)Data Management, Processing, Cloud Medical and Healthcare Fintech AVSemiconductor Industrial Automation, Network Retail Fitness and WellnessNLP, Customer Support Energy, Oil, and Gas Cybersecurity, Data Protection DronesMarketing, Digital Ads HR Tech Facial Recognition InsurtechAgritech Sales Enablement AR/VR Ed TechGeospatial Legal Tech Entertainment Music, Video ContentVC5.86 6.05 5.521.341.653.92 4.200.531.01 1.615.381.883.051.63 0.07 1.740.873.18 2.390.370.71 0.83 0.87 1.720.02Private Investment in AI by Focus Area, 2017\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index Report4.2 InvestmentChapter 4: The EconomyFigure 4.2.20Table of Contents Chapter 4 Preview 197Artificial IntelligenceIndex Report 20232018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 20220242018 2020 2022024Total Investment (in Billions of U.S. Dollars)Data Management, Processing, Cloud Medical and Healthcare Fintech AVSemiconductor Industrial Automation, Network Retail Fitness and WellnessNLP, Customer Support Energy, Oil, and Gas Cybersecurity, Data Protection DronesMarketing, Digital Ads HR Tech Facial Recognition InsurtechAgritech Sales Enablement AR/VR Ed TechGeospatial Legal Tech Entertainment Music, Video ContentVCEU/UK, 0.24 EU/UK, 0.76 EU/UK, 0.94 EU/UK, 0.02EU/UK, 0.01 EU/UK, 1.65 EU/UK, 2.07 EU/UK, 0.14EU/UK, 0.04 EU/UK, 0.20 EU/UK, 0.23 EU/UK, 0.04EU/UK, 0.76 EU/UK, 1.28 EU/UK, 0.00 EU/UK, 1.29EU/UK, 0.08 EU/UK, 0.16 EU/UK, 0.06 EU/UK, 0.10EU/UK, 0.01 EU/UK, 0.06 EU/UK, 0.17 EU/UK, 0.44EU/UK, 0.02US, 3.13 US, 4.19 US, 3.23 US, 0.69US, 0.58 US, 0.87 US, 1.52 US, 0.23US, 0.69 US, 0.80 US, 3.87 US, 1.60US, 1.14 US, 0.24 US, 0.07 US, 0.39US, 0.55 US, 1.12 US, 2.07 US, 0.12US, 0.55 US, 0.71 US, 0.47 US, 1.10US, 0.00CN, 1.87 CN, 0.25 CN, 0.03 CN, 0.49CN, 1.02 CN, 1.06 CN, 0.01 CN, 0.00CN, 0.13 CN, 0.34 CN, 1.07 CN, 0.03CN, 0.88 CN, 0.00 CN, 0.00 CN, 0.00CN, 0.10 CN, 1.68 CN, 0.01 CN, 0.01CN, 0.03 CN, 0.05 CN, 0.18 CN, 0.03CN, 0.00Private Investment in AI by Focus Area and Geographic Area, 2017\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index Report4.2 InvestmentChapter 4: The EconomyFigure 4.2.21Finally, 4.2.21 shows private investment in AI by focus area over time within select geographic regions, highlighting how private investment priorities in AI differ across geographies. For example, in 2022, private investment in AI-related drone technology in the United States ($1.6 billion) was nearly 53 times more than that in China ($0.03 billion), and 40 times more than that in the European Union and the United Kingdom ($0.04 billion). Chinese private investment in AI-related semiconductors ($1.02 billion) was 1.75 times more than that in the United States ($0.58 billion), and 102 times more than that in the European Union and the United Kingdom ($0.01 billion).Table of Contents Chapter 4 Preview 198Artificial IntelligenceIndex Report 2023This section explores how corporations tangibly use AI. First, it highlights industry adoption trends and asks how businesses adopt AI and what particular AI technologies they find most useful, and identifies how AI adoption affects their bottom line. Second, the section considers industry motivations and explores what questions industry leaders consider when thinking about incorporating AI technologies. Finally, it paints a qualitative picture of business AI use by examining trends in AI-related earnings calls.2017 2018 2019 2020 2021 20220%10%20%30%40%50%60%% of Respondents50%Share of Respondents Who Say Their Organizations Have Adopted AI in at Least One Function, 2017\u201322Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Industry AdoptionThe following subsection on the industry adoption of AI borrows data from McKinsey\u2019s \u201cThe State of AI in 2022\u2014and a Half Decade in Review,\u201d as well as previous years\u2019 editions. The 2022 report drew on data from a survey of 1,492 participants representing a wide range of regions, industries, company sizes, functional specialties, and tenures.4.3 Corporate ActivityAdoption of AI CapabilitiesAccording to the most recent McKinsey report, as of 2022, 50% of surveyed organizations reported having adopted AI in at least one business unit or function (Figure 4.3.1). This total is down slightly from 56% in 2021, although up significantly from 20% in 2017. AI usage has rapidly grown in the past half-decade, but leveled off since 2020.4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.1Table of Contents Chapter 4 Preview 199Artificial IntelligenceIndex Report 20232018 2019 2020 2021 20220.000.501.001.502.002.503.003.504.00Number of AI Capabilities (Average)3.80Average Number of AI Capabilities That Respondents\u2019 Organizations Have Embedded Within at Least OneFunction or Business Unit, 2018\u201322Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report In the last half-decade, the average number of AI capabilities that organizations have embedded has doubled from 1.9 in 2018 to 3.8 in 2022 (Figure 4.3.2). Some of the AI capabilities that McKinsey features in their survey include recommender systems, NL text understanding, and facial recognition.44.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.24 In the 2022 edition of the McKinsey survey, 16 total AI capabilities are considered: computer vision, deep learning, digital twins, facial recognition, GAN, knowledge graphs, NL generation, NL speech understanding, NL text understanding, physical robotics, recommender systems, reinforcement learning, robotic process automation, transfer learning, transformers, and virtual agents.Table of Contents Chapter 4 Preview 200Artificial IntelligenceIndex Report 202324%16%14%20%19%16%19%19%17%15%0% 4% 8% 12% 16% 20% 24%Predictive Service and InterventionRisk Modeling and AnalyticsContact-Center AutomationProduct Feature OptimizationCustomer Acquisition and Lead GenerationNew AI-Based Enhancements of ProductsCustomer Service AnalyticsCustomer SegmentationCreation of New AI-Based ProductsService Operations OptimizationService OperationsProduct and/orService DevelopmentMarketing and SalesRisk% of RespondentsMost Commonly Adopted AI Use Cases by Function, 2022Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report The most commonly adopted AI use case in 2022 was service operations optimization (24%), followed by the creation of new AI-based products (20%), customer segmentation (19%), customer service analytics (19%), and new AI-based enhancement of products (19%) (Figure 4.3.3).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.3Table of Contents Chapter 4 Preview 201Artificial IntelligenceIndex Report 202334% 30% 24% 18% 11% 25% 18% 23% 33% 20% 25% 20% 39% 16% 11% 33%32% 37% 31% 11% 8% 26% 12% 22% 34% 19% 23% 26% 46% 16% 11% 30%33% 36% 25% 19% 13% 18% 20% 11% 22% 24% 32% 19% 25% 7% 11% 40%24% 22% 18% 24% 13% 29% 20% 30% 42% 14% 30% 19% 47% 17% 12% 33%32% 18% 16% 5% 5% 14% 5% 12% 29% 11% 16% 13% 16% 9% 6% 14%37% 45% 24% 16% 15% 23% 24% 29% 40% 15% 34% 23% 48% 22% 15% 43%Computer VisionDeep LearningDigital TwinsFacial RecognitionGANKnowledge GraphsNL GenerationNL Speech UnderstandingNL Text UnderstandingPhysical RoboticsRecommender SystemsReinforcement LearningRobotic Process AutomationTransfer LearningTransformers (e.g., GPT-3)Virtual AgentsHigh Tech/TelecomHealthcare Systems/Pharma andMed. ProductsFinancial ServicesConsumer Goods/RetailBusiness, Legal, andProfessional ServicesAll Industries% of Respondents (AI Capability)IndustryAI Capabilities Embedded in at Least One Function or Business Unit, 2022Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report With respect to the type of AI capabilities embedded in at least one function or business unit, as indicated by Figure 4.3.4, robotic process automation had the highest rate of embedding within high tech/telecom, financial services and business, and legal and professional services industries\u2014the respective rates of embedding were 48%, 47%, and 46%. Across all industries, the most embedded AI technologies were robotic process automation (39%), computer vision (34%), NL text understanding (33%), and virtual agents (33%).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.4Table of Contents Chapter 4 Preview 202Artificial IntelligenceIndex Report 202311% 8% 5% 10% 19% 19% 21% 9%11% 10% 9% 8% 16% 20% 19% 12%14% 4% 3% 4% 15% 31% 29% 11%1% 8% 7% 31% 17% 24% 23% 2%15% 7% 2% 4% 22% 12% 8% 8%6% 6% 4% 7% 38% 21% 25% 8%Human ResourcesManufacturingMarketing and SalesProduct and/orService DevelopmentRiskService OperationsStrategy andCorporate FinanceSupply-ChainManagementHigh Tech/TelecomHealthcare Systems/Pharma andMed. ProductsFinancial ServicesConsumer Goods/RetailBusiness, Legal, andProfessional ServicesAll Industries% of Respondents (Function)IndustryAI Adoption by Industry and Function, 2022Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Figure 4.3.5 shows AI adoption by industry and AI function in 2022. The greatest adoption was in risk for high tech/telecom (38%), followed by service operations for consumer goods/retail (31%) and product and/or service development for financial services (31%).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.5Table of Contents Chapter 4 Preview 203Artificial IntelligenceIndex Report 20232% -4% -15% -13% 6% -6% 12% -4%-3% 2% -19% -7% 3% -6% 11% -1%12% -14% -19% -13% 14% 16% 25% -7%-9% 4% -17% 11% -15% -16% 10% -6%6% -4% -12% -25% 9% -5% -4% -1%-6% -5% -24% -38% 22% -13% 15% -8%Human ResourcesManufacturingMarketing and SalesProduct and/orService DevelopmentRiskService OperationsStrategy andCorporate FinanceSupply-ChainManagementHigh Tech/TelecomHealthcare Systems/Pharma andMed. ProductsFinancial ServicesConsumer Goods/RetailBusiness, Legal, andProfessional ServicesAll IndustriesPercentage Point Change in Responses (Function)IndustryPercentage Point Change in Responses of AI Adoption by Industry and Function 2021 Vs. 2022Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Figure 4.3.6 shows how rates of AI adoption by industry and AI function vary from 2021 to 2022 in order to demonstrate how rates of AI adoption have changed over the last year. The greatest year-over-year increases were in consumer goods/retail, for strategy and corporate finance (25 percentage points); followed by high tech/telecom, for risk  (22 percentage points). The most significant decreases were in high tech/telecom, for product and/or service development (38 percentage points); and healthcare systems, also for product and/or service development (25 percentage points).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.6Table of Contents Chapter 4 Preview 204Artificial IntelligenceIndex Report 20236%10%29%7%32%25%21%8%30%7%41%6%20%8%31%23%45% 42% 29% 28% 43% 52% 30% 43% 32% Average Across All ActivitiesStrategy and Corporate FinanceProduct and/or Service DevelopmentSupply Chain ManagementRiskMarketing and SalesHuman ResourcesManufacturingService Operations 10% 10% 37%10% 18% 33%14% 13% 31%9% 20% 41%10% 11% 27%14% 17% 28%13% 24% 33%8% 16% 41%8% 19% 36% 57% 61% 58% 70% 48% 59% 70% 65% 63%Decrease by <10% Decrease by 10\u201319% Decrease by \u226520% Increase by >10% Increase by 6\u201310% Increase by \u22645%FunctionCost Decrease and Revenue Increase From AI Adoption by Function, 2021Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report % of RespondentsOrganizations report AI adoption leading to both cost decreases and revenue increases. On the cost side, the functions that most respondents saw decreases in as a result of AI adoption were supply chain management (52%), service operations (45%), strategy and corporate finance (43%), and risk (43%) (Figure 4.3.7). On the revenue side, the functions that most respondents saw increases in as a result of AI adoption were marketing and sales (70%), product and/or service development (70%), and strategy and corporate finance (65%).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.7Table of Contents Chapter 4 Preview 205Artificial IntelligenceIndex Report 202350%55%48%59%41%44%56%64%51%55%61%52%0% 10% 20% 30% 40% 50% 60%Developing Markets(incl. India,Latin America,MENA)Greater China(incl. Hong Kong,Taiwan)North AmericaEuropeAsia-PacicAll Geographies20222021% of RespondentsAI Adoption by Organizations in the World, 2021 Vs. 2022Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Figure 4.3.8 shows AI adoption by organizations globally, broken out by regions of the world. In 2022, North America led (59%), followed by Asia-Pacific (55%) and Europe (48%). The average adoption rate across all geographies was 50%, down 6% from 2021. Notably, \u201cGreater China\u201d registered a 20 percentage point decrease from 2021.4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.8Table of Contents Chapter 4 Preview 206Artificial IntelligenceIndex Report 20232019 2020 2021 20220%10%20%30%40%50%60%% of Respondents9%, Political Stability13%, National Security20%, Physical Safety28%, Workforce/Labor Displacement30%, Equity and Fairness32%, Organizational Reputation37%, Explainability40%, Personal/Individual Privacy45%, Regulatory Compliance59%, CybersecurityRisks From Adopting AI That Organizations Consider Relevant, 2019\u201322Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Consideration and Mitigation of Risks From Adopting AIAs has been the case in the last few iterations of the McKinsey report, in 2022 respondents identified cybersecurity as the most relevant risk when adopting AI technology (59%) (Figure 4.3.9). The next most cited risks were regulatory compliance (45%), personal/individual privacy (40%), and explainability (37%).  The least salient risks identified by organizations were national security (13%) and political stability (9%).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.9Table of Contents Chapter 4 Preview 207Artificial IntelligenceIndex Report 20232019 2020 2021 20220%10%20%30%40%50%% of Respondents4%, Political Stability7%, National Security15%, Physical Safety17%, Equity and Fairness18%, Workforce/Labor Displacement22%, Explainability22%, Organizational Reputation28%, Personal/Individual Privacy36%, Regulatory Compliance51%, CybersecurityRisks From Adopting AI That Organizations Take Steps to Mitigate, 2019\u201322Source: McKinsey & Company Survey, 2022 | Chart: 2023 AI Index Report Figure 4.3.10 highlights the AI risks that organizations are taking steps to mitigate. The top three responses were cybersecurity (51%), followed by regulatory compliance (36%) and personal/individual privacy (28%). As was the case in previous years, there are meaningful gaps between the risks organizations cite as relevant and those which organizations have taken steps to mitigate. For instance, there is a gap of 8 percentage points for cybersecurity, 9 percentage points for regulatory compliance, and 12 percentage points for personal/individual privacy. These differences suggest there is a gap between the awareness organizations have of various risks and their steps taken to mitigate such risks.4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.10Table of Contents Chapter 4 Preview 208Artificial IntelligenceIndex Report 2023In 2021,  launched a technical preview of Copilot, a generative AI tool that enables developers and coders to present a coding problem in natural language and then have Copilot generate a solution in code. Copilot can also translate between various programming languages. In 2022, GitHub surveyed over 2,000 developers who were using the tool to determine its effect on their productivity, well-being, and workflow.5Figure 4.3.11 summarizes the results of the survey. Developers overwhelmingly reported feeling more productive, satisfied, and efficient when working with Copilot. More specifically, 88% of surveyed respondents commented feeling more productive, 74% reported being able to focus on more satisfying work, and 88% claimed to have completed tasks more quickly. One software engineer stated, \u201c[With Copilot] I have to think less, and when I have to think, it\u2019s the fun stuff. It sets off a little spark that makes coding more fun and more efficient.\u201d6As part of the same survey, GitHub recruited 95 developers and randomly split them into two groups, one of which used Copilot as part of a coding task and the other which did not. The results of this experiment are summarized in Figure 4.3.12. The developers who used Copilot reported a completion rate of 78%, 8 percentage points higher than those who did not use Copilot. Likewise, it only took the developers using Copilot 71 minutes to complete their task, which was 56% less time than the developers who did not use Copilot (161 minutes). These survey and experiment results are evidence of the tangible ways in which AI tools improve worker productivity.4.3 Corporate ActivityChapter 4: The EconomyThe Effects of GitHub\u2019s Copilot on Developer Productivity and HappinessNarrative Highlight: 5 Most of the developers surveyed, around 60%, were professional developers; 30% were students and 7% were hobbyists.6 The quote is taken from this source.It took the developers using Copilot only 71 minutes to complete their task\u201456% less time than the developers who did not use Copilot (161 minutes).Table of Contents Chapter 4 Preview 209Artificial IntelligenceIndex Report 2023 4.3 Corporate ActivityChapter 4: The EconomyThe Effects of GitHub\u2019s Copilot on Developer Productivity and Happiness (cont\u2019d)Narrative Highlight: 73%77%87%88%96%59%60%74%88%0% 20% 40% 60% 80% 100%More In The FlowLess Time SearchingLess Mental E\ufffdorts on Repetitive TasksFaster CompletionFaster With Repetitive TasksLess Frustrated When CodingMore Ful\ufffdlled With My JobFocus on More Satisfying Work I Am More ProductivePerceived ProductivitySatisfaction and Well-BeingE\ufffdciency and Flow% of Participants That Agreed or Strongly AgreedMeasuring Dimensions of Developer Productivity When Using Copilot: Survey Responses, 2022Source: GitHub Survey, 2022 | Chart: 2023 AI Index ReportNumber of DevelopersCompletion Rate (%)Average Time Taken toComplete the Task (Minutes)457871UsedGitHub Copilot 5070161Did Not UseGitHub CopilotSummary of the Experiment Process and ResultsSource: GitHub Survey, 2022 | Table: 2023 AI Index ReportFigure 4.3.11Figure 4.3.12Table of Contents Chapter 4 Preview 210Artificial IntelligenceIndex Report 202394%, Important5%, Somewhat Important1%, Not ImportantImportance of AI Solutions for Organizations\u2019Overall SuccessSource: Deloitte Survey, 2022 | Chart: 2023 AI Index Report82%, Strongly Agree / Agree16%, Neither Agree nor Disagree2%, Strongly Disagree / Disagree1%, UnsureBelieve AI Enhances Performance and JobSatisfaction, 2022Source: Deloitte Survey, 2022 | Chart: 2023 AI Index ReportIndustry MotivationThis section explores the motivations industry leaders have in deploying AI and examines the degree to which they feel AI is important, the reasons they are eager to embrace AI, and the factors that have hindered further scaling of AI solutions. The data from this section comes from Deloitte\u2019s \u201cState of AI in Enterprise\u201d report, which has surveyed companies about their use of AI since 2017. This year\u2019s survey polled 2,620 business leaders from a wide range of countries, industries, and corporate levels.Perceived Importance of AIFigures 4.3.13 and 4.3.14 suggest that an overwhelming majority of business leaders perceive AI to be important for their businesses. More specifically, when asked how important AI solutions were for their organization\u2019s overall success, 94% responded \u201cimportant,\u201d 5% said \u201csomewhat important,\u201d and 1% answered \u201cnot important\u201d (Figure 4.3.13).Similarly, when asked whether they believe that AI enhances performance and job satisfaction, 82% responded \u201cstrongly agree/agree,\u201d 16% said they \u201cneither agree nor disagree,\u201d and only 2% selected \u201cstrongly disagree/disagree\u201d (Figure 4.3.14).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.13Figure 4.3.14Table of Contents Chapter 4 Preview 211Artificial IntelligenceIndex Report 20232018 2019 2020 2021 20220%20%40%60%80%% of Respondents76%Expected AI Investment Increase in the Next Fiscal Year Source: Deloitte Survey, 2022 | Chart: 2023 AI Index ReportAI Investments and Implementation OutcomesIn 2022, 76% of surveyed leaders reported expecting to increase AI investments in the next fiscal year (Figure 4.3.15). Although this represents 4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.15a 9 percentage point decrease since 2021 and a 12 percentage point decrease since 2018, a significantly large portion of business leaders continue to express interest in AI investment.Table of Contents Chapter 4 Preview 212Artificial IntelligenceIndex Report 202328%30%30%31%32%32%32%32%33%33%33%34%34%37%0% 10% 20% 30%Anticipate Constituent NeedsImprove Constituent EngagementActivate the Potential of Existing Headcount and/or Improve Talent ManagementIncrease RevenueEnable New Business/Service ModelsPredict DemandCreate New Products/Programs and ServicesImprove Decision-MakingMake Organizational Processes More E\ufffdcientEnter New Markets/Expand Services toNew ConstituentsDiscover Valuable InsightsLower CostsImprove Collaboration Across Business Functions/Organizations% of RespondentsMain Outcomes of AI Implementation, 2022Source: Deloitte Survey, 2022 | Chart: 2023 AI Index ReportCustomize or Improve Product/Programs,Services, or OffersFigure 4.3.16 highlights the main outcomes that business leaders achieved by embracing AI solutions.7 The top outcome was lowered costs (37%), followed by improved collaboration across business functions/organizations (34%) and having discovered valuable insights (34%).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.167 Figure 4.3.16 is drawn from the chart in the Deloitte survey: \u201cOutcomes\u2014\u2018Achieved to a high degree.\u2019\u201dTable of Contents Chapter 4 Preview 213Artificial IntelligenceIndex Report 202333%34%37%0% 5% 10% 15% 20% 25% 30% 35%Choosing the RightAI TechnologiesLack of Executive CommitmentProving BusinessValue% of RespondentsTop Three Challenges in Starting AI Projects, 2022Source: Deloitte Survey, 2022 | Chart: 2023 AI Index ReportChallenges in Starting and Scaling AI ProjectsThe top three challenges that business leaders identified in terms of starting AI-related projects were proving business value (37%), lack of executive commitment (34%), and choosing the right AI technologies (33%) (Figure 4.3.17).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.17Table of Contents Chapter 4 Preview 214Artificial IntelligenceIndex Report 202340%42%44%50%0% 10% 20% 30% 40% 50%Proving Business ValueImplementingAI TechnologiesObtaining Needed Dataor Input to Train ModelManagingAI-Related Risks% of RespondentsMain Barriers in Scaling AI Initiatives, 2022Source: Deloitte Survey, 2022 | Chart: 2023 AI Index ReportThe main barrier leaders faced in scaling existing AI initiatives was managing AI-related risks (50%), obtaining more data or inputs to train a model (44%), and implementing AI technologies (42%) (Figure 4.3.18).4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.18Table of Contents Chapter 4 Preview 215Artificial IntelligenceIndex Report 20232018 2019 2020 2021 20220100200300Number of Earnings Calls268Number of Fortune 500 Earnings Calls Mentioning AI, 2018\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportEarnings CallsThe following subsection presents data from NetBase Quid, which uses natural language processing tools to analyze trends in corporate earnings calls. NetBase Quid analyzed all 2022 earnings calls from Fortune 500 companies, identifying all mentions of \u201cArtificial Intelligence,\u201d \u201cAI,\u201d \u201cMachine Learning,\u201d \u201cML,\u201d and \u201cdeep learning.\u201d4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.19Aggregate TrendsIn the 2022 fiscal year, there were 268 earnings calls from Fortune 500 companies that mentioned AI-related keywords (Figure 4.3.19). The number of such mentions dropped from the previous year, when there were 306, but has increased since 2018 when there were 225.Table of Contents Chapter 4 Preview 216Artificial IntelligenceIndex Report 20235.36%2.61%0.87%2.32%5.22%0.29%2.46%6.67%2.17%5.94%2.46%1.74%3.33%8.26%6.67%3.62%4.20%7.97%6.81%2.90%5.94%11.74%0.71% (-87%)1.00% (-62%)1.28% (+47%)1.85% (-20%)2.13% (-59%)2.42% (+734%)2.70% (+10%)2.99% (-55%)2.99% (+37%)3.13% (-47%)3.27% (+33%)3.84% (+121%)4.13% (+24%)4.84% (-41%)5.26% (-21%)6.26% (+73%)7.11% (+69%)7.40% (-7%)8.39% (+23%)8.82% (+204%)8.82% (+48%)9.96% (-15%)0% 1% 2% 3% 4% 5% 6% 7% 8% 9% 10% 11% 12% 13% 14%Digital TransformationNvidia RTXInvestmentsData Center GPUAzure Cognitive ServicesCustomer SupportAdobe ExperienceData Storage and ManagementData ProcessingAutonomous VehiclesRevenue GrowthNvidia AI Use CasesEdge IntelligenceDeep LearningPersonalizing Customer ExperienceCloud PlatformsHealthcare and Medical PracticesSupport Decision-MakingProcess AutomationAdvertising and MarketingPricing and Inventory ManagementBusiness Integration20222018Theme Mentioned (% of Total)Themes for AI Mentions in Fortune 500 Earnings Calls, 2018 Vs. 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportSpecific ThemesMentions of AI in Fortune 500 earnings calls were associated with a wide range of themes. In 2022, the most cited themes were business integration (10.0%); pricing and inventory management (8.8%); and 4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.20advertising and marketing (8.8%) (Figure 4.3.20). Compared to 2018, some of the less prevalent AI-related themes in 2022 included deep learning (4.8%), autonomous vehicles (3.1%), and data storage and management (3.0%).Table of Contents Chapter 4 Preview 217Artificial IntelligenceIndex Report 2023To better understand business attitudes that surround AI, it is worth looking at AI-related excerpts from the Fortune 500 earnings calls.For example, on the topic of business integration, companies often cite AI and machine learning (ML) use cases to reassure business audiences of safer business practices, growing opportunities, streamlining processes, and capability expansion.4.3 Corporate ActivityChapter 4: The EconomyWhat Are Business Leaders Actually Saying About AI?Narrative Highlight: \u201cIn September, we opened a next-gen fulfillment center in Illinois. This 1.1 million square foot facility features robotics, machine learning, and automated storage, resulting in increased productivity and a better service for our customers at faster delivery times.\u201d \u2013 John David, CFO, Walmart (Q3 2022)In terms of process automation, business leaders emphasize the ability of AI tools to accelerate productivity gains and to deliver a better customer experience.\u201cWe spent $100 million building certain risk and fraud systems so that when we process payments on the consumer side, losses are down $100 million to $200 million. Volume is way up. That\u2019s a huge benefit.\u201d  \u2013 Jamie Dimon, CEO, JP Morgan Chase & Co. (Q2 2022)\u201cWe spent a ton of money on Cloud. We spend a ton of money on adding capabilities. And over time, as you do it on one platform, it all becomes more efficient. So, I think it\u2019s a lot of little things, but it adds up with our base of people and fixed cost, it adds up significantly over time. We\u2019ve been able to maintain our headcount at a level we feel good about, and we think  we can grow massively on top of that without having to add lots of bodies to be able to do it.\u201d \u2013 Peter Kern, CEO, Expedia Group (Q4 2022)\u201cEspecially in the last year or so, the field of robotics itself has actually changed because with AI and ML coming to the picture, there\u2019s significant developments in the robotics field. So we think it\u2019s a huge opportunity for us.\u201d  \u2013 Raj Subramaniam, CEO, FedEx (Q3 2022)\u201cWe continue to drive the use of automation and artificial intelligence to drive productivity gains to help offset inflationary pressures.\u201d \u2013 Jim Davis, CEO, Quest Diagnostics (Q4 2022)\u201cWe have improved the experience for customers by applying artificial intelligence to match them with an expert who is right for their specific situation and to deliver insights to experts so they can provide excellent service.\u201d \u2013 Sasan Goodarzi, CEO, Intuit (Q2 2022)Table of Contents Chapter 4 Preview 218Artificial IntelligenceIndex Report 2023The conversation surrounding pricing and inventory management saw companies reassuring business audiences on how their use of AI would improve their operational strength, especially in environments of high inflation and supply chain challenges.4.3 Corporate ActivityChapter 4: The EconomyWhat Are Business Leaders Actually Saying About AI? (cont\u2019d)Narrative Highlight: \u201cWe continue to see opportunities across [the software and analytics] segment as payers, providers, and partners take advantage of our high ROI solutions and realize the benefits of our data, AI models, and workflow capabilities.\u201d  \u2013 Neil de Crescenzo, CEO, UnitedHealth Group (Q2 2022)There is also a vibrant discussion about the ways in which AI can change healthcare and medical practices, more specifically to reduce costs, improve the patient experience, and better serve clinicians.\u201cWe are \u2026 continuing to refine and invest in machine learning tools that will allow for more sophisticated competitive pricing and greater automation at scale.\u201d  \u2013 Adrian Mitchell, CFO, Macy\u2019s (Q3 2022)\u201cOur teams are utilizing technology, innovative data analytics and AI to forecast supply chain lead times and changes in market demand to ensure optimal levels. These actions along with our pricing initiatives positively impacted our gross margin in the second quarter.\u201d  \u2013 Bert Nappier, CFO, Genuine Parts Company (Q3 2022)\u201c[Using] machine learning and robotics, we can now resolve a wide range of prescription drug claims which previously required the attention of our pharmacists, freeing them up to spend time with patients. This advanced approach reduces overall cost and improves the patient experience.\u201d  \u2013 Karen Lynch, CEO, CVS Health (Q2 2022)\u201cI\u2019d like to highlight productivity efforts in our preauthorization process where we\u2019re leveraging an in-house artificial intelligence solution to automatically match incoming faxes to the correct authorization requests. This solution creates administrative efficiencies across millions of inbound images. We are also scaling this solution to multiple business units such as pharmacy and are also expanding the application of this type of AI to provide decision support to clinicians, which will result in improvements to authorization turnaround times, reduction in friction for providers and creating a better member experience.\u201d \u2013 Bruce Broussard, CEO, Humana (Q3 2022)Table of Contents Chapter 4 Preview 219Artificial IntelligenceIndex Report 202386%%84%%84%%85%%84%%81%%81%% 76%%86%% 81%%81%% 77%%77%%84%% 79%%81%%80%%87%% 79%%80%%13%%15%%15%%14%%16%%19%%18%%23%%12%%18%%18%%21%%22%%16%%19%%17%%17%%13%%20%%17%%1%% 1%% 1%% 1%% 1%%1%%2%% 1%% 2%% 1%%0%%3%%1%% 1%% 2%%2%% 3%%0%%1%%3%%Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q42018 2019 2020 2021 20220%20%40%60%80%100%NegativeMixedPositiveSentiment SummarySentiment Summary Distribution for AI Mentions in Fortune 500 Earnings Calls by Publication Date, 2018\u201322Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportSentiment AnalysisNetBase Quid also runs the AI-related text of Fortune 500 earnings calls through a sentiment analysis machine-learning algorithm that identifies whether the sentiment associated with the mention of AI is positive, mixed, or negative8. Overall, since 2018, the 4.3 Corporate ActivityChapter 4: The EconomyFigure 4.3.21sentiment associated with mentions of AI has been overwhelmingly positive (Figure 4.3.21). Mentions of AI were rarely negative, suggesting that large businesses tend to have positive associations when it comes to AI tools.8 Chapter 2 of the 2023 AI Index highlights trends in the performance of sentiment analysis algorithms.Table of Contents Chapter 4 Preview 220Artificial IntelligenceIndex Report 2023Given that robots are frequently deployed with AI-based software technologies, it is possible to gain insights on AI-ready infrastructure being deployed in the real world by tracking the installation of industrial robots. Data in this section comes from the International Federation of Robotics (IFR), an international nonprofit organization that works to promote, strengthen, and protect the robotics industry. Every year the IFR releases the World Robotics Report, which tracks global trends in installations of robots.95172011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210100200300400500Number of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in the World, 2011\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportAggregate TrendsThe following subsection includes data on the installation and operation of industrial robots, which are defined as an \u201cautomatically controlled, reprogrammable, multipurpose manipulator, programmable in three or more axes, which can be either fixed in place or mobile for use in industrial automation applications.\u201d4.4 Robot Installations2021 saw a rebound in the total number of worldwide robot installations. The 517,000 industrial robots installed in 2021 represented a 31.3% increase from 2020 and a 211.5% increase since 2011 (Figure 4.4.1).4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.19 Due to the timing of the IFR\u2019s survey, the most recent data is from 2021.Table of Contents Chapter 4 Preview 221Artificial IntelligenceIndex Report 20233,4772011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202105001,0001,5002,0002,5003,0003,500Number of Industrial Robots (in Thousands)Operational Stock of Industrial Robots in the World, 2011\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportThe worldwide operational stock of industrial robots also continues to steadily increase year over year (Figure 4.4.2). The total number of operational industrial robots jumped 14.6% to 3,477,000 in 2021, from 3,035,000 in 2020. In the last decade, the number of industrial robots being installed and the number being used have both steadily increased.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.2Table of Contents Chapter 4 Preview 222Artificial IntelligenceIndex Report 2023389 405 370 36847839400424391 3945172017 2018 2019 2020 20210100200300400500TraditionalCollaborativeNumber of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in the World by Type, 2017\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportIndustrial Robots: Traditional Vs.  Collaborative RobotsA distinction can be drawn between traditional robots that work for humans and collaborative robots that are designed to work with humans. Recently, the robotics community has been excited about the potential of collaborative robots given that they can be safer, more flexible, and more scalable than traditional robots, and are capable of iterative learning.In 2017, only 2.8% of all newly installed industrial robots were collaborative (Figure 4.4.3). As of 2021, that number increased to 7.5%. Although traditional industrial robots still lead new installations, the number of collaborative robots is slowly increasing.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.3Table of Contents Chapter 4 Preview 223Artificial IntelligenceIndex Report 20233.303.403.503.904.304.905.405.909.6014.1023.8031.1035.0047.20268.200 30 60 90 120 150 180 210 240 270PolandSpainSingaporeThailandCanadaIndiaMexicoFranceTaiwanItalyGermanySouth KoreaUnited StatesJapanChinaNumber of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed by Country, 2021Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportBy Geographic AreaCountry-level data on robot installations can illustrate which countries are prioritizing the integration of robots into their economy. In 2021, China installed the most industrial robots, with 268,200, 5.7 times the amount installed by Japan (47,200) and 7.7 times the amount installed by the United States (35,000) (Figure 4.4.4). The countries with the next most installations were South Korea (31,100) and Germany (23,800).4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.4Table of Contents Chapter 4 Preview 224Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021050100150200250Number of Industrial Robots Installed (in Thousands)24, Germany31, South Korea35, United States47, Japan268, ChinaNumber of New Industrial Robots Installed in Top Five Countries, 2011\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportIn 2013, China overtook Japan as the nation installing the most industrial robots (Figure 4.4.5). Since then, the gap between the total number of industrial robots installed by China and the next-nearest nation has only widened. In 2013, Chinese industrial robot installations represented 20.8% of the world\u2019s share, whereas in 2021, they represented 51.8%.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.5Table of Contents Chapter 4 Preview 225Artificial IntelligenceIndex Report 20232016 2017 2018 2019 2020 2021050100150200250Number of Industrial Robots Installed (in Thousands) 249, Rest of the World268, ChinaNumber of Industrial Robots Installed (China Vs. Rest of the World), 2016\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportChina consolidated its dominance in industrial robotics in 2021, the first year in which the country installed more industrial robots than the rest of the world combined (Figure 4.4.6).4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.6Table of Contents Chapter 4 Preview 226Artificial IntelligenceIndex Report 2023-35%1%2%6%11%14%22%31%36%51%54%56%61%65%66%\u221240% \u221230% \u221220% \u221210% 0% 10% 20% 30% 40% 50% 60% 70%SingaporeSpainSouth KoreaGermanyFranceUnited StatesJapanTaiwanThailandChinaIndiaPolandMexicoItalyCanadaAnnual Growth Rate of Industrial Robots InstalledAnnual Growth Rate of Industrial Robots Installed by Country, 2020 Vs. 2021Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportFigure 4.4.7 shows the annual growth rate of industrial robot installations from 2020 to 2021 by country. Virtually every country surveyed by the IFR reported a yearly increase in the total number of industrial robot installations. The countries that reported the highest growth rates were Canada (66%), Italy (65%), and Mexico (61%).4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.7Table of Contents Chapter 4 Preview 227Artificial IntelligenceIndex Report 2023Another important class of robots are service robots, which the ISO defines as a robot \u201cthat performs useful tasks for humans or equipment excluding industrial automation applications.\u201d10 Figure 4.4.8 is an example of a robot being used in medicine, Figure 4.4.9 illustrates how a robot can help with professional cleaning, and Figure 4.4.10 shows a robot designed for maintenance and inspection.Chapter 4: The EconomyCountry-Level Data on Service RoboticsNarrative Highlight: Service Robots in MedicineSource: UL Solutions, 2022Service Robots in Maintenance and InspectionSource: Robotnik, 2022Service Robots in Professional CleaningSource: This Week in FM, 2021Figure 4.4.8Figure 4.4.10Figure 4.4.910 A more detailed definition can be accessed here.4.4 Robot InstallationsTable of Contents Chapter 4 Preview 228Artificial IntelligenceIndex Report 2023Compared to 2020, 2021 saw a higher number of professional service robots installed in the world for several key application areas, including hospitality, medical robotics, professional cleaning, and transportation and logistics (Figure 4.4.11). The category that registered the greatest year-over-year increase was transportation and logistics: In 2021, 1.5 times the number of such service robots were installed as in 2020.Chapter 4: The EconomyCountry-Level Data on Service Robotics (cont\u2019d)Narrative Highlight: 3410121185013152080 5 10 15 20 25 30 35 40 45 50Transportation and LogisticsProfessional CleaningMedical RoboticsHospitalityAgriculture20212020Number of Professional Service Robots Installed (in Thousands)Number of Professional Service Robots Installed in the World by Application Area, 2020 Vs. 2021Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportFigure 4.4.114.4 Robot InstallationsTable of Contents Chapter 4 Preview 229Artificial IntelligenceIndex Report 2023As of 2022, the United States has the greatest number of professional service robot manufacturers, roughly 2.16 times as many as the next nation, China. Other nations with significant numbers of robot manufacturers include Germany (91), Japan (66), and France (54) (Figure 4.4.12).Chapter 4: The EconomyCountry-Level Data on Service Robotics (cont\u2019d)Narrative Highlight: 2 1 3 119494796149 44 44 34 3529225104916654 524739 39United States China Germany Japan France Russia South Korea Switzerland Canada050100150200StartupsIncumbentsUnknownNumber of Professional Service Robot ManufacturersNumber of Professional Service Robot Manufacturers in Top Countries by Type of Company, 2022Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportFigure 4.4.124.4 Robot InstallationsTable of Contents Chapter 4 Preview 230Artificial IntelligenceIndex Report 2023871852118910230871944121108437107246415137119520 20 40 60 80 100 120 140UnspeciedPlastic and Chemical ProductsMetal and MachineryFoodElectrical/ElectronicsAutomotiveAll Others202120202019Number of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in the World by Sector, 2019\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportSectors and Application TypesOn a global level, the sector that saw the greatest amount of robot installations was electrical/electronics (137,000), followed by automotive (119,000) (Figure 4.4.13). Each of the highlighted sectors has recorded increases in the total number of industrial robot installations since 2019.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.13Table of Contents Chapter 4 Preview 231Artificial IntelligenceIndex Report 2023745571771226407060516983250968072301132620 20 40 60 80 100 120 140 160 180 200 220 240WeldingUnspeciedProcessingHandlingDispensingCleanroomAssembling202120202019Number of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in the World by Application, 2019\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportRobots can also be deployed in a wide range of applications, from assembling to dispensing and handling. Figure 4.4.14 illustrates how the application of industrial robots has changed since 2021. Handling continues to be the application case toward which the most industrial robots are deployed. In 2021, 230,000 industrial robots were installed for handling functions, 2.4 times more than for welding (96,000) and 3.7 times more than for assembling (62,000). Every application category, with the exception of dispensing and processing, saw more robot installations in 2021 than in 2019.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.14Table of Contents Chapter 4 Preview 232Artificial IntelligenceIndex Report 20233141223423212305122364312143613448862290 10 20 30 40 50 60 70 80 90UnspeciedRubber and PlasticsPharma/CosmeticsMetal and MachineryFoodElectrical/ElectronicsAutomotiveAll Others202120202019Number of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in China by Sector, 2019\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportChina Vs. United StatesThe Chinese industrial sectors that installed the greatest number of industrial robots in 2022 were electrical/electronics (88,000), automotive (62,000), and metal and machinery (34,000) (Figure 4.4.15). Every industrial sector in China recorded a greater number of robot installations in 2021 than in 2019.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.15Table of Contents Chapter 4 Preview 233Artificial IntelligenceIndex Report 20235.002.503.802.203.5013.003.506.302.602.302.703.7010.502.607.103.503.803.402.909.804.500 2 3 5 6 8 9 11 12 14UnspeciedPlastic and Chemical ProductsMetal and MachineryFoodElectrical/ElectronicsAutomotiveAll Others202120202019Number of Industrial Robots Installed (in Thousands)Number of Industrial Robots Installed in the United States by Sector, 2019\u201321Source: International Federation of Robotics (IFR), 2022 | Chart: 2023 AI Index ReportThe automotive industry installed the greatest number of industrial robots in the United States in 2021, although installation rates for that sector decreased year over year (Figure 4.4.16). However, other sectors like food, along with plastic and chemical products, saw year-over-year increases in robot installations.4.4 Robot InstallationsChapter 4: The EconomyFigure 4.4.16Table of Contents Chapter 5 Preview 234Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023CHAPTER 5: EducationTable of Contents Chapter 5 Preview 235Artificial IntelligenceIndex Report 2023EducationCHAPTER 5 PREVIEW:Overview 236Chapter Highlights 2375.1 Postsecondary AI Education 238 CS Bachelor\u2019s Graduates 238CS Master\u2019s Graduates 240CS PhD Graduates 242CS, CE, and Information Faculty 246 Narrative Highlight:  Who Funds  CS Departments in the U.S.? 2555.2 K\u201312 AI Education 257 United States 257 State-Level Trends 257 AP Computer Science 258 Narrative Highlight:  The State of  International K\u201312 Education 260ACCESS THE PUBLIC DATATable of Contents Chapter 5 Preview 236Artificial IntelligenceIndex Report 2023OverviewStudying the state of AI education is important for gauging some of the ways in which the AI workforce might evolve over time. AI-related education has typically occurred at the postsecondary level; however, as AI technologies have become increasingly ubiquitous, this education is being embraced at the K\u201312 level. This chapter examines trends in AI education at the postsecondary and K\u201312 levels, in both the United States and the rest of the world.We analyze data from the Computing Research Association\u2019s annual Taulbee Survey on the state of computer science and AI postsecondary education in North America, Code.org\u2019s repository of data on K\u201312 computer science in the United States, and a recent UNESCO report on the international development of K\u201312 education curricula.Chapter 5: EducationTable of Contents Chapter 5 Preview 237Artificial IntelligenceIndex Report 2023Chapter HighlightsMore and more AI specialization. The proportion of new computer science PhD graduates from U.S. universities who specialized in AI jumped to 19.1% in 2021, from 14.9% in 2020 and 10.2% in 2010.New AI PhDs increasingly head to industry. In 2011, roughly the same proportion of new AI PhD graduates took jobs in industry (40.9%) as opposed to academia (41.6%). Since then, however, a majority of AI PhDs have headed to industry. In 2021, 65.4% of AI PhDs took jobs in industry, more than double the 28.2% who took jobs in academia.The gap in external  research funding for  private versus public American CS departments continues to widen. In 2011, the median amount of total expenditure from external sources for computing research was roughly the same for private and public CS departments in the United States. Since then, the gap has widened, with private U.S. CS departments receiving millions more in additional funding than public universities. In 2021, the median expenditure for private universities was $9.7 million, compared to  $5.7 million for public universities.New North American  CS, CE, and information faculty hires stayed flat.  In the last decade, the total number of new North American computer science (CS), computer engineering (CE), and information faculty hires has decreased: There were 710 total hires in 2021 compared to 733 in 2012. Similarly, the total number of tenure-track hires peaked in 2019 at 422 and then dropped to 324 in 2021.Interest in K\u201312 AI and computer science education grows in both the United States and the rest of the world.  In 2021, a total of 181,040 AP computer science exams were taken by American students, a 1.0% increase from the previous year. Since 2007, the number of AP computer science exams has increased ninefold. As of 2021, 11 countries, including Belgium, China, and South Korea, have officially endorsed and implemented a K\u201312 AI curriculum.Chapter 5: EducationTable of Contents Chapter 5 Preview 238Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationCS Bachelor\u2019s GraduatesAt the undergraduate level, most AI-related courses are offered as part of a computer science (CS) curriculum. Therefore, trends in new CS bachelor\u2019s graduates give us a proxy for undergraduate interest in AI. In 2021, the total number of new North American CS bachelor\u2019s graduates was 33,059\u2014nearly four times greater than in 2012 (Figure 5.1.1).33,0592010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202105,00010,00015,00020,00025,00030,000Number of New CS Bachelor\u2019s GraduatesNew CS Bachelor\u2019s Graduates in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index Report5.1 Postsecondary AI EducationFigure 5.1.1Table of Contents Chapter 5 Preview 239Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationFigure 5.1.2 looks at the proportion of CS bachelor\u2019s graduates in North America who are international students. The number stood at 16.3% in 2021 and has been steadily increasing since 2012\u2014the proportion of such students has risen 9.5 percentage points since 2012.2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%4%8%12%16%New International CS Bachelor\u2019s Graduates (% of Total)16.30%New International CS Bachelor\u2019s Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.2Table of Contents Chapter 5 Preview 240Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationCS Master\u2019s GraduatesAI courses are also commonly offered in CS master\u2019s degree programs. Figure 5.1.3 shows the total number of new CS master\u2019s graduates in North America since 2010. In 2021 there were roughly twice as many master\u2019s graduates as in 2012. However, from 2018 to 2021 the total number of new master\u2019s graduates plateaued, declining slightly from 15,532 to 15,068.15,0682010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202102,0004,0006,0008,00010,00012,00014,00016,000Number of New CS Master\u2019s GraduatesNew CS Master\u2019s Graduates in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.3Table of Contents Chapter 5 Preview 241Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationInterestingly, the number of CS master\u2019s students at North American universities who are international started declining in 2016 after rising in the early 2010s (Figure 5.1.4). Despite the decline, in 2021 the majority of CS master\u2019s graduates remained international (65.2%).2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%20%40%60%80%New International CS Master\u2019s Graduates (% of Total)65.20%New International CS Master\u2019s Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.4Table of Contents Chapter 5 Preview 242Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationCS PhD GraduatesUnlike the trends in bachelor\u2019s and master\u2019s CS graduates, since 2010 there have not been large increases in the number of new PhD graduates in computer science (Figure 5.1.5). There were fewer CS PhD graduates in 2021 (1,893) than in 2020 (1,997) and 2012 (1,929).1,8932010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202105001,0001,5002,000Number of New CS PhD GraduatesNew CS PhD Graduates in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.5Table of Contents Chapter 5 Preview 243Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationCS PhD graduates in North American universities are becoming increasingly international (Figure 5.1.6). In 2010, 45.8% of CS PhD graduates were international students; the proportion rose to 68.6% in 2021.2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%New International CS PhD Graduates (% of Total)68.60%New International CS PhD Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.6Table of Contents Chapter 5 Preview 244Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationMoreover, now a significantly larger proportion of new CS PhD students are specializing in AI (Figure 5.1.7). In 2021, 19.1% of new CS PhD students in North American institutions specialized in AI, a 4.2 percentage point increase since 2020 and 8.6 percentage point increase since 2012.2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%2%4%6%8%10%12%14%16%18%New AI PhD Students (% of Total)19.10%New CS PhD Students (% of Total) Specializing in AI, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.7Table of Contents Chapter 5 Preview 245Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationWhere do new AI PhDs choose to work following graduation? Mirroring trends reported in last year\u2019s AI Index report, an increasingly large proportion of AI PhD graduates are heading to industry (Figures 5.1.8 and 5.1.9). In 2011, for example, roughly the same percentage of graduates took jobs in industry (40.9%) as in academia (41.6%). However, as of 2021 a significantly larger proportion of students (65.4%) went to industry after graduation than to academia (28.2%). The amount of new AI PhDs entering government was 0.7% and has remained relatively unchanged in the last half-decade.76641017485 77134116162180153195726347514342636073656184154134154132 136123201178238249219281201020112012201320142015201620172018201920202021050100150200250IndustryGovernmentAcademiaNumber of New AI PhD GraduatesEmployment of New AI PhDs in North America bySource: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportSector, 2010\u2013212010201120122013201420152016201720182019202020210%10%20%30%40%50%60%New AI PhD Graduates (% of Total)0.67%, Government28.19%, Academia65.44%, IndustryEmployment of New AI PhDs (% of Total) in NorthAmerica by Sector, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.8 Figure 5.1.91 The sums in Figure 5.1.9 do not add up to 100, as there is a subset of new AI PhDs each year who become self-employed, unemployed, or report an \u201cother\u201d employment status in the CRA survey. These students are not included in the chart.Table of Contents Chapter 5 Preview 246Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationCS, CE, and  Information FacultyTo better understand trends in AI and CS education, it is instructive to consider data on computer science faculty in addition to postsecondary students. Figure 5.1.10 highlights the total number of CS, CE (computer engineering), and information faculty in North American universities. The amount of faculty has marginally increased in the last year, by 2.2%. Since 2011 the number of CS, CE, and information faculty has grown by 32.8%. 4,366  4,536  4,549  4,548 4,711  4,786 5,059  5,214  5,252  5,231  5,310 669  661  487 863 1,014  1,122 1,180  831  895 1,183  1,150 494 617 736  861 447 515  676 529 432  390 432 465 426 296  306 656 602  766 689 649  589 691 653 668 530 5226,1386,314 6,4786,6296,806 6,8877,3627,6577,858 7,9768,1492011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202101,0002,0003,0004,0005,0006,0007,0008,000Tenure Track Teaching Professors Other Instructors Research PostdocNumber of CS, CE, and Information FacultyNumber of CS, CE, and Information Faculty in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.10Table of Contents Chapter 5 Preview 247Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationIn 2021 there were a total of 6,789 CS faculty members in the United States (Figure 5.1.11). The total number of CS faculty in the United States increased by only 2.0% in the last year, but by 39.0% since 2011. 3,455  3,725  3,564  3,559 3,880  3,971 4,176  4,366  4,384  4,390 4,482 521 550 421  679 826  903 947  671  715 946  899 436  534 618  693 387 460 491 455 396 364 408 426  382 276 287 522 521 592 509 535  491 567 531  518 424  4284,8855,2565,068 5,2025,637 5,7296,0986,430 6,5336,654 6,7892011 2012 2013 2014 2015 2016 2017 2018 2019 2020 202101,0002,0003,0004,0005,0006,0007,000Tenure Track Teaching Professors Other Instructors Research PostdocNumber of CS FacultyNumber of CS Faculty in the United States, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.11Table of Contents Chapter 5 Preview 248Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationFigure 5.1.12 reports the total number of new CS, CE, and information faculty hires in North American universities. In the last decade, the total number of new faculty hires has decreased: There were 710 total hires in 2021, while in 2012 there were 733. Similarly, the total number of tenure-track hires peaked in 2019 at 422 and has since dropped to 324 in 2021.583543733572749691800749878860765710249 258294218348320358396 4064223743242010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210200400600800Total Tenure-TrackNumber of New CS, CE, and Information Faculty HiresNew CS, CE, and Information Faculty Hires in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.12Table of Contents Chapter 5 Preview 249Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationIn 2021, the greatest percentage of new CS, CE, and information faculty hires (40%) came straight from receiving a PhD (Figure 5.1.13). Only 11% of new CS and CE faculty came from industry. 40% 39% 29% 38% 15% 16% 17% 15% 34% 34% 41% 34% 11% 11% 13% 13%2018 2019 2020 20210%20%40%60%80%100%New PhD From Postdoc From Other Academic From IndustrySource of New FacultySource of New Faculty in North American CS, CE, and Information Departments, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.13Table of Contents Chapter 5 Preview 250Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationThe share of filled new CS, CE, and information faculty positions in North American universities has remained relatively stable in the last decade (Figure 5.1.14). In 2021, 89.3% of new faculty positions were filled, compared to 82.7% in 2011.2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%90%Share of Filled New CS, CE, and Information Faculty Positions 89.28%Share of Filled New CS, CE, and Information Faculty Positions in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.14Table of Contents Chapter 5 Preview 251Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationAmong open CS, CE, and information faculty positions in 2021, the most commonly cited reason for their remaining unfilled was offers being turned down (53%) (Figure 5.1.15). In 22% of cases, hiring was still in progress, while 14% of the time, a candidate had not been identified who met the department\u2019s hiring goals. 14% 8% 13% 14% 14% 16% 26% 26% 37% 37% 37% 53% 44% 56% 51% 52% 43% 40% 36% 55% 45% 34% 6% 12% 6% 5% 10% 10% 17% 22% 18% 23% 27% 28% 31% 26% 25% 10%  18%2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%20%40%60%80%100%Didn\u2019t \ufffdnd a person who met our hiring goalsO\ufffders turned downTechnically vacant, not \ufffdlled for admin reasonsHiring in progressOtherReason Faculty Positions Remained Un\ufffdlled (% of Total)Reason Why New CS, CE, and Information Faculty Positions Remained Unlled (% of Total), 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.15Table of Contents Chapter 5 Preview 252Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationFigure 5.1.16 highlights the median nine-month salaries of CS faculty in the United States by position since 2015. During that period, the salaries for all classes of professors have increased. In 2021, the average full professor in computer science made 3.2% more than they did in 2020, and 12.8% more than they did in 2015. (Note: These figures have not been adjusted for inflation.)176.01170.57168.87164.54159.96158.97156.02127.47123.71121.55119.48117.5113.95111.67 114.07109.23107.55105.45103.01101.1699.122015 2016 2017 2018 2019 2020 2021020406080100120140160180 Full Professor Associate Professor Assistant ProfessorMedian Salary of CS Faculty (in Thousands of U.S. Dollars)Median Nine-Month Salary of CS Faculty in United States, 2015\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.16Table of Contents Chapter 5 Preview 253Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationWhat proportion of new CS, CE, and information faculty tenure-track hires are international? The data suggests that it is not a substantial proportion. In 2021, only 13.2% of new CS, CE, and information faculty hires were international (Figure 5.1.17).2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%5%10%15%20%25%New International Tenure-Track Faculty Hires (% of Total)13.20%Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportNew International CS, CE, and Information Tenure-Track Faculty Hires (% of Total) in North America,2010\u201321Figure 5.1.17Table of Contents Chapter 5 Preview 254Artificial IntelligenceIndex Report 2023 5.1 Postsecondary AI EducationChapter 5: EducationThe majority of CS, CE, and Information faculty losses in North American departments (36.3%) were the result of faculty taking academic positions elsewhere (Figure 5.1.18). In 2021, 15.2% of faculty took nonacademic positions, which is roughly the same amount as those who took such positions a decade prior, in 2011 (15.9%). 67 89 74  65 94  90 80 94  103  91 100 52 62 74  86 77  89 85 126 139 113 110 34 27  32 44 24 42 26 34 43 33 46 23 36 22 20 22 20 20 372132212322462372702343033273123032011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021050100150200250300DiedRetiredTook academic position elsewhereTook nonacademic positionRemained, but changed to part-timeOtherUnknownFaculty LossesFaculty Losses in North American CS, CE, and Information Departments, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.18Table of Contents Chapter 5 Preview 255Artificial IntelligenceIndex Report 2023Who Funds CS Departments in the U.S.?The CRA tracks data on the external funding sources of CS departments in the United States. The main funder of American CS departments continues to be the National Science Foundation (NSF), which in 2021 accounted for 34.9% of external funds. However, the share of funding provided by NSF has decreased since 2003 (Figure 5.1.19). In 2021, the next largest sources of funding came from defense agencies such as the Army Research Office, the Office of Naval Research, and the Air Force Research Laboratory (20.3%); industrial sources (12.1%); the Defense Advanced Research Projects Agency (DARPA) (8.8%); and the National Institutes of Health (NIH) (6.8%). The diminishing share of NSF funds over time has been partially offset by increasing funds from industry and NIH.Narrative Highlight: 5.1 Postsecondary AI EducationChapter 5: Education2003 2006 2009 2012 2015 2018 20210%5%10%15%20%25%30%35%40%45%External Funding Sources (% of Total)0.00%, IMLS0.40%, Unallocated1.50%, State Agencies2.30%, DOE3.60%, Other Federal4.60%, Other4.90%, Private Foundation6.80%, NIH8.80%, DARPA12.10%, Industrial Sources20.30%, Other Defense34.90%, NSFExternal Funding Sources (% of Total) of CS Departments in United States, 2003\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.19Table of Contents Chapter 5 Preview 256Artificial IntelligenceIndex Report 2023Who Funds CS Departments in the U.S.? (cont\u2019d)Figure 5.1.20 shows the median total expenditures from external sources for computing research in American CS departments. In 2021, the median total expenditure for private universities was $9.7 million compared with $5.7 million for public universities. Although total median expenditures have increased over the last decade for both private and public CS departments, the gap in expenditure has widened, with private universities beginning to significantly outspend public ones.Narrative Highlight: 5.1 Postsecondary AI EducationChapter 5: Education2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210246810Median Total Expenditure (in Millions of U.S. Dollars)5.69, Public9.71,  PrivateMedian Total Expenditure From External Sources for Computing Research of U.S. CS Departments, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 5.1.20Table of Contents Chapter 5 Preview 257Artificial IntelligenceIndex Report 2023ALAKAZ ARCA COCTDEDCFLGAHIID IL INIAKS KYLAMEMTNENVNHNJNMNYNCNDOHOKORMDMAMIMNMSMOPARISCSDTNTXUTVTVAWAWVWIWYYesNoStates Requiring That All High Schools Offer a Computer ScienceCourse, 2022Source: Code.org, 2022 | Chart: 2023 AI Index ReportAL85%AK51%AZ36%AR92%CA40%CO57%CT77%DE40%DC45%FL40%GA66%HI77%ID38%IL44%IN85%IA71%KS40%KY63%LA32%ME60%MT36%NE52%NV83%NH82%NJ67%NM41%NY48%NC61%ND44%OH48%OK62%OR63%MD98%MA78%MI46%MN21%MS60%MO49%PA77%RI86%SC93%SD39%TN60%TX47%UT73%VT76%VA75%WA47%WV78%WI66%WY55%Public High Schools Teaching Computer Science (% of Total in State),2022Source: Code.org, 2022 | Chart: 2023 AI Index Report5.2 K\u201312 AI EducationUnited StatesData on the state of K\u201312 CS education in the United States comes from Code.org, an education innovation nonprofit dedicated to ensuring that every school includes computer science as part of its core K\u201312 education. Tracking trends in K\u201312 CS education can partially serve as a proxy for understanding the state of K\u201312 AI education in AmericaState-Level TrendsFigure 5.2.1 highlights the 27 states that in 2022 required that all high schools offer a computer science course.Figure 5.2.2 highlights the percentage of public high schools in a state that teach computer science. The top three states in terms of rate of computer science teaching are Maryland (98%), South Carolina (93%), and Arkansas (92%).5.2 K\u201312 AI EducationThe following subsection shows trends in K\u201312 AI education based on K\u201312 computer science education data in the United States as well as survey data from UNESCO on the state of global K\u201312 AI education.Figure 5.2.1Figure 5.2.2Chapter 5: EducationTable of Contents Chapter 5 Preview 258Artificial IntelligenceIndex Report 2023181.042007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021020406080100120140160180Number of AP Computer Science Exams Taken (in Thousands)Number of AP Computer Science Exams Taken, 2007\u201321Source: Code.org, 2022 | Chart: 2023 AI Index Report5.2 K\u201312 AI EducationAP Computer ScienceAnother barometer for tracking the state of K\u201312 CS education in the United States is analyzing trends in the total number of AP computer science exams taken.2Year over year the total number of AP computer science exams continued to increase. In 2021, the most recent year for which there is data, there were a total of 181,040 AP computer science exams taken, roughly the same number as the previous year, after several years of significant increases. This leveling could be the result of the pandemic. Since 2007, the number of AP computer science exams has increased over ninefold.Figure 5.2.3Chapter 5: Education2 There are two types of AP CS exams: Computer Science A and Computer Science Principles. Data on computer science exams taken includes both exams. AP CS Principles was initially offered in 2017.Table of Contents Chapter 5 Preview 259Artificial IntelligenceIndex Report 2023AK100AL2,399AR1,406AZ1,587CA31,189CO2,584CT3,251DC352DE513FL14,864GA7,221HI782IA521ID429IL8,572IN2,883KS236KY1,462LA1,191MA5,451MD7,662ME242MI4,504MN1,432MO1,199MS400MT42NC6,273ND109NE514NH403NJ9,391NM270NV1,701NY13,304OH3,754OK500OR714PA6,104RI617SC2,159SD26TN2,046TX17,307UT612VA6,034VT150WA4,034WI2,080WV352WY112Source: Code.org, 2022 | Chart: 2023 AI Index ReportNumber of AP Computer Science Exams Taken, 2021AK13.62AL47.51AR46.43AZ21.84CA79.68CO44.47CT89.72DC52.63DE51.05FL68.10GA66.94HI54.04IA16.29ID22.53IL67.57IN42.31KS8.03KY32.44LA25.74MA77.99MD124.09ME17.57MI44.87MN25.07MO19.43MS13.56MT3.80NC59.37ND14.01NE26.18NH29.04NJ101.33NM12.76NV54.06NY67.00OH31.91OK12.53OR16.78PA46.91RI56.25SC41.57SD2.90TN29.36TX58.55UT18.33VA69.70VT23.18WA52.11WI35.37WV19.71WY19.33Number of AP Computer Science Exams Taken per 100,000 Inhabitants,2021Source: Code.org, 2022 | Chart: 2023 AI Index Report5.2 K\u201312 AI EducationIn 2021, the states which saw the greatest number of AP computer science exams taken were California (31,189), followed by Texas (17,307), Florida (14,864), New York (13,304), and New Jersey (9,391) (Figure 5.2.4).Figure 5.2.5 looks at the number of AP CS exams taken per capita.3 The state with the largest per capita amount of AP computer science exams taken in 2021 was Maryland, with 124.1 exams per 100,000 inhabitants. The next states were New Jersey (101.3), Connecticut (89.7), California (79.7), and Massachusetts (78.0).Figure 5.2.4Figure 5.2.5Chapter 5: Education3 More specifically, Figure 5.2.5 normalizes the number of AP CS exams taken\u2014the total number of exams taken in a particular state in 2021 is divided by the state\u2019s population based on the 2021 U.S. Census.Table of Contents Chapter 5 Preview 260Artificial IntelligenceIndex Report 2023The State of International K\u201312 EducationIn 2021, UNESCO released one of the most comprehensive reports to date on the international state of government-endorsed AI curricula. To gather information, UNESCO released two surveys: the first to representatives of 193 UNESCO member states and the second to over 10,000 private- and third-sector actors. As part of these surveys, respondents were asked to report on the status of AI curricula for students in K\u201312 general education.Figure 5.2.6, taken from the UNESCO report, highlights the governments that have taken steps to implement AI curricula and across which levels of education. For example, Germany is in the process of developing government-endorsed AI curricular standards on the primary, middle, and high-school levels, and the Chinese government has already endorsed and implemented standards across those same three levels.Narrative Highlight: Chapter 5: EducationArmeniaAustriaBelgiumChinaIndiaKuwaitPortugalQatarSerbiaSouth KoreaUnited Arab EmiratesBulgariaGermanyJordanSaudia ArabiaSerbia Country Endorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedIn DevelopmentIn DevelopmentIn DevelopmentIn DevelopmentIn Development Status \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 Primary School \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 Middle School \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 High School Government Implementation of AI Curricula by Country, Status, and Education LevelSource: UNESCO, 2022 | Table: 2023 AI Index ReportiI iitrt alatarSerbiaSouth KoreaUnited Arab EmiratesBulgariaGermanyJordanSaudia ArabiaSerbiaEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedEndorsed and ImplementedIn DevelopmentIn DevelopmentIn DevelopmentIn DevelopmentIn Development Status \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 Primary Sch ol \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 Mi dle School \u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713 High School  I l tation of AI Cu ricula by Country, Status, and Education Level: ,  | able: 2023 AI Index Reporto ugFigure 5.2.645.2 K\u201312 AI Education4 According to the UNESCO report, Serbia has already endorsed and implemented certain kinds of K\u201312 AI curricula, but is also simultaneously in the process of developing others\u2014thus it is listed under both categories.Table of Contents Chapter 5 Preview 261Artificial IntelligenceIndex Report 2023The State of International K\u201312 Education (cont\u2019d)Figure 5.2.7 identifies the topic areas most emphasized in the K\u201312 AI curricula profiled in the UNESCO report. The four topics toward which the most time was allocated were algorithms and programming (18%),  AI technologies (14%), data literacy (12%), and application of AI to other domains (12%).Narrative Highlight: Chapter 5: Education10%2%9%14%5%7%12%11%12%18%0% 2% 4% 6% 8% 10% 12% 14% 16% 18%UnspeciedAI TechniquesDeveloping AI TechnologiesAI TechnologiesSocial Implications of AIEthics of AIApplication of AI to Other DomainsContextual Problem-SolvingData LiteracyAlgorithms and ProgrammingAI FoundationsEthics and Social ImpactUnderstanding, Using, and Developing AIUnspeci ed% of Time AllocatedTime Allocated (% of Total) in K\u201312 AI Curricula by Topic, 2022Source: UNESCO, 2022 | Chart: 2023 AI Index ReportFigure 5.2.75.2 K\u201312 AI EducationTable of Contents Chapter 5 Preview 262Artificial IntelligenceIndex Report 2023The State of International K\u201312 Education (cont\u2019d)Narrative Highlight: Chapter 5: Education5.2 K\u201312 AI EducationWhat might an actual K\u201312 AI curriculum look like in practice? The UNESCO report includes detailed information about a sample curriculum that was deployed in Austria, the Austrian Data Science and Artificial Intelligence curriculum.  As noted in the report:\u201cThe Austrian Data Science and Artificial Intelligence curriculum includes digital basics such as using an operating system to store and print files, design presentations, and use spreadsheets and word-processing software. It also covers design and reflection on types and social issues in digital media, and safe digital media use. Students in high school engage programming languages, algorithms and simulations. They learn the basic principles of data literacy, including collecting data, structuring a spreadsheet, and carrying out analyses and visualizations. They apply criteria to evaluate the credibility and reliability of data sources as well as digital content. Students are expected to know about careers in ICT, including AI, and the social applications of emerging technologies. They create digital media and learn about the cloud and how to connect and network computers. They also gain an understanding of the ethical dilemmas that are associated with the use of such technologies, and become active participants in social discourse on these issues. Finally, students are tasked with using technology to make public statements and understand how this reflects the democratic process.\u201d\u201cThey also gain an understanding of the ethical dilemmas that  are associated with the use of such technologies, and become active participants in social discourse on these issues.\u201dTable of Contents 263Artificial IntelligenceIndex Report 2023Chapter 6 PreviewArtificial IntelligenceIndex Report 2023CHAPTER 6: Policy and  GovernanceTable of Contents 264Artificial IntelligenceIndex Report 2023Chapter 6 PreviewOverview 265Chapter Highlights 2666.1 AI and Policymaking 267Global Legislation Records on AI 267 By Geographic Area 269  Narrative Highlight:  A Closer Look at   Global AI Legislation 270United States Federal AI Legislation 271United States State-Level AI Legislation 272  Narrative Highlight:  A Closer Look   at State-Level AI Legislation 275Global AI Mentions 276 By Geographic Area 277  Narrative Highlight:  A Closer Look   at Global AI Mentions 279United States Committee Mentions 280United States AI Policy Papers 283 By Topic 2846.2 National AI Strategies 285Aggregate Trends 285By Geographic Area 2856.3 U.S. Public Investment in AI 286Federal Budget for Nondefense AI R&D 286U.S. Department of Defense  Budget Requests 287U.S. Government AI-Related  Contract Spending 288 Total Contract Spending 288 6.4 U.S. AI-Related Legal Cases 291Total Cases 291Geographic Distribution 292Sector 293Type of Law 294  Narrative Highlight:  Three Significant   AI-Related Legal Cases 295Policy and GovernanceCHAPTER 6 PREVIEW:ACCESS THE PUBLIC DATATable of Contents 265Artificial IntelligenceIndex Report 2023Chapter 6 PreviewOverviewThe growing popularity of AI has prompted intergovernmental, national, and regional organizations to craft strategies around AI governance. These actors are motivated by the realization that the societal and ethical concerns surrounding AI must be addressed to maximize its benefits. The governance of AI technologies has become essential for governments across the world.This chapter examines AI governance on a global scale. It begins by highlighting the countries leading the way in setting AI policies. Next, it considers how AI has been discussed in legislative records internationally and in the United States. The chapter concludes with an examination of trends in various national AI strategies, followed by a close review of U.S. public sector investment in AI.Chapter 6: Policy and GovernanceTable of Contents 266Artificial IntelligenceIndex Report 2023Chapter 6 PreviewChapter HighlightsPolicymaker interest  in AI is on the rise.  An AI Index analysis of the legislative records of 127 countries shows that the number of bills containing \u201cartificial intelligence\u201d that were passed into law grew from just 1 in 2016 to 37 in 2022. An analysis of the parliamentary records on AI in 81 countries likewise shows that mentions of AI in global legislative proceedings have increased nearly 6.5 times since 2016.When it comes to AI, policymakers have  a lot of thoughts.  A qualitative analysis of the parliamentary proceedings of a diverse group of nations reveals that policymakers think about AI from a wide range of perspectives. For example, in 2022, legislators in the United Kingdom discussed the risks of AI-led automation; those in Japan considered the necessity of safeguarding human rights in the face of AI; and those in Zambia looked at the possibility of using AI for weather forecasting.From talk to enactment\u2014 the U.S. passed more  AI bills than ever before.  In 2021, only 2% of all federal AI bills in the United States were passed into law. This number jumped to 10% in 2022. Similarly, last year 35% of all state-level AI bills were passed into law.The U.S. government continues to increase  spending on AI. Since 2017, the amount of U.S. government AI-related contract spending has increased roughly 2.5 times.The legal world is  waking up to AI. In 2022, there were 110 AI-related legal cases in United States state and federal courts, roughly seven times more than in 2016. The majority of these cases originated in California, New York, and Illinois, and concerned issues relating to civil, intellectual property, and contract law.Chapter 6: Policy and GovernanceTable of Contents 267Artificial IntelligenceIndex Report 2023Chapter 6 PreviewIn the last 10 years, AI governance discussions have accelerated, resulting in numerous policy proposals in various legislative bodies. This section begins by exploring the legislative initiatives related to AI that have been suggested or enacted in different countries and regions, followed by an in-depth examination of state-level AI legislation in the United States. The section then scrutinizes records of AI-related discussions in parliaments and congresses worldwide and concludes with the number of AI policy papers published in the United States.01\u201356\u20131011\u20131516\u201325Number of AI-Related Bills Passed Into Law by Country, 2016\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report No Available DataGlobal Legislative  Records on AIThe AI Index conducted an analysis of laws passed by legislative bodies in 127 countries that contain the words \u201cartificial intelligence\u201d from 2016 to 2022.2 Of the 127 countries analyzed, since 2016, 31 have 6.1 AI and Policymaking16.1 AI and Policymakingpassed at least one AI-related bill, and together they have passed a total of 123 AI-related bills (Figure 6.1.1). Figure 6.1.2 shows that from 2016 to 2022, there has been a sharp increase in the total number of AI-related bills passed into law, with only one passed in 2016, climbing to 37 bills passed in 2022.Figure 6.1.1Chapter 6: Policy and Governance1  Note that the analysis of passed AI policies may undercount the number of actual bills, given that large bills can include multiple sub-bills related to AI; for example, the CHIPS and Science Act passed by the U.S. in 2022.2 The full list of countries analyzed is in the Appendix. The AI Index team attempted to research the legislative bodies of every country in the world; however, publicly accessible legislative databases were not made available for certain countries.Table of Contents 268Artificial IntelligenceIndex Report 2023Chapter 6 Preview2016 2017 2018 2019 2020 2021 202205101520253035Number of AI-Related Bills37Number of AI-Related Bills Passed Into Law in 127 Select Countries, 2016\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report 6.1 AI and PolicymakingFigure 6.1.2Chapter 6: Policy and GovernanceTable of Contents 269Artificial IntelligenceIndex Report 2023Chapter 6 Preview95422222211111110 1 2 3 4 5 6 7 8 9SloveniaLiechtensteinLatviaKyrgyz RepublicGermanyCroatiaAustriaUnited KingdomRussiaPortugalItalyBelgiumAndorraPhilippinesSpainUnited StatesNumber of AI-Related BillsNumber of AI-Related Bills Passed Into Law in Select Countries, 2022Source: AI Index, 2022 | Chart: 2023 AI Index Report 221310997655543330 2 4 6 8 10 12 14 16 18 20 22JapanGermanyChinaFrancePhilippinesKorea, Rep.AustriaUnited KingdomBelgiumRussiaItalySpainPortugalUnited StatesNumber of AI-Related BillsNumber of AI-Related Bills Passed Into Law in Select Countries, 2016\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report By Geographic AreaFigure 6.1.3 shows the number of laws containing mentions of AI that were enacted in 2022. The United States led the list with 9 laws, followed by Spain and 6.1 AI and Policymakingthe Philippines, which passed 5 and 4 laws, respectively. Figure 6.1.4 shows the total number of laws passed since 2016. The United States leads the list with 22 bills, followed by Portugal, Spain, Italy, and Russia.Figure 6.1.3Figure 6.1.4Chapter 6: Policy and GovernanceTable of Contents 270Artificial IntelligenceIndex Report 2023Chapter 6 PreviewA Closer Look at Global AI LegislationNarrative Highlight: Kyrgyz RepublicLatviaPhilippinesSpainUnited StatesCountryAbout the Creative Industries ParkAmendments to the National Security LawSecond Congressional Commission on Education(EDCOM II) ActRight to equal treatment and non-discriminationAI Training ActBill NameThis law determines the legal status, management, and operationprocedures of the Creative Industries Park, established to accelerate thedevelopment of creative industries, including arti\ufffdcial intelligence.A provision of this act establishes restrictions on commercial companies,associations, and foundations important for national security, including acommercial company that develops arti\ufffdcial intelligence.A provision of this act creates a congressional commission to review,assess, and evaluate the state of Philippine education; to recommendinnovative and targeted policy reforms in education; and to appropriatefunds. The act calls for reforms to meet the new challenges to educationcaused by the Fourth Industrial Revolution characterized, in part, by therapid development of arti\ufffdcial intelligence.A provision of this act establishes that arti\ufffdcial intelligence algorithmsinvolved in public administrations\u2019 decision-making take into accountbias-minimization criteria, transparency, and accountability, whenevertechnically feasible.This bill requires the O\ufffdce of Management and Budget to establish orotherwise provide an AI training program for the acquisition workforce ofexecutive agencies (e.g., those responsible for program management orlogistics), with exceptions. The purpose of the program is to ensure thatthe workforce has knowledge of the capabilities and risks associated withAI.DescriptionAI-Related Legislation From Select Countries, 2022Source: AI Index, 2022 | Table: 2023 AI Index Reportr  lit iili iinite  tatestr  i  i     i l i   r i l i i   ti(  II) ti t t  l tr t t  - is ri i tiI raining ctill i  l  i   l l    ti   i  I i   li   l r t  tl   i  i i , i l i  i i l i lli . i i   i   li  i i   i l i ,i i ,  f i  i  f  i l i , i l i  i l  t t l  ti i l i t lli . r i i  f t i  t r t   r i l i i  t  r i ,,  l t  t  t t  f ili i  ti ; t  ri ti   t r t  li  r f r  i  ti ;  t  r ri tf .  t ll  f r r f r  t  t t   ll  t  tis   t  rt  I stri l l ti  r t ri , i  rt,  tr i  l t f rti i l i t lli . r visi  f t is t st lis s t t rti i l i t lli  l rit si v lv  i  li  a i istrati s\u2019 isi - aki  tak  i t  a tias- i i izati  criteria, tra s are cy, a  acc ta ility, e evertec ically feasi le.his ill requires the ce of anage ent and udget to esta lish orother ise provide an I training progra  for the acquisition orkforce ofexecutive agencies (e.g., those responsible for progra  anage ent orlogistics), ith exceptions. The purpose of the progra  is to ensure thatthe orkforce has kno ledge of the capabilities and risks associated ithI.r : I I ,  | l :  I I  Kyrgyz RepublicLatviaPhilippinesSpainUnited StatesCountryAbout the Creative Industries ParkA end ents to the National Security LawSecond Congressional Co ission on Education(EDCO  II) ActRight to equal treat ent and non-discri inationAI Training ActBill Na eThis law deter ines the legal status, anage ent, and operationprocedures of the Creative Industries Park, established to accelerate thedevelop ent of creative industries, including arti\ufffdcial intelligence.A provision of this act establishes restrictions on co ercial co panies,associations, and foundations i portant for national security, including aco ercial co pany that develops arti\ufffdcial intelligence.A provision of this act creates a congressional co ission to review,assess, and evaluate the state of Philippine education; to reco endinnovative and targeted policy refor s in education; and to appropriatefunds. The act calls for refor s to eet the new challenges to educationcaused by the Fourth Industrial Revolution characterized, in part, by therapid develop ent of arti\ufffdcial intelligence.A provision of this act establishes that arti\ufffdcial intelligence algorith sinvolved in public ad inistrations\u2019 decision- aking take into accountbias- ini ization criteria, transparency, and accountability, whenevertechnically feasible.This bill requires the O ce of anage ent and Budget to establish orotherwise provide an AI training progra  for the acquisition workforce ofexecutive agencies (e.g., those responsible for progra  anage ent orlogistics), with exceptions. The purpose of the progra  is to ensure thatthe workforce has knowledge of the capabilities and risks associated withAI.DescriptionI- elate  Le islation Fro  elect ountries, 2 22Source: AI Index, 2022 | Table: 2023 AI Index ReportThe following subsection delves into some of the AI-related legislation passed into law during 2022. Figure 6.1.5 samples five different countries\u2019 laws covering a range of AI-related issues.Figure 6.1.56.1 AI and PolicymakingChapter 6: Policy and GovernanceTable of Contents 271Artificial IntelligenceIndex Report 2023Chapter 6 Preview2015 2016 2017 2018 2019 2020 2021 2022020406080100120140Number of AI-Related Bills9, Passed88, Proposed Number of AI-Related Bills in the United States, 2015\u201322 (Proposed Vs. Passed)Source: AI Index, 2022 | Chart: 2023 AI Index Report United States Federal  AI LegislationA closer look at the U.S. federal legislative record shows a sharp increase in the total number of proposed bills that relate to AI (Figure 6.1.6). In 2015, 6.1 AI and Policymakingjust one federal bill was proposed, while in 2021, 134 bills were proposed. In 2022 this number fell to 88 proposed bills. While fewer bills were proposed in 2022, the number of passed bills, which remained at 3 for each of the past four years, increased to 9.Figure 6.1.6Chapter 6: Policy and GovernanceTable of Contents 272Artificial IntelligenceIndex Report 2023Chapter 6 Preview5322211111110 1 2 3 4 5VermontNorth CarolinaMassachusettsLouisianaIdahoHawaiiAlabamaWashingtonNew JerseyColoradoMarylandCaliforniaNumber of AI-Related BillsNumber of AI-Related Bills Passed Into Law in Select U.S. States, 2022Source: AI Index, 2022 | Chart: 2023 AI Index Report United States State-Level  AI LegislationFigure 6.1.7 shows the number of laws containing mentions of AI that were passed by U.S. states in 2022. California leads the list with 5, followed by 6.1 AI and PolicymakingMaryland with 3. Figure 6.1.8 shows the total volume of legislation passed from 2016 to 2022 for select states, with Maryland leading the list with 7 bills, followed by California, Massachusetts, and Washington. Figure 6.1.9 highlights the number of state-level AI-related bills passed by all states since 2016.Figure 6.1.7Chapter 6: Policy and GovernanceTable of Contents 273Artificial IntelligenceIndex Report 2023Chapter 6 Preview765533322222220 1 2 3 4 5 6 7OhioNorth CarolinaNew YorkNew JerseyMichiganColoradoAlabamaVermontUtahIllinoisWashingtonMassachusettsCaliforniaMarylandNumber of AI-Related BillsNumber of AI-Related Bills Passed Into Law in Select U.S. States, 2016\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report AL2AK0AZ0AR0CA6CO2CT0DE0FL0GA0HI1ID1IL3IN0IA0KS0KY1LA1ME0MD7MA5MI2MN0MS1MO0MT0NE0NV1NH0NJ2NM0NY2NC2ND1OH2OK0OR0PA0RI0SC0SD0TN0TX2UT3VT3VA1WA5WV1WI0WY0Number of State-Level AI-Related Bills Passed Into Law in the United States by State, 2016\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report (Sum)6.1 AI and PolicymakingFigure 6.1.8Figure 6.1.9Chapter 6: Policy and GovernanceTable of Contents 274Artificial IntelligenceIndex Report 2023Chapter 6 Preview2015 2016 2017 2018 2019 2020 2021 20220102030405060Number of AI-Related Bills21, Passed60, ProposedNumber of State-Level AI-Related Bills in the United States, 2015\u201322 (Proposed Vs. Passed)Source: AI Index, 2022 | Chart: 2023 AI Index Report Growing policy interest in AI can also be seen at the state level, with 60 AI-related bills proposed in 2022 (Figure 6.1.10)\u2014a dramatic increase from the 5 bills proposed in 2015. Additionally, the proportion of bills being passed has risen throughout the years. In 2015, 1 bill was passed, representing 16% of the total bills proposed that year; while in 2022, 21 bills were passed, or 35% out of the total that were proposed.6.1 AI and PolicymakingFigure 6.1.10Chapter 6: Policy and GovernanceTable of Contents 275Artificial IntelligenceIndex Report 2023Chapter 6 PreviewA Closer Look at State-Level AI LegislationNarrative Highlight: AlabamaCaliforniaMarylandNew JerseyVermontStateArti\ufffdcial Intelligence, Limit the Useof Facial Recognition, to EnsureArti\ufffdcial Intelligence Is Not the OnlyBasis for ArrestBudget Act of 2022Conservation Finance Act21st Century Integrated DigitalExperience ActAn Act Relating to the Use andOversight of Arti\ufffdcial Intelligence inState GovernmentBill NameThis bill prohibits state or local law enforcement agencies from using facial recognitionmatch results as the sole basis for making an arrest or for establishing probable cause in acriminal investigation.A provision of this appropriations bill for the 2022\u201323 \ufffdscal year allocates $1,300,000 toCalifornia State University, Sacramento, to improve the campus childcare center,including the development of an arti\ufffdcial intelligence mixed-reality classroom. A provision of this act establishes that the Department of Natural Resources shall studyand assess the potential for digital tools and platforms including arti\ufffdcial intelligence andmachine learning to contribute to Chesapeake Bay restoration and climate solutions.A provision of this act, which concerns the modernization of state government websites, establishes that the chief technology o\ufffdcer, in consultation with the chief innovationo\ufffdcer and the New Jersey Information Technology Project Review Board, shall evaluateon an annual basis the feasibility of state agencies using arti\ufffdcial intelligence andmachine learning to provide public services.This act creates the Division of Arti\ufffdcial Intelligence within the Agency of Digital Servicesto review all aspects of arti\ufffdcial intelligence developed, employed, or procured by thestate government. The act requires the Division of Arti\ufffdcial Intelligence to, among otherthings, propose a state code of ethics on the use of arti\ufffdcial intelligence in stategovernment and make recommendations to the General Assembly on policies, laws, andregulations regarding arti\ufffdcial intelligence in state government. DescriptionAI-Related Legislation From Select States, 2022Source: AI Index, 2022 | Table: 2023 AI Index Reportllif r ir l rr tt trti i l I lli , i i   f i l i i ,  rti i l I lli  I    li  f  t   i  i  1 t t  I t t  i it lri  t t l ti  t  t   r i t f rti i l I t lli  it t  r till i  ill i i    l l l   i   i  i l i i l    l  i   i      li i  l   i  i i l i i i . i i   i  i i  ill    l  ll  , ,  li i   i i , ,  i    il  ,i l i   l    i i l i lli  i li  l .  i i   i   li      l  ll    i l  i i l l   l  i l i  i i l i lli  i  l i   i     i   li  l i . r i i  f t i  t, i  r  t  r i ti  f t t  r t it , t li  t t t  i f t l  r, i  lt ti  it  t  i f i tir  t   r  I f r ti  l  r j t i  r , ll l t  l i  t  f i ilit  f t t  i  i  rti i l i t lli  i  l r i  t  r i  li  r i .i  t r t  t  i i i  f rti i l I t lli  it i  t   f i it l r it  r i  ll t  f rti i l i t lli  l , l , r r r   tt t  r t.  t r ir  t  i i i  f rti i l I t lli  t ,  t rt i , r   t t   f t i   t   f rti i l i t lli  i  t tr t   r ti s t  t  r l ss l   li i s, l s, r l ti s r r i  rti i l i t lli  i  st t  r t. I l  r : I I ,  | l :  I I  rtAlabamaCaliforniaarylandNew JerseyVermontStateArti\ufffdcial Intelligence, Limit the Useof Facial Recognition, to EnsureArti\ufffdcial Intelligence Is Not the OnlyBasis for ArrestBudget Act of 2022Conservation Finance Act21st Century Integrated DigitalExperience ActAn Act Relating to the Use andOversight of Arti\ufffdcial Intelligence inState GovernmentBill NameThis bill prohibits state or local law enforcement agencies from using facial recognitionmatch results as the sole basis for making an arrest or for establishing probable cause in acriminal investigation.A provision of this appropriations bill for the 2022\u201323 \ufffdscal year allocates $1,300,000 toCalifornia State University, Sacramento, to improve the campus childcare center,including the development of an arti\ufffdcial intelligence mixed-reality classroom. A provision of this act establishes that the Department of Natural Resources shall studyand assess the potential for digital tools and platforms including arti\ufffdcial intelligence andmachine learning to contribute to Chesapeake Bay restoration and climate solutions.A provision of this act, which concerns the modernization of state government websites, establishes that the chief technology o cer, in consultation with the chief innovationo cer and the New Jersey Information Technology Project Review Board, shall evaluateon an annual basis the feasibility of state agencies using arti\ufffdcial intelligence andmachine learning to provide public services.This act creates the Division of Arti\ufffdcial Intelligence within the Agency of Digital Servicesto review all aspects of arti\ufffdcial intelligence developed, employed, or procured by thestate government. The act requires the Division of Arti\ufffdcial Intelligence to, among otherthings, propose a state code of ethics on the use of arti\ufffdcial intelligence in stategovernment and make recommendations to the General Assembly on policies, laws, andregulations regarding arti\ufffdcial intelligence in state government. DescriptionI-Related Legislation Fro  Select States, 2022Source: AI Index, 2022 | Table: 2023 AI Index ReportThe following subsection highlights some of the AI-related legislation passed into law at the state level during 2022. Figure 6.1.11 focuses on wide-ranging AI-related laws from five states around the country.Figure 6.1.116.1 AI and PolicymakingChapter 6: Policy and GovernanceTable of Contents 276Artificial IntelligenceIndex Report 2023Chapter 6 Preview2016 2017 2018 2019 2020 2021 20222004006008001,0001,2001,4001,600Number of Mentions1,340Number of Mentions of AI in Legislative Proceedings in 81 Select Countries, 2016\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report Global AI MentionsAnother barometer of legislative interest is the number of mentions of \u201cartificial intelligence\u201d in governmental and parliamentary proceedings. The AI Index conducted an analysis of the minutes or proceedings of legislative sessions in 81 countries that 6.1 AI and Policymakingcontain the keyword \u201cartificial intelligence\u201d from 2016 to 2022.3 Figure 6.1.12 shows that mentions of AI in legislative proceedings in these countries registered a small decrease from 2021 to 2022, from 1,547 to 1,340.Figure 6.1.12Chapter 6: Policy and Governance3 The full list of countries that was analyzed is in the Appendix. The AI Index research team attempted to review the governmental and parliamentary proceedings of every country in the world; however, publicly accessible governmental and parliamentary databases were not made available for all countries.Table of Contents 277Artificial IntelligenceIndex Report 2023Chapter 6 Preview01\u20135556\u2013110111\u2013165166\u2013220221\u2013280Number of Mentions of AI in Legislative Proceedings by Country, 2022Source: AI Index, 2022 | Chart: 2023 AI Index Report No Available DataBy Geographic AreaFigure 6.1.13 shows the number of legislative proceedings containing mentions of AI in 2022.4 From the 81 countries considered, 46 had at least one mention, and Spain topped the list with 273 mentions, followed by Canada (211), the United Kingdom (146), and the United States (138).6.1 AI and PolicymakingFigure 6.1.13Chapter 6: Policy and Governance4 For mentions of AI in legislative proceedings around the world, the AI Index performed searches of the keyword \u201cartificial intelligence,\u201d in the respective languages, on the websites of different countries\u2019 congresses or parliaments, usually under sections named \u201cminutes,\u201d \u201cHansard,\u201d etc.Table of Contents 278Artificial IntelligenceIndex Report 2023Chapter 6 Preview01\u2013220221\u2013440441\u2013660661\u2013880881\u20131100Number of Mentions of AI in Legislative Proceedings by Country, 2016\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report No Available DataFigure 6.1.14 shows the total number of AI mentions in the past seven years. Of the 81 countries considered, 62 had at least one mention, and the United Kingdom dominates the list with 1,092 mentions, followed by Spain (832), the United States (626), Japan (511), and Hong Kong (478).6.1 AI and PolicymakingFigure 6.1.14Chapter 6: Policy and GovernanceTable of Contents 279Artificial IntelligenceIndex Report 2023Chapter 6 PreviewA Closer Look at Global AI MentionsNarrative Highlight: AustraliaBrazilJapanUnitedKingdomZambiaCountryHouse ofRepresentativesDiary of theChamber of theMembers210th Session ofthe Diet House ofCouncilorsCommission onthe ConstitutionNo. 2House ofCommonsThe House,NationalAssemblyLegislatureEd Husic, Australian LaborParty, Minister for Industryand ScienceMr. Gustavo Fruet,Democratic Labor PartyKohei Otsuka, DemocraticParty for the People,ShinryokufukaiDame Angela Eagle, LaborHon. Collins Nzovu, United Party for National Development,Minister of Green Economy and EnvironmentSpeaker\u201cWorking with our international partners we cantransform Australian know-how into globally recognisedskills and manufacturing in defence industries. And wecan build on our undeniable expertise in areas likequantum technologies, robotics and articialintelligence. We will seek to partner with industry andstate and territory governments to identify investmentopportunities within priority areas. An on-ramp, if youwill, of turn-key opportunities for investment to makesure the NRF is well placed for success.\u201d\u201cThere has been a lot of talk about the future of work dueto technology. In the book The Fourth IndustrialRevolution, Klaus Schwab even points out professionsthat will be extinct and professions that will demandmore and more quali\ufffdcations, in times of 5G, Internet ofThings and Arti cial Intelligence. In this sense, it is goodto highlight that the pandemic, among othercontradictions, ended up anticipating the use oftechnology, especially in the telework.\u201d\u201cIn the \ufffdeld of human rights, we believe that it isnecessary to update human rights guarantees in order torespond to changes in the times that were unpredictablewhen the Constitution was enacted. In particular, as thefusion of arti cial intelligence and Internet technologyprogresses, the international community is concernedabout the problems of individual scoring anddiscrimination, and the problem of Internet advertisingthat unfairly in\ufffduences the voting behavior of citizens.We need a constitutional argument to guarantee theautonomous decision-making of individuals and protectbasic data rights in the digital age.\u201d\u201cWhat would be the use of arti cial intelligence in tryingto decide how automated these things could become?Would there be worries about over-automation? Howwould that be looked at in terms of regulation? Howopen are we going to be about the way in which AI isapplied and how it might evolve in ways that mightembed discrimination such that we get a system wherecertain people may be discriminated against andexcluded?\u201d\u201cMadam Speaker, in order to enhance quality andaccuracy of weather forecast, the Government, with\ufffdnancial support from the United Nations DevelopmentProgramme Strengthening Climate Resilience ofAgricultural Livelihoods in Agro-Ecological (UNDPSCRALA) project is currently partnering with theUniversity of Zambia (UNZA) to develop a seasonalweather forecasting system using arti cial intelligence.\u201dQuoteNational Reconstruction Fund Corporation Bill 2022 - Second ReadingPresentation of Bill No.135, of 2022, on theamendment of the CLT -Consolidation of LaborLaws, with a view togranting telework toparents of children up to 8years oldThe Commission on theConstitutionFinancial Services andMarkets Bill (FourthSitting)Ministerial Statements;Weather and ClimateServices and the2022/2023 rainfall forecastAgenda ItemAI-Related Parliamentary Men tions From Select Countries, 2022Source: AI Index, 2022  | Table: 2023 AI Index Reporttr lir ilitia iatr fr t tii r  f tr f tr1 t  i  ft  i t  fil ri i  t  tit ti. s  fshe use,ationalsse lyi l t i , li  , i i   I i.  ,i   i t , r tirt  f r t  l ,i r f i l  l , rn. llins z vu, nite  arty for ational evelo ent,inister of reen Econo y and Environ ent  ill    i  i    i    i i  ii i  i i  i i     i  ill    i i   i      i  ll l   .    l   l        l . I      I i ll i , l    i   i ill  i   i   ill    li i , i  i   , I  i   i i l I lli . I  i  , i  i   i li    i ,  t i ti ,   ti i ti  t   ft l , i ll  i  t  t l r .I  t  l  f  ri t ,  li  t t it ir  t  t   ri t  r t  i  r r tr  t   i  t  ti  t t r  r i t l t  tit ti   t . I  rti l r,  tf i  f rti i l i t lli   I t r t t lr r , t  i t r ti l it  i  rt t  r l  f i i i l ri  i ri i ti ,  t  r l  f I t r t rti it t f irl  i  t  ti  i r f iti .   stit ti l r t t  r t  tt s isi - i  f i i i ls  r t tsi  t  ri ts i  t  i it l .\u201d\u201c t l   t  s  f rti i l i t lli  i  tr it  i   t t  t s  t i s l  l  t r   rri s t v r- t ti  l  t t  l  t i  t r s f r l ti ?  r   i  t   t t   i  i  I isa li  a   it i t v lv  i  ays t at i t iscri i ati  s c  t at  t a syst  rcertai  e le ay e iscri i ate  a ai st aexcl e ?\u201d\u201c a a  eaker, in r er t  enhance uality anaccuracy of eather forecast, the overn ent, ithnancial su ort fro  the nite  ations evelo entrogra e Strengthening li ate esilience ofgricultural Livelihoods in gro-Ecological ( PS L ) project is currently partnering ith theniversity of a bia ( ) to develop a seasonaleather forecasting syste  using arti cial intelligence.\u201di   ill .,  ,      li i   , i   i  i  l   f il     l i i   ttit tii i l r i s r ts ill ( rtitti )inisterial tate ents;eather an  li ateervices an  the2 22/2 23 rainfall forecastl   r : I I ,   | l :  I I  tAustraliaBrazilJapanUnitedKingdomZambiaCountryHouse ofRepresentativesDiary of theChamber of theMembers210th Session ofthe Diet House ofCouncilorsCommission onthe ConstitutionNo. 2House ofCommonsThe House,NationalAssemblyLegislatureEd Husic, Australian LaborParty, Minister for Industryand ScienceMr. Gustavo Fruet,Democratic Labor PartyKohei Otsuka, DemocraticParty for the People,ShinryokufukaiDame Angela Eagle, LaborHon. Collins Nzovu, UnitedParty for NationalDevelopment, TheMinister of GreenEconomy and EnvironmentSpeaker\u201cWorking with our international partners we cantransform Australian know-how into globally recognisedskills and manufacturing in defence industries. And wecan build on our undeniable expertise in areas likequantum technologies, robotics and arti cialintelligence. We will seek to partner with industry andstate and territory governments to identify investmentopportunities within priority areas. An on-ramp, if youwill, of turn-key opportunities for investment to makesure the NRF is well placed for success.\u201d\u201cThere has been a lot of talk about the future of work dueto technology. In the book The Fourth IndustrialRevolution, Klaus Schwab even points out professionsthat will be extinct and professions that will demandmore and more quali\ufffdcations, in times of 5G, Internet ofThings and Arti cial Intelligence. In this sense, it is goodto highlight that the pandemic, among othercontradictions, ended up anticipating the use oftechnology, especially in the telework.\u201d\u201cIn the \ufffdeld of human rights, we believe that it isnecessary to update human rights guarantees in order torespond to changes in the times that were unpredictablewhen the Constitution was enacted. In particular, as thefusion of arti cial intelligence and Internet technologyprogresses, the international community is concernedabout the problems of individual scoring anddiscrimination, and the problem of Internet advertisingthat unfairly in\ufffduences the voting behavior of citizens.We need a constitutional argument to guarantee theautonomous decision-making of individuals and protectbasic data rights in the digital age.\u201d\u201cWhat would be the use of arti cial intelligence in tryingto decide how automated these things could become?Would there be worries about over-automation? Howwould that be looked at in terms of regulation? Howopen are we going to be about the way in which AI isapplied and how it might evolve in ways that mightembed discrimination such that we get a system wherecertain people may be discriminated against andexcluded?\u201d\u201cMadam Speaker, in order to enhance quality andaccuracy of weather forecast, the Government, with\ufffdnancial support from the United Nations DevelopmentProgramme Strengthening Climate Resilience ofAgricultural Livelihoods in Agro-Ecological (UNDPSCRALA) project is currently partnering with theUniversity of Zambia (UNZA) to develop a seasonalweather forecasting system using arti cial intelligence.\u201dQuoteNational ReconstructionFund Corporation Bill 2022- Second ReadingPresentation of Bill No.135, of 2022, on theamendment of the CLT -Consolidation of LaborLaws, with a view togranting telework toparents of children up to 8years oldThe Commission on theConstitutionFinancial Services andMarkets Bill (FourthSitting)Ministerial Statements;Weather and ClimateServices and the2022/2023 rainfall forecastAgenda ItemI- elate  arlia entary en tions Fro  Select Countries, 2022Source: AI Index, 2022  | Table: 2023 AI Index ReportThe following subsection examines mentions of AI in government proceedings in 2022. Figure 6.1.15 quotes discussions across a geographically diverse set of countries.Figure 6.1.156.1 AI and PolicymakingChapter 6: Policy and GovernanceTable of Contents 280Artificial IntelligenceIndex Report 2023Chapter 6 Preview107th (2001\u201302)108th (2003\u201304)109th (2005\u201306)110th (2007\u201308)111th (2009\u201310)112th (2011\u201312)113th (2013\u201314)114th (2015\u201316)115th (2017\u201318)116th (2019\u201320)117th (2021\u201322)01020304050607080Number of Mentions73Mentions of AI in U.S. Committee Reports by Legislative Session, 2001\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report United States  Committee MentionsAn additional indicator of legislative interest is the number of mentions of \u201cartificial intelligence\u201d in committee reports produced by House and Senate 6.1 AI and Policymakingcommittees that address legislative and other policy issues, investigations, and internal committee matters. Figure 6.1.16 shows a sharp increase in the total number of mentions of AI within committee reports beginning with the 115th legislative session.Figure 6.1.16Chapter 6: Policy and GovernanceTable of Contents 281Artificial IntelligenceIndex Report 2023Chapter 6 Preview2095332221111111110 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21Ways and MeansSmall BusinessHouse AdministrationHomeland SecurityForeign AairsFinancial ServicesEnergy and CommerceEducation and the WorkforceBudgetOversight and AccountabilityNatural ResourcesIntelligence (Permanent Select)Transportation and InfrastructureArmed ServicesRulesScience, Space, and TechnologyAppropriationsNumber of MentionsMentions of AI in Committee Reports of the U.S. House of Representatives for the 117th CongressionalSession, 2021\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report 833220 1 2 3 4 5 6 7 8Intelligence (Select)Armed ServicesCommerce, Science,and TransportationAppropriationsHomeland Security andGovernmental A airsNumber of MentionsMentions of AI in Committee Reports of the U.S. Senate for the 117th Congressional Session, 2021\u201322Source: AI Index, 2022 | Chart: 2023 AI Index Report Figure 6.1.17 shows the mentions in committee reports for the 117th Congressional Session, which took place from 2021 to 2022. The Appropriations Committee leads the House reports, while the Homeland Security and Governmental Affairs Committee leads the Senate reports (Figure 6.1.18).6.1 AI and PolicymakingFigure 6.1.17Figure 6.1.18Chapter 6: Policy and GovernanceTable of Contents 282Artificial IntelligenceIndex Report 2023Chapter 6 Preview1611109750 2 4 6 8 10 12 14 16Intelligence (Select)Energy andNatural ResourcesCommerce, Science,and TransportationArmed ServicesHomeland Security andGovernmental AairsAppropriationsNumber of MentionsMentions of AI in Committee Reports of the U.S. Senate, 2001\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report 452714988664433222222110 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46House AdministrationAgricultureWays and MeansSmall BusinessNatural ResourcesJudiciaryForeign A airsBudgetVeterans\u2019 A airsHomeland SecurityTransportation and InfrastructureEducation and the WorkforceIntelligence (Permanent Select)Financial ServicesOversight and AccountabilityEnergy and CommerceArmed ServicesRulesScience, Space, and TechnologyAppropriationsNumber of MentionsMentions of AI in Committee Reports of the U.S. House of Representatives, 2001\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report Figure 6.1.19 shows the total number of mentions in committee reports from the past 10 congressional sessions, which took place from 2001 to 2022. The House and Senate Appropriations Committees, which regulate expenditures of money by the government, lead their respective lists (Figure 6.1.19 and 6.1.20).6.1 AI and PolicymakingFigure 6.1.19Figure 6.1.20Chapter 6: Policy and GovernanceTable of Contents 283Artificial IntelligenceIndex Report 2023Chapter 6 Preview2018 2019 2020 2021 2022050100150200250300Number of Policy Papers284Number of AI-Related Policy Papers by U.S.-Based Organizations, 2018\u201322Source: Stanford Institute for Human-Centered AI (HAI) Policy and Society | Chart: 2023 AI Index ReportUnited States AI Policy PapersTo estimate activities outside national governments that are also informing AI-related lawmaking, the AI Index tracked 55 U.S.-based organizations that published policy papers in the past five years. Those organizations include: think tanks and policy institutes (19); university institutes and research programs (14); civil society organizations, associations, and consortiums (9); industry and consultancy organizations (9); and government agencies (4). A policy paper in this section is defined as a research paper, research report, brief, or blog 6.1 AI and Policymakingpost that addresses issues related to AI and makes specific recommendations to policymakers. Topics of those papers are divided into primary and secondary categories: A primary topic is the main focus of the paper, while a secondary topic is a subtopic of the paper or an issue that is briefly explored.Figure 6.1.21 highlights the total number of U.S.-based, AI-related policy papers published from 2018 to 2022. After a slight dip from 2020 to 2021, the total increased to 284 in 2022. Since 2018, the total number of such papers has increased 3.2 times, signaling greater interest over time.Figure 6.1.21Chapter 6: Policy and GovernanceTable of Contents 284Artificial IntelligenceIndex Report 2023Chapter 6 Preview10790825939343430261312984310 20 40 60 80 100Communications and MediaHumanitiesSocial and Behavioral SciencesEnergy and EnvironmentEducation and SkillsPhysical SciencesJustice and Law EnforcementHealth and Biological SciencesDemocracyWorkforce and LaborInt\u2019l Aairs and Int\u2019l SecurityEquity and InclusionEthicsPrivacy, Safety, and SecurityGov\u2019t and Public AdministrationInnovation and TechnologyIndustry and Regulation6965595026251817131010853110 20 40 60 80 100Primary Topic Secondary TopicNumber of AI-Related Policy Papers by U.S.-Based Organization by Topic, 2022Source: Stanford Institute for Human-Centered AI (HAI) Policy and Society | Chart: 2023 AI Index ReportNumber of Policy PapersBy TopicIn 2022, the most frequent primary topics were industry and regulation (107), innovation and technology (90), and government and publication administration (82) (Figure 6.1.22). Privacy, safety, and security, which was the most reported topic in 2021, 6.1 AI and Policymakingsat in fourth position as of 2022. All of these leading topics were also well represented as secondary topics. Topics that received comparatively little attention included social and behavioral sciences; humanities; and communications and media.Figure 6.1.22Chapter 6: Policy and GovernanceTable of Contents 285Artificial IntelligenceIndex Report 2023Chapter 6 PreviewThis subsection presents an overview of national AI strategies\u2014policy plans developed by a country\u2019s government to steer the development and deployment of AI technologies within its borders. Tracking trends in national strategies can be an important way of gauging the degree to which countries are prioritizing the management and regulation of AI technologies. Sources include websites of national or regional governments, the OECD AI Policy Observatory (OECD.AI), and news coverage. \u201cAI strategy\u201d is defined as a policy document that communicates the objective of supporting the development of AI while also maximizing the benefits of AI for society.5201720182019202020212022YearCanada, China, FinlandAustralia, France, Germany, India, Mauritius, Mexico, SwedenArgentina, Austria, Bangladesh, Botswana, Chile, Colombia,Cyprus, Czech Republic, Denmark, Egypt, Estonia, Japan,Kenya, Lithuania, Luxembourg, Malta, Netherlands, Portugal,Qatar, Romania, Russia, Sierra Leone, Singapore, United ArabEmirates, United States of America, UruguayAlgeria, Bulgaria, Croatia, Greece, Hungary, Indonesia, Latvia,Norway, Poland, Saudi Arabia, Serbia, South Korea, Spain,SwitzerlandBrazil, Ireland, Peru, Philippines, Slovenia, Tunisia, Turkey,Ukraine, United Kingdom, VietnamItaly, ThailandCountryYearly Release of AI National Strategies by CountrySource: AI Index, 2022 | Table: 2023 AI Index ReportReleasedIn DevelopmentCountries With a National Strategy on AI, 2022Source: AI Index, 2022 | Chart: 2023 AI Index Report Not ReleasedAggregate TrendsCanada officially launched the first national AI strategy in March of 2017; since then a total of 62 national AI strategies have been released (Figure 6.2.1). The number of released strategies peaked in 2019.By Geographic AreaFigure 6.2.2 highlights the countries which, as of December 2022, have either released or developed a national AI strategy. Figure 6.2.3 enumerates the countries that, in 2021 and 2022, pledged to develop an AI strategy . The first nations to officially release national AI strategies were Canada, China, and Finland in 2017. Only two nations released national AI strategies in 2022: Italy and Thailand.6.2 National AI Strategies6.2 National AI StrategiesFigure 6.2.1Figure 6.2.3Figure 6.2.2Chapter 6: Policy and Governance20212022YearArmenia, Bahrain, Cuba, Iceland, Morocco, New Zealand, OmanAzerbaijan, Belgium, Benin, Israel, Jordan, Nigeria, UzbekistanCountryAI National Strategies in Development by Countryand YearSource: AI Index, 2022 | Table: 2023 AI Index Report5 The AI Index research team made efforts to identify whether there was a national AI strategy that was released or in development for every nation in the world. It is possible that some strategies were missed.Table of Contents 286Artificial IntelligenceIndex Report 2023Chapter 6 PreviewThis section examines public AI investment in the United States based on data from the U.S. government and Govini, a company that uses AI and machine learning technologies to track U.S. public and commercial spending. 0.56 1.11 1.43 1.75  1.73 1.84FY18 (Enacted) FY19 (Enacted) FY20 (Enacted) FY21 (Enacted) FY22 (Enacted) FY23 (Requested)0.000.501.001.50Budget (in Billions of U.S. Dollars)U.S. Federal Budget for Nondefense AI R&D, FY 2018\u201323Source: U.S. NITRD Program, 2022 | Chart: 2023 AI Index ReportFederal Budget for  Nondefense AI R&DIn December 2022, the National Science and Technology Council published a report on the public-sector AI R&D budget across departments and agencies participating in the Networking and Information Technology Research and Development (NITRD) Program and the National Artificial Intelligence Initiative. The report does not include information on classified AI R&D investment by defense and intelligence agencies.In fiscal year (FY) 2022, nondefense U.S. government agencies allocated a total of $1.7 billion to AI R&D spending (Figure 6.3.1). The amount allocated in FY 2022 represented a slight decline from FY 2021 and a 208.9% increase from FY 2018. An even greater amount, $1.8 billion, has been requested for FY 2023.6.3 U.S. Public Investment in AI6.3 U.S. Public Investment in AIFigure 6.3.16Chapter 6: Policy and Governance6 A previous report on the public-sector AI R&D budget released in 2021 classed the FY21 spending as totaling $1.53 billion. However, the most recent report, released in 2022, upgraded the total spent in 2022 to $1.75 billion.Table of Contents 287Artificial IntelligenceIndex Report 2023Chapter 6 Preview 0.93 0.84 0.87 1.10FY20 Funding FY21 Funding FY22 Funding FY23 Funding0.000.200.400.600.801.00Budget Request (in Billions of U.S. Dollars)U.S. DoD Budget Request for AI-Specific Research, Development, Test, and Evaluation (RDT&E), FY 2020\u201323Source: U.S. Office of the Under Secretary of Defense (Comptroller), 2022 | Chart: 2023 AI Index ReportU.S. Department of Defense Budget RequestsEvery year the DoD releases the amount of funding they have requested for nonclassified AI-specific research, development, test, and evaluation. According to the 2022 report, the DoD requested $1.1 billion in FY 2023, a 26.4% increase from the funding they received in FY 2022 (Figure 6.3.2).6.3 U.S. Public Investment in AIFigure 6.3.2Chapter 6: Policy and GovernanceTable of Contents 288Artificial IntelligenceIndex Report 2023Chapter 6 Preview0.320.46 0.550.731.011.190.430.440.410.460.530.820.210.310.450.580.430.410.240.260.30.430.520.690.210.210.171.291.561.832.412.703.282017 2018 2019 2020 2021 20220.000.501.001.502.002.503.003.50Decision Science Computer Vision Machine Learning Autonomy Natural Language ProcessingU.S. Government Spending (in Billions of U.S. Dollars)U.S. Government Spending by Segment, FY 2017\u201322Source: Govini, 2022 | Chart: 2023 AI Index ReportU.S. Government AI-Related Contract SpendingPublic investment in AI can also be measured by federal government spending on the contracts that U.S. government agencies award to private companies for the supply of goods and services. Such contracts typically occupy the largest share of an agency\u2019s budget.Data in this section comes from Govini, which created a taxonomy of spending by the U.S. government on critical technologies including AI. Govini applied supervised machine learning and natural language processing to parse, analyze, and categorize large volumes of federal contracts data, including prime contracts, grants, and other transaction authority (OTA) awards. The use of AI models enables Govini to analyze data that is otherwise often inaccessible.Total Contract SpendingFigure 6.3.3 highlights total U.S. government spending on AI, subdivided by various AI segments. From 2021 to 2022, total AI spending increased from $2.7 billion to $3.3 billion. Since 2017, total spending has increased nearly 2.5 times. In 2022, the AI subsegments that saw the greatest amount of government spending included decision science ($1.2 billion), and computer vision ($0.8 billion).6.3 U.S. Public Investment in AIFigure 6.3.3Chapter 6: Policy and GovernanceTable of Contents 289Artificial IntelligenceIndex Report 2023Chapter 6 Preview0.210.430.520.531.010.17 (-19%)0.41 (-5%)0.69 (+33%)0.82 (+55%)1.19 (+18%)0.00 0.20 0.40 0.60 0.80 1.00 1.20Natural LanguageProcessingMachine LearningAutonomyComputer VisionDecision Science20222021U.S. Government Spending (in Billions of U.S. Dollars)U.S. Government Spending by Segment, FY 2021 Vs. 2022Source: Govini, 2022 | Chart: 2023 AI Index ReportFigure 6.3.4 shows U.S. government spending by AI segment in FY 2021 and FY 2022. Spending increased for the decision science, computer vision, and autonomy segments, while spending on machine learning, and natural language processing dropped slightly.6.3 U.S. Public Investment in AIFigure 6.3.4Chapter 6: Policy and GovernanceTable of Contents 290Artificial IntelligenceIndex Report 2023Chapter 6 Preview2017 2018 2019 2020 2021 20220.000.501.001.502.00Total Value Awarded (in Billions of U.S. Dollars)0.09, OTAs1.15, Grants2.05, ContractsTotal Value of Contracts, Grants, and OTAs Awarded by the U.S. Government for AI/ML and Autonomy,FY 2017\u201322Source: Govini, 2022 | Chart: 2023 AI Index ReportIn FY 2022, the majority of federal AI contracts were prime contracts (62.5%), followed by grants (34.9%) and other transaction authority (OTA) awards (2.6%) (Figure 6.3.5). From FY 2021 to FY 2022, the share of contracts remained about the same, while the share of grants rose.6.3 U.S. Public Investment in AIFigure 6.3.5Chapter 6: Policy and GovernanceTable of Contents 291Artificial IntelligenceIndex Report 2023Chapter 6 PreviewIn 2022, the AI Index partnered with Elif Kiesow Cortez, a scholar of artificial intelligence law, in a research project tracking trends in American legal cases from 2000 to 2022 that contain AI-related keywords.72000 2002 2004 2006 2008 2010 2012 2014 2016 2018 2020 2022020406080100Number of AI-Related Legal Cases110Number of AI-Related Legal Cases in the United States, 2000\u201322Source: AI Index, 2022 | Chart: 2023 AI Index ReportTotal CasesIn the last few years, there has been a sharp spike in AI-related jurisprudence in the United States. In 2022, 6.4 U.S. AI-Related Legal Cases6.4 U.S. AI-Related Legal CasesFigure 6.4.1Chapter 6: Policy and Governance7 The Index analyzed both federal and state-level cases. Specific keywords in the search included \u201cartificial intelligence,\u201d \u201cmachine learning,\u201d and \u201cautomated decision-making.\u201d Some of these cases did not directly concern issues related to AI jurisprudence. As a next step of this project, we will aim to identify the cases that most centrally concern issues of AI-related law.there were a total of 110 AI-related cases in U.S. federal and state courts, 6.5 times more than in 2016 (Figure 6.4.1).Table of Contents 292Artificial IntelligenceIndex Report 2023Chapter 6 Preview2317117754443333220 2 4 6 8 10 12 14 16 18 20 22 24MissouriVirginiaPennsylvaniaOhioTexasDistrict of ColumbiaMarylandMassachusettsKansasWashingtonFloridaDelawareNew YorkIllinoisCaliforniaNumber of AI-Related Legal CasesNumber of AI-Related Legal Cases in the United States by State, 2022Source: AI Index, 2022 | Chart: 2023 AI Index ReportGeographic DistributionIn 2022, the majority of AI-related legal cases originated in California (23), Illinois (17), and New York (11) (Figure 6.4.2). The aggregate number of AI-related cases since 2000 show a similar geographic distribution (Figure 6.4.3). California and New York\u2019s inclusion in the top three is unsurprising given that 6.4 U.S. AI-Related Legal CasesFigure 6.4.28Chapter 6: Policy and Governance8 Figures 6.4.2 and 6.4.3 include information for states and districts, given that cases sometimes originate from American districts like the District of Columbia or Puerto Ricothey are home to many large businesses that have integrated AI. In recent years, there have been a greater number of AI-related legal cases originating from Illinois\u2014this follows the state\u2019s enactment of the Biometric Information Privacy Act (BIPA), which requires that companies doing business in Illinois follow a number of regulations related to the collection and storage of biometric information.Table of Contents 293Artificial IntelligenceIndex Report 2023Chapter 6 Preview127663626191918161212121210980 10 20 30 40 50 60 70 80 90 100 110 120 130MinnesotaKansasOhioFloridaDistrict of ColumbiaVirginiaMichiganPennsylvaniaWashingtonMassachusettsDelawareTexasIllinoisNew YorkCaliforniaNumber of AI-Related Legal CasesNumber of AI-Related Legal Cases in the United States by State, 2000\u201322 (Sum)Source: AI Index, 2022 | Chart: 2023 AI Index Report6.4 U.S. AI-Related Legal CasesFigure 6.4.3Chapter 6: Policy and Governance48181466443221110 4 8 12 16 20 24 28 32 36 40 44 48Oil and Gas Production, Oil ReningMechanical and Electrical EngineeringBasic Metal ProductionTransport Equipment ManufacturingFood, Drink, TobaccoTransport (Including Civil Aviation,Railways, Road Transport)Postal and Telecommunications ServicesHotels, Catering, TourismHealth ServicesEducationPublic ServiceMedia, Culture, GraphicalFinancial Services, Professional ServicesNumber of AI-Related Legal CasesSector at Issue in AI-Related Legal Cases in the United States, 2022Source: AI Index, 2022 | Chart: 2023 AI Index ReportSectorFigure 6.4.4 groups U.S.-based legal cases by economic sector. The predominant sector in 2022 was financial services and professional services (48 cases); followed by media, culture, graphical (18); and public service (14).Figure 6.4.4Table of Contents 294Artificial IntelligenceIndex Report 2023Chapter 6 Preview32211511865432210 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32Tort  Terrorism and National Security Financial Corporate Criminal Employment andLabor Constitutional Competition Contract IntellectualProperty Civil Number of AI-Related Legal CasesArea of Law of AI-Related Legal Cases in the United States, 2022Source: AI Index, 2022 | Chart: 2023 AI Index ReportHuman Rights andImmigrationType of LawThe greatest proportion of AI-related legal cases concerned civil law (29%) (Figure 6.4.5). There were also a large number of AI-related legal cases in the domain of intellectual property (19%), as well as contract law (13.6%).Figure 6.4.56.4 U.S. AI-Related Legal CasesChapter 6: Policy and GovernanceTable of Contents 295Artificial IntelligenceIndex Report 2023Chapter 6 PreviewThree Significant AI-Related Legal CasesNarrative Highlight: The section below profiles three significant AI-related cases in the United States, highlighting some of the legal issues that are at stake when AI is brought into the courts.6.4 U.S. AI-Related Legal CasesChapter 6: Policy and Governance9 The defendant was Tina M. Stanford, as Chairwoman of the New York State Board of Parole.Duerr v. Bradley University (2022-Mar-10) \u2013 United States Court of Appeals for the Seventh CircuitThe plaintiffs, who were enrolled as undergraduates in a private university in Peoria, Illinois, during the fall 2020 semester, were told to use a third-party proctoring tool called Respondus Monitor for remote, online exams. This tool made use of artificial intelligence technologies. The plaintiffs claimed that the defendants violated Illinois\u2019 Biometric Information Privacy Act (BIPA) by not adequately following its guidelines concerning the collection of biometric information. BIPA does not apply to financial institutions. Ultimately, the court ruled that under the Gramm-Leach-Bliley Act, the defendants were a financial institution by virtue of lending functions they engaged in and therefore exempt from BIPA. As such, the plaintiff\u2019s case was dismissed.Flores v. Stanford9 (2021-Sep-28) \u2013 United States Court of Appeals for the Second CircuitThe plaintiffs, offenders denied parole, sued the New York State Board of Parole over being refused access to information used by the board in its review of their cases. Northpointe, Inc., petitioned the court as a non-party because its Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), an AI-powered risk assessment tool, had been used by the parole board in its determinations. Northpointe wanted to prevent the disclosure of AI trade secrets to one of the plaintiff\u2019s expert witnesses. The court ruled that the confidential material in question was relevant to the plaintiff\u2019s case and posed little risk of competitive injury. As such, the material was ordered to be released under a supplemental protective order.Dyroff v. Ultimate Software Grp., Inc (2017-Nov-26) \u2013 United States Court of Appeals for the Ninth CircuitPlaintiff Kristanalea Dyroff sued Ultimate Software after her 29-year-old son died from an overdose of heroin laced with fentanyl, which he allegedly bought from a drug dealer that he encountered on Ultimate Software\u2019s social network site. Dyroff asserted seven claims against Ultimate Software which included negligence, wrongful death, and civil conspiracy. At the core of these claims was the argument that Ultimate Software mined the data of users and deployed that data, alongside an algorithm, to recommend drug-related discussion groups to her son. Ultimate Software moved to dismiss the claims and claimed partial immunity under the Communications Decency Act, which protects website operators from liability for third-party content on their site. The Court ruled that Ultimate Software was immune and that its use of algorithms did not sufficiently amount to novel content creation.Table of Contents Chapter 7 Preview 296Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023CHAPTER 7: DiversityTable of Contents Chapter 7 Preview 297Artificial IntelligenceIndex Report 2023Overview 298Chapter Highlights 2997.1 AI Conferences 300Women in Machine Learning (WiML)  NeurIPS Workshop 300 Workshop Participants 300 Demographic Breakdown 3017.2 AI Postsecondary Education 305CS Bachelor\u2019s Graduates 305CS Master\u2019s Graduates 307CS PhD Graduates 309  Narrative Highlight:    Disability Status of CS, CE,   and Information Students 311New AI PhDs 312CS, CE, and Information Faculty 3137.3 K\u201312 Education 316AP Computer Science: Gender 316AP Computer Science: Ethnicity 318DiversityCHAPTER 7 PREVIEW:ACCESS THE PUBLIC DATATable of Contents Chapter 7 Preview 298Artificial IntelligenceIndex Report 2023OverviewAI systems are increasingly deployed in the real world. However, there often exists a disparity between the individuals who develop AI and those who use AI. North American AI researchers and practitioners in both industry and academia are predominantly white and male. This lack of diversity can lead to harms, among them the reinforcement of existing societal inequalities and bias.This chapter highlights data on diversity trends in AI, sourced primarily from academia. It borrows information from organizations such as Women in Machine Learning (WiML), whose mission is to improve the state of diversity in AI, as well as the Computing Research Association (CRA), which tracks the state of diversity in North American academic computer science. Finally, the chapter also makes use of Code.org data on diversity trends in secondary computer science education in the United States.Note that the data in this subsection is neither comprehensive nor conclusive. Publicly available demographic data on trends in AI diversity is sparse. As a result, this chapter does not cover other areas of diversity, such as sexual orientation. The AI Index hopes that as AI becomes more ubiquitous, the amount of data on diversity in the field will increase such that the topic can be covered more thoroughly in future reports.Chapter 7: DiversityTable of Contents Chapter 7 Preview 299Artificial IntelligenceIndex Report 2023Chapter HighlightsNorth American bachelor\u2019s, master\u2019s, and PhD-level computer science students are becoming more ethnically diverse. Although white students are still the most represented ethnicity among new resident bachelor\u2019s, master\u2019s, and PhD-level computer science graduates, students from other ethnic backgrounds (for example, Asian, Hispanic, and Black or African American) are becoming increasingly  more represented. For example, in 2011, 71.9% of new resident CS bachelor\u2019s graduates were white. In 2021, that number dropped to 46.7%.Chapter 7: DiversityNew AI PhDs are still overwhelmingly male.  In 2021, 78.7% of new AI PhDs were male. Only 21.3% were female, a 3.2 percentage point increase from 2011. There continues to be a gender imbalance in higher-level AI education.American K\u201312 computer science education has become more diverse,  in terms of both gender and ethnicity.  The share of AP computer science exams taken by female students increased from 16.8% in 2007 to 30.6% in 2021. Year over year, the share of Asian, Hispanic/Latino/Latina, and Black/African American students  taking AP computer science has likewise increased.Women make up an increasingly greater share of CS, CE, and information faculty hires.  Since 2017, the proportion of new female CS, CE, and information faculty hires has increased from 24.9% to 30.2%. Still, most CS, CE, and information faculty in North American universities are male (75.9%). As of 2021, only 0.1% of CS, CE, and information faculty identify as nonbinary.Table of Contents Chapter 7 Preview 300Artificial IntelligenceIndex Report 20232010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 202202004006008001,0001,2001,400Number of Attendees1,157Attendance at NeurIPS Women in Machine Learning Workshop, 2010\u201322Source: Women in Machine Learning, 2022 | Chart: 2023 AI Index ReportWomen in Machine Learning (WiML) NeurIPS WorkshopWomen in Machine Learning (WiML),  founded in 2006, is an organization dedicated to supporting and increasing the impact of women in machine learning. This subsection of the AI Index report presents data from the WiML annual technical workshop, hosted at NeurIPS. Since 2020, WiML has also been hosting the Un-Workshop, which serves to advance research via 7.1 AI Conferencescollaboration and interaction among participants from diverse backgrounds at the International Conference of Machine Learning (ICML).Workshop ParticipantsFigure 7.1.1 shows the number of participants that have attended the WiML workshop since 2010. In the last decade, there has been a steady increase: 1,157 individuals participated in 2022, 13 times the number in 2010. However, from 2021 to 2022, the number of workshop participants decreased from 1,486 to 1,157.17.1 AI ConferencesFigure 7.1.1Chapter 7: Diversity1 The recent decrease in WiML workshop attendance may be attributable to the overall recent decrease in NeurIPS attendance. This overall decrease may in turn be a result of NeurIPS moving away from a purely virtual format.Table of Contents Chapter 7 Preview 301Artificial IntelligenceIndex Report 20230.20%1.40%1.60%3.40%17.10%34.20%41.50%0% 5% 10% 15% 20% 25% 30% 35% 40% 45%AntarcticaAustralia/OceaniaSouthAmericaAfricaAsiaEuropeNorthAmerica% of RespondentsContinent of Residence of Participants at NeurIPS Women in Machine Learning Workshop, 2022Source: Women in Machine Learning, 2022 | Chart: 2023 AI Index ReportDemographic BreakdownFigure 7.1.2 breaks down the continent of residence of the 2022 workshop participants. The data in the following figures comes from a survey completed by participants who consented to having such information aggregated. Among survey respondents, around 41.5% were from North America, followed by Europe (34.2%), Asia (17.1%), and Africa (3.4%). In 2022, there was greater representation from Europe, Asia, and South America.7.1 AI ConferencesFigure 7.1.22Chapter 7: Diversity2 At the time of the survey, one of the respondents was temporarily residing in Antarctica.Table of Contents Chapter 7 Preview 302Artificial IntelligenceIndex Report 20230.20%0.20%0.50%25.80%36.30%37.00%0% 5% 10% 15% 20% 25% 30% 35% 40%GenderNon-ConformingGender FluidNonbinaryMalePrefer Not to SayFemale% of RespondentsGender Breakdown of Participants at NeurIPS Women in Machine Learning Workshop, 2022Source: Women in Machine Learning, 2022 | Chart: 2023 AI Index ReportThe majority of participants at the 2022 WiML workshop were female-identifying (37.0%), another 25.8% were male-identifying, and 0.5% were nonbinary-identifying (Figure 7.1.3).7.1 AI ConferencesFigure 7.1.3Chapter 7: DiversityTable of Contents Chapter 7 Preview 303Artificial IntelligenceIndex Report 20230.20%1.40%1.60%2.00%2.30%3.50%3.50%3.50%4.40%8.40%20.80%49.40%0% 10% 20% 30% 40% 50%Recent GraduateLecturerRecruiterUndergraduateStudentMSc StudentPostdocOthersCEO/DirectorFacultySoftware Engineer/Data EngineerResearch Scientist/Data ScientistPhD Student% of RespondentsProfessional Positions of Participants at NeurIPS Women in Machine Learning Workshop, 2022Source: Women in Machine Learning, 2022 | Chart: 2023 AI Index ReportThe most represented professional positions at the workshop were PhD students (49.4%), research scientists/data scientists (20.8%), software engineers/data engineers (8.4%), and faculty (4.4%) (Figure 7.1.4).7.1 AI ConferencesFigure 7.1.4Chapter 7: DiversityTable of Contents Chapter 7 Preview 304Artificial IntelligenceIndex Report 20231.00%1.00%3.30%3.80%5.30%7.20%7.70%14.80%23.40%32.50%0% 5% 10% 15% 20% 25% 30%TheoryOptimization Probabilistic MethodsNeuroscience and Cognitive Science Data, Challenges, Implementations, SoftwareReinforcement Learning and PlanningDeep LearningSocial Aspects of Machine LearningAlgorithmsApplications% of RespondentsPrimary Subject Area of Submissions at NeurIPS Women in Machine Learning Workshop, 2022Source: Women in Machine Learning, 2022 | Chart: 2023 AI Index ReportThe WiML workshop participants at NeurIPS submitted papers covering a wide range of subjects (Figure 7.1.5). The most popular submission topics were applications (32.5%), algorithms (23.4%), and deep learning (14.8%).7.1 AI ConferencesFigure 7.1.5Chapter 7: DiversityTable of Contents Chapter 7 Preview 305Artificial IntelligenceIndex Report 2023Another proxy for studying diversity in AI is looking at trends in postsecondary AI education. The following subsection borrows data from the Computing Research Association\u2019s (CRA) annual Taulbee Survey.32010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%90%New CS Bachelor\u2019s Graduates (% of Total)0.04%, Nonbinary/Other22.30%, Female77.66%, MaleGender of New CS Bachelor\u2019s Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportCS Bachelor\u2019s GraduatesThe number of female CS bachelor\u2019s graduates rose to 22.3% from 2020 to 2021 (Figure 7.2.1). This increase mirrors a broader trend observed in the 7.2 AI Postsecondary Educationlast decade whereby an increasingly large number of CS bachelor\u2019s graduates were women. The CRA survey also included a nonbinary gender category: In 2021, the number of nonbinary/other-identifying CS bachelor\u2019s graduates was 0.04%.7.2 AI Postsecondary EducationFigure 7.2.1Chapter 7: Diversity3 The charts in this subsection look only at the ethnicity of domestic or native CS students and faculty. Although the CRA reports data on the proportion of nonresident aliens in each educational level (i.e., Bachelor\u2019s, Master\u2019s, PhD, and faculty), data on the ethnicity of nonresident aliens is not included. For the proportion of nonresident aliens in each category, see footnotes.Table of Contents Chapter 7 Preview 306Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%New CS Bachelor\u2019s Graduates (% of Total)0.22%, American Indian or Alaska Native0.24%, Native Hawaiian or Pacic Islander3.85%, Black or African-American4.10%, Multiracial (Not Hispanic)10.91%, Hispanic (Any Race)33.99%, Asian46.69%, WhiteEthnicity of New Resident CS Bachelor\u2019s Graduates (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 7.2.2 breaks down the ethnicity of new CS bachelor\u2019s graduates in North America: The top ethnicity was white (46.7%), followed by Asian (34.0%) and Hispanic (10.9%). In the last decade, the proportion of new CS bachelor\u2019s graduates who were Asian, Hispanic, or multiracial (not Hispanic) steadily increased.4Figure 7.2.2Chapter 7: Diversity4 In 2021, 16.3% of new CS bachelor graduates were nonresident aliens.7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 307Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%New CS Master\u2019s Graduates (% of Total)0.90%, Nonbinary/Other27.83%, Female71.27%, MaleGender of New CS Master\u2019s Graduates (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportCS Master\u2019s GraduatesFigure 7.2.3 shows the gender of CS master\u2019s graduates. The proportion of female CS master\u2019s graduates has not substantially increased over time, moving to 27.8% in 2021 from 24.6% in 2011. In 2021, 0.9% of CS master\u2019s graduates identified as nonbinary/other.Figure 7.2.3Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 308Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%New CS Master\u2019s Graduates (% of Total)0.12%, Native Hawaiian or Pacic Islander0.25%, American Indian or Alaska Native3.45%, Multiracial (Not Hispanic)3.82%, Black or African-American7.25%, Hispanic (Any Race)34.83%, Asian50.28%, WhiteEthnicity of New Resident CS Master\u2019s Graduates (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportOf domestic students, the most represented ethnicities are white (50.3%), followed by Asian (34.8%), and Hispanic (7.3%) (Figure 7.2.4). As with CS bachelor\u2019s graduates, in the last decade white students have represented an increasingly smaller proportion of new CS master\u2019s graduates.5Figure 7.2.4Chapter 7: Diversity5 In 2021, 65.2% of new CS master\u2019s graduates were nonresident aliens.7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 309Artificial IntelligenceIndex Report 20232010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%New CS PhD Graduates (% of Total)0.12%, Nonbinary/Other23.30%, Female76.58%, MaleGender of New CS PhD Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportCS PhD GraduatesIn 2021, the number of new female CS PhD graduates rose to 23.3% from 19.9% (Figure 7.2.5). Despite this rise, most new CS PhD graduates continue to be male. There remains a large gap between new male and female CS PhDs.Figure 7.2.5Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 310Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%New CS PhD Graduates (% of Total)0.43%, Native Hawaiian or Pacic Islander0.64%, American Indian or Alaska Native2.13%, Multiracial (Not Hispanic)4.05%, Black or African-American5.12%, Hispanic (Any Race)29.00%, Asian58.64%, WhiteEthnicity of New Resident CS PhD Graduates (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportBetween 2011 and 2021, the number of new white resident CS PhD graduates declined by 9.4 percentage points. Asians are the next most represented group (29%), followed by Hispanics (5.1%) and Black or African Americans (4%) (Figure, 7.2.6).6Figure 7.2.6Chapter 7: Diversity7.2 AI Postsecondary Education6 In 2021, 68.6% of new CS PhD graduates were nonresident aliens.Table of Contents Chapter 7 Preview 311Artificial IntelligenceIndex Report 2023The 2021 edition of the CRA Taulbee Survey was the first to gather information about the prevalence of CS, CE, and information students with disabilities. The CRA asked departments to identify the number of students at each degree level who received disability accommodations in the last year.  The number of such students was relatively small. Only 4.0% of bachelor\u2019s, 1.0% of PhD students, and 0.8% of master\u2019s students reported needing accommodations (Figure 7.2.7).Disability Status of CS, CE, and Information StudentsNarrative Highlight: 4.10%1.00%0.80%Bachelor\u2019s PhDs Master\u2019s0%1%2%3%4%CS, CE, and Information Students (% of Total)CS, CE, and Information Students (% of Total) With Disability Accomodations in North America, 2021Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportFigure 7.2.7Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 312Artificial IntelligenceIndex Report 20232010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%New AI PhD Graduates (% of Total)21.30%, Female78.70%, MaleGender of New AI PhD Graduates (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportNew AI PhDsFigure 7.2.8 looks at demographic trends for new AI PhD graduates who focus on artificial intelligence. In 2021, 78.7% of new AI PhDs were male and 21.3% were female. While the number of female AI PhDs marginally increased from 2020 to 2021, we find no meaningful trends in the last decade relating to the gender of new AI PhDs.Figure 7.2.8Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 313Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%CS, CE, and Information Faculty (% of Total)0.12%, Nonbinary/Other23.94%, Female75.94%, MaleGender of CS, CE, and Information Faculty (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportCS, CE, and  Information FacultyData on the ethnicity and gender of CS, CE, and information faculty helps to paint a picture of diversity trends in academic AI and CS. As of 2021, most CS, CE, and information faculty members are predominantly male (75.9%) (Figure 7.2.9). Women make up 23.9% of CS, CE, and information faculty, and nonbinary individuals make up 0.1%. The share of female CS, CE, and information faculty has slowly increased; since 2011, the number of female faculty members has risen 5 percentage points.Figure 7.2.9Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 314Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%New CS, CE, and Information Faculty Hires (% of Total)0.57%,  Nonbinary/Other30.17%, Female69.26%, MaleGender of New CS, CE, and Information Faculty Hires (% of Total) in North America, 2011\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportAlthough most new CS, CE, and information faculty hires in North American universities are still male, the proportion of women among faculty hires reached 30.2% in 2021, up about 9 percentage points from 2015 (Figure 7.2.10).Figure 7.2.10Chapter 7: Diversity7.2 AI Postsecondary EducationTable of Contents Chapter 7 Preview 315Artificial IntelligenceIndex Report 20232011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%CS, CE, and Information Faculty (% of Total)0.13%, Native Hawaiian or Pacic Islander0.25%, American Indian or Alaska Native0.67%, Multiracial (Not Hispanic)2.54%, Black or African-American2.80%, Hispanic (Any Race)5.82%, Unknown29.70%, Asian58.08%, WhiteEthnicity of Resident CS, CE, and Information Faculty (% of Total) in North America, 2010\u201321Source: CRA Taulbee Survey, 2022 | Chart: 2023 AI Index ReportThe majority of resident CS, CE, and information faculty are white as of 2021 (58.1%), followed by Asian (29.7%) (Figure 7.2.11). However, the gap between white CS, CE, and information faculty and faculty of the next nearest ethnicity is slowly narrowing: In 2011, the gap stood at 46.1%, whereas in 2021 it dropped to 28.4%.7Figure 7.2.11Chapter 7: Diversity7.2 AI Postsecondary Education7 In 2021, 6.7% of CS, CE, and information faculty in North America were nonresident aliens.Table of Contents Chapter 7 Preview 316Artificial IntelligenceIndex Report 2023How do trends in AI diversity measure at the K\u201312 level, prior to students entering university? This subsection borrows data from Code.org, an American nonprofit that aims to promote K\u201312 computer science education in the United States.2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%70%80%AP Computer Science Exams Taken (% of Total)0.26%, Other30.58%, Female69.16%, MaleAP Computer Science Exams Taken (% of Total) by Gender, 2007\u201321 Source: Code.org, 2022 | Chart: 2023 AI Index ReportAP Computer Science: GenderIn 2021, 69.2% of AP computer science exams were taken by male students, 30.6% by female students, and 0.3% by students who identified as neither male 7.3 K\u201312 Educationnor female (Figure 7.3.1). It is still the case that male students take more AP computer science exams than any other gender, but the proportion of female students has almost doubled in the last decade.7.3 K\u201312 EducationFigure 7.3.1Chapter 7: DiversityTable of Contents Chapter 7 Preview 317Artificial IntelligenceIndex Report 2023AK20%AL36%AR29%AZ27%CA31%CO26%CT30%DC36%DE22%FL31%GA29%HI30%IA24%ID26%IL32%IN23%KS15%KY31%LA35%MA30%MD35%ME27%MI30%MN23%MO22%MS33%MT21%NC31%ND16%NE25%NH24%NJ31%NM29%NV35%NY35%OH27%OK25%OR21%PA27%RI31%SC34%SD15%TN35%TX30%UT23%VA28%VT23%WA32%WI23%WV30%WY31%AP Computer Science Exams Taken by Female Students (% of Total),2021Source: Code.org, 2022 | Chart: 2023 AI Index ReportOn a percent basis, the states with the largest number of female AP computer science test-takers were Alabama (36%) and Washington, D.C. (36%), followed by Nevada (35%), Louisiana (35%), Tennessee (35%), Maryland (35%), and New York (35%) (Figure 7.3.2). Other states with notable CS and AI activity include California, Texas, and Washington, with rates of women taking AP computer science tests at rates hovering around 30 percent.7.3 K\u201312 EducationFigure 7.3.2Chapter 7: DiversityTable of Contents Chapter 7 Preview 318Artificial IntelligenceIndex Report 2023 7.3 K\u201312 EducationChapter 7: DiversityAP Computer Science: EthnicityCode.org collects data that speaks to trends in the ethnicity of AP computer science test-takers. White students took the greatest proportion of the exams in 2021 (42.7%), followed by Asian (28.8%) and Hispanic/Latino/Latina students (16.5%) (Figure 7.3.3). As with most postsecondary computer science fields, the pool of AP computer science test-takers is becoming more ethnically diverse over time. White students are still the greatest test-taking group; however, over time, more Asian, Hispanic/Latino/Latina and Black/African American students have taken AP computer science exams.2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 20210%10%20%30%40%50%60%AP Computer Science Exams Taken (% of Total Responding Students)0.00%, Other0.15%, Native Hawaiian/Pacic Islander0.62%, Native American/Alaskan4.92%, Two or More Races6.32%, Black/African American16.48%, Hispanic/Latino/Latina28.78%, Asian 42.74%, White AP Computer Science Exams Taken (% of Total Responding Students) by Race/Ethnicity, 2007\u201321Source: Code.org, 2022 | Chart: 2023 AI Index ReportFigure 7.3.3Table of Contents Chapter 8 Preview 319Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023CHAPTER 8: Public OpinionTable of Contents Chapter 8 Preview 320Artificial IntelligenceIndex Report 2023Public OpinionCHAPTER 8 PREVIEW:Overview 321Chapter Highlights 3228.1 Survey Data 323Global Insights 323 AI Products and Services 323 AI: Harm or Help? 327United States 329 Narrative Highlight:  How Does the  Natural Language Processing (NLP)  Research Community Feel About AI? 3348.2 Social Media Data 340Dominant Models 340ACCESS THE PUBLIC DATATable of Contents Chapter 8 Preview 321Artificial IntelligenceIndex Report 2023OverviewAI has the potential to have a transformative impact on society. As such it has become increasingly important to monitor public attitudes toward AI. Better understanding trends in public opinion is essential in informing decisions pertaining to AI\u2019s development, regulation, and use.This chapter examines public opinion through global, national, demographic, and ethnic lenses. Moreover, we explore the opinions of AI researchers, and conclude with a look at the social media discussion that surrounded AI in 2022. We draw on data from two global surveys, one organized by IPSOS, and another by Lloyd\u2019s Register Foundation and Gallup, along with a U.S-specific survey conducted by PEW Research.It is worth noting that there is a paucity of longitudinal survey data related to AI asking the same questions of the same groups of people over extended periods of time. As AI becomes more and more ubiquitous, broader efforts at understanding AI public opinion will become increasingly important.Chapter 8: Public OpinionTable of Contents Chapter 8 Preview 322Artificial IntelligenceIndex Report 2023Chapter HighlightsChinese citizens are among  those who feel the most positively about AI products  and services. Americans \u2026  not so much.  In a 2022 IPSOS survey, 78% of Chinese respondents (the highest proportion of surveyed countries) agreed with the statement that products and services using AI have more benefits than drawbacks. After Chinese respondents, those from Saudi Arabia (76%) and India (71%) felt the most positive about AI products. Only 35% of sampled Americans (among the lowest of surveyed countries) agreed that products and services using AI had more benefits than drawbacks.Men tend to feel more  positively about AI products  and services than women.  Men are also more likely than women to believe that AI will mostly help rather than harm.  According to the 2022 IPSOS survey, men are more likely than women to report that AI products and services make their lives easier, trust companies that use AI, and feel that AI products and services have more benefits than drawbacks. A 2021 survey by Gallup and Lloyd\u2019s Register Foundation likewise revealed that men are more likely than women to agree with the statement that AI will mostly help rather than harm their country in the next 20 years.People across the world and especially America remain unconvinced by self-driving cars.  In a global survey, only 27% of respondents reported feeling safe in a self-driving car. Similarly, Pew Research suggests that only 26% of Americans feel that driverless passenger vehicles are a good idea for society.Different causes  for excitement  and concern.  Among a sample of surveyed Americans, those who report feeling excited about AI are most excited about the potential to make life and society better (31%) and to save time and make things more efficient (13%). Those who report feeling more concerned worry about the loss of human jobs (19%); surveillance, hacking, and digital privacy (16%); and the lack of human connection (12%).NLP researchers \u2026 have some strong opinions as well. According to a survey widely distributed to NLP researchers, 77% either agreed or weakly agreed that private AI firms have too much influence, 41% said that NLP should be regulated, and 73% felt that AI could soon lead to revolutionary societal change. These were some of the many strong opinions held by the NLP research community.Chapter 8: Public OpinionTable of Contents Chapter 8 Preview 323Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionGlobal InsightsHow do opinions of AI vary across the globe? The first subsection of this chapter provides a response by looking at survey data from IPSOS and Pew Research, as well as one poll that was a collaboration of Gallup and Lloyd\u2019s Register Foundation. The surveys suggest that public perceptions concerning AI differ across countries and by demographic groups.AI Products and ServicesIn late 2021, IPSOS ran a survey on global attitudes toward AI products and services. The survey consisted of interviews with 19,504 adults ages 16\u201374 in 28 different countries.1Figure 8.1.1 highlights global opinions (aggregated results across the entire survey subsample) for a variety of questions relating to AI products and services. It shows the percentage of respondents who agree with a particular question. The majority of the survey sample, 60%, believe that AI products and services will profoundly change their daily life in the near future\u2014and make their life easier. A very slight majority, 52%, feel that products and services that use AI have more benefits than drawbacks. Only 40% of respondents report that AI products and services make them feel nervous.64%60%60%52%50%50%49%39%0% 10% 20% 30% 40% 50% 60%Products and services using artificial intelligence have profoundly changed my daily lifein the past 3\u20135 yearsProducts and services using artificial intelligence make me nervousI trust companies that use artificial intelligence as much as I trust other companiesI know which types of products and services use artificial intelligenceProducts and services using artificial intelligence will profoundly change my daily lifein the next 3\u20135 yearsProducts and services using artificial intelligence make my life easierProducts and services using artificial intelligence have more benefits than drawbacksI have a good understanding of what artificial intelligence is% of Respondents That \u201cAgree\u201dGlobal Opinions on Products and Services Using AI (% of Total), 2022Source: IPSOS, 2022 | Chart: 2023 AI Index Report8.1 Survey DataFigure 8.1.11 See Appendix for more details about the survey methodology.Table of Contents Chapter 8 Preview 324Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionOpinions vary widely across countries as to the relative advantages and disadvantages of AI. The IPSOS survey suggests that 78% of Chinese respondents, 76% of Saudi Arabian respondents, and 71% of Indian respondents feel that products and 78%76%71%70%65%65%64%63%62%60%57%57%55%53%53%50%49%48%42%40%38%38%37%37%35%33%32%31%0% 10% 20% 30% 40% 50% 60% 70% 80%FranceCanadaNetherlandsUnited StatesGermanyAustraliaGreat BritainBelgiumSwedenJapanPolandHungaryItalyRussiaSpainArgentinaSouth AfricaBrazilTurkeySouth KoreaChileColombiaMalaysiaMexicoPeruIndiaSaudi ArabiaChina% of Respondents That \u201cAgree\u201d\u2018Products and services using AI have more benets than drawbacks,\u2019 by Country (% of Total), 2022Source: IPSOS, 2022 | Chart: 2023 AI Index ReportFigure 8.1.2services using AI have more benefits than drawbacks (Figure 8.1.2). However, only 35% of American respondents share that sentiment. Among the 28 surveyed countries, France and Canada held the most negative views.Table of Contents Chapter 8 Preview 325Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionFigure 8.1.3 breaks down answers to all of IPSOS\u2019 AI products and services questions by country. Generally, sentiment relating to AI products and services seems to be strongly correlated within specific countries. For example, Chinese respondents seem to feel among the most positive about AI products and services: 87% of Chinese respondents claim that AI products and services make their lives easier, 76% report trusting 64% 59% 60%69% 59% 76% 67% 71% 50% 50% 57% 67% 72% 42% 41% 61% 74% 65% 76% 66% 75% 73% 78% 72% 62% 60%68% 63%60% 50% 52% 61% 44% 67% 80% 65% 45% 44% 46% 55% 74% 53% 53% 71% 65% 53% 71% 56%60% 80% 72% 76% 56% 50% 73% 46%59% 46%49% 65% 44% 70% 87% 71% 39% 45% 45% 50% 72% 54% 52% 71% 73% 47% 74% 58% 64% 80% 67% 74% 59% 46% 71% 41%55% 37% 38% 57% 32% 63% 78% 64% 31% 37% 38% 49% 71% 50% 42% 65% 65% 33% 70% 48% 53% 76% 57% 62% 53% 40%60% 35%47% 38% 37% 58% 36% 59% 76% 62% 34% 37% 37% 38% 69% 45% 32% 61% 62% 41% 63% 52% 57% 69% 57% 60% 46% 37% 60% 39%55% 36%40%50% 34% 56% 76% 57% 34% 42% 35% 48% 68% 48% 39% 61% 60% 38% 60% 51% 52% 73% 56% 46% 50% 39% 63% 35%53% 37% 37% 51% 32% 58% 73% 58% 32% 31% 33% 38% 67% 41% 30% 65% 62% 40% 65% 45% 50% 72% 56% 62% 49% 30%60% 36%33% 51% 42% 35% 49% 36% 30% 39% 32% 37% 50% 31% 53% 26% 20% 48% 38% 36% 35% 30% 28% 51% 52% 32% 48% 37% 48% 52%ArgentinaAustraliaBelgiumBrazilCanadaChileChinaColombiaFranceGermanyGreat BritainHungaryIndiaItalyJapanMalaysiaMexicoNetherlandsPeruPolandRussiaSaudia ArabiaSouth AfricaSouth KoreaSpainSwedenTurkeyUnited StatesProducts and services using artificial intelligence make me nervousProducts and services using artificial intelligence have profoundly changed my daily life in the past 3\u20135 yearsI know which types of products and services use artificial intelligenceI trust companies that use artificial intelligence as much as I trust other companiesI have a good understanding of what artificial intelligence isProducts and services using artificial intelligence will profoundly change my daily life in the next 3\u20135 yearsProducts and services using artificial intelligence make my life easierProducts and services using artificial intelligence have more benefits than drawbacksOpinions About AI by Country (% Agreeing With Statement), 2022Source: IPSOS, 2022 | Chart: 2023 AI Index ReportFigure 8.1.3companies that use AI as much as other companies, and only 30% say that AI products and services using AI make them nervous. Conversely, American respondents are among the most negative when it comes to AI. Only 41% claim that AI products and services make their lives easier, 35% report trusting AI companies as much as other companies, and 52% report that AI products and services make them feel nervous.Table of Contents Chapter 8 Preview 326Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionFigure 8.1.4 breaks down opinions in all countries across demographic groups such as gender, age, household income, and employment status. IPSOS results suggest that men feel more positively about AI products and services than women\u2014for example, compared to women, men are more likely to report feeling that AI products and services make their lives easier. Age-specific opinions vary. For instance, while individuals under 35 are most likely to report 69% 60% 66% 65% 61% 57% 63% 71% 56% 64% 71% 73% 74% 67% 59%63% 57% 63% 61% 55% 56% 58% 67% 53% 58% 68% 70% 72% 64% 54%62% 58% 64% 62% 54% 56% 58% 66% 53% 58% 67% 67% 70% 63% 55%55% 49% 47% 53% 46% 50% 51% 57% 45% 50% 59% 63% 64% 55% 47%55% 46% 54% 51% 45% 46% 50% 57% 44% 48% 58% 63% 65% 54% 44%53% 47% 54% 51% 44% 47% 48% 57% 45% 48% 56% 61% 62% 53% 45%51% 46% 54% 50% 41% 46% 47% 54% 43% 46% 55% 61% 62% 52% 43%38% 41% 40% 40% 38% 41% 41% 38% 41% 37% 40% 48% 46% 40% 38%MaleFemaleUnder 3535 to 4950 to 74LowMediumHighLowMediumHighBusiness OwnerSr. Exec./Decision MakerEmployedNon-EmployedGender Age Household Income Education Employment StatusProducts and services using artificial intelligence make me nervousProducts and services using artificial intelligence have profoundly changed my daily life in the past 3\u20135 yearsI trust companies that use artificial intelligence as much as I trust other companiesI have a good understanding of what artificial intelligence isProducts and services using artificial intelligence will profoundly change my daily life in the next 3\u20135 yearsProducts and services using artificial intelligence make my life easierProducts and services using artificial intelligence have more benefits than drawbacksI know which types of products and services use artificial intelligenceOpinions About AI by Demographic Group (% Agreeing With Statement), 2022Source: IPSOS, 2022 | Chart: 2023 AI Index ReportFigure 8.1.4feeling that AI products and services make their lives easier, they are also less likely than the 35-to-49 age category to believe that AI products and services have more benefits than drawbacks. Finally, households with higher incomes are more positive, compared to those with lower incomes, about AI products and services making life easier and having more benefits than drawbacks.Table of Contents Chapter 8 Preview 327Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionAI: Harm or Help?In 2021, Lloyd\u2019s Register Foundation, an independent global charity, collaborated with Gallup to poll 125,911 people across 121 countries about their perceptions of artificial intelligence and other digital trends. Figure 8.1.5 shows the responses to the survey question, \u201cDo you think artificial intelligence will mostly help or mostly harm people in this country in the next 20 years?\u201d39%28%2%22%8%35%29%2%24%9%42%27%2%20%8%Mostly help Mostly harm Neither Don\u2019t have an opinion Don\u2019t know/refused0%10%20%30%40%% World % Women % Men% of RespondentsViews on Whether AI Will \u2018Mostly Help\u2019 or \u2018Mostly Harm\u2019 People in the Next 20 Years Overall and byGender (% of Total), 2021Source: Lloyd\u2019s Register Foundation and Gallup, 2022 | Chart: 2023 AI Index ReportFigure 8.1.5A greater proportion of respondents believed that AI will mostly help (39%) compared to a smaller proportion who believed that it would mostly harm (28%). Mirroring the disparity in responses across gender evident in the IPSOS survey, men in the Lloyd\u2019s-Gallup poll were more likely than women to report believing that AI will mostly help people in the next 20 years.Table of Contents Chapter 8 Preview 328Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionEastern Asia, Northern/Western Europe, and Southern Europe are the regions of the world where people are most likely to report believing that AI will mostly help versus mostly harm (Figure 8.1.6). More specifically, among the Eastern Asian survey sample, The Lloyd\u2019s Register survey also polled respondents about their perceptions of certain AI technologies, such as self-driving cars. The majority of survey respondents reported not feeling safe in a self-driving car (65%), compared to only 27% who reported feeling safe (Figure 8.1.7).4.401.801.701.401.301.201.201.001.000.900.900.800.700.600.400 1 2 3 4Eastern AfricaSouthern AfricaNorthern AfricaCentral/Western AfricaLatin America and CaribbeanSouthern AsiaMiddle EastNorthern AmericaSoutheastern AsiaCentral AsiaSouthern EuropeEastern EuropeAustralia and New ZealandNorthern/Western EuropeEastern AsiaRatio of \u201cMostly Help\u201d/\u201cMostly Harm\u201dViews on Whether AI Will \u2018Mostly Help\u2019 or \u2018Mostly Harm\u2019 People in the Next 20 Years by Region:Source: Lloyd\u2019s Register Foundation and Gallup, 2022 | Chart: 2023 AI Index ReportRatio of \u2018Mostly Help\u2019/\u2018Mostly Harm\u2019, 202165%, Would not feel safe27%, Would feel safe8%, Don\u2019t know/refusedPerceptions of the Safety of Self-Driving Cars(% of Total), 2021Source: Lloyd\u2019s Register Foundation and Gallup, 2022 | Chart: 2023 AI Index ReportFigure 8.1.6Figure 8.1.7for every 1 response of \u201cmostly harm\u201d there were 4.4 responses suggesting that AI will \u201cmostly help.\u201d The regions whose populations are most pessimistic about the potential benefits of AI include Eastern Africa, Northern Africa, and Southern Africa.Table of Contents Chapter 8 Preview 329Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionUnited StatesIn 2022, Pew Research released one of the most comprehensive surveys to date about Americans\u2019 views on AI. The survey interviewed 10,260 panelists from a wide range of demographic groups about their broad AI-related opinions, as well as their perspectives on specific AI use cases.245% of Americans report feeling equally concerned and excited about the use of AI programs in daily life, while 37% report feeling more concerned than excited (Figure 8.1.8). Only 18% of Americans report feeling more excited than concerned about AI technology.Which AI applications are Americans most excited about? A large proportion report feeling very or somewhat excited about AI being used to perform household chores (57%), to perform repetitive workplace tasks (46%), and to diagnose medical problems (40%) (Figure 8.1.9). Americans are very or somewhat concerned about AI being used to make important life decisions for people (74%) and to know people\u2019s thoughts and behaviors (75%).45%, Equally concernedand excited37%, More concernedthan excited 18%, More excitedthan concerned<1%, No answerAmericans\u2019 Feelings Toward Increased Use of AIPrograms in Daily Life (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.8 57% 46% 40% 27% 9% 9% 24% 27% 24% 26% 16% 16% 19% 26% 35% 47% 74% 75%0% 20% 40% 60% 80% 100%Know people\u2019s thoughtsand behaviorsMake important life decisionsfor peopleHandle customer service callsDiagnose medical problemPerform repetitive workplacetasksPerform household choresVery/somewhat excited Equally excited and concerned Very/somewhat concerned% of RespondentsAmericans\u2019 Feelings on Potential AI Applications (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.932 See Appendix for more details about the survey methodology.3 The numbers in Figure 8.1.9 may not sum up to 100% due to rounding.Table of Contents Chapter 8 Preview 330Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionThere are two specific AI use cases that Americans are more likely to report feeling are good ideas for society rather than bad: police use of facial recognition technology, and social media companies using AI to find false information on their sites (Figure 8.1.10). More specifically, 46% of Americans believe that police using facial recognition technology is a good idea for society compared to 27% who believe it is a bad idea. However, Americans are not as excited about driverless passenger vehicles: More feel that driverless passenger vehicles are a bad idea for society than a good idea.Figure 8.1.8 27% 31% 44% 46% 38% 26% 27% 30% 29%0% 20% 40% 60% 80% 100%Driverless passenger vehiclesComputer programs by socialmedia companies to nd falseinformation on their sitesFacial recognition technologyby policeBad idea for society Good idea for society Not sure% of RespondentsAmericans\u2019 Perceptions of Specific AI Use Cases (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.1044 The numbers in Figure 8.1.10 may not sum up to 100% due to rounding.Table of Contents Chapter 8 Preview 331Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionOf the sample of Americans who reported being more concerned than excited about AI, Figure 8.1.11 outlines the main reasons for their concern. The primary reasons include loss of human jobs (19%); surveillance, hacking, and digital privacy (16%); and lack of human connection (12%). Americans reported being less concerned about the potential loss of freedom and issues relating to lack of oversight and regulation.Figure 8.1.819%16%12%8%8%7%6%3%3%2%2%2%2%7%0% 5% 10% 15% 20%Unforeseen consequences/effectsLoss of freedomHuman bias coded into AI Lack of oversight and regulation OtherDon\u2019t trust AI or people wielding itConcerns about government/tech companies using AIAI fails, makes mistakesPeople becoming too reliant on AI/techPeople misusing AIAI will get too powerful/outsmarting peopleLack of human connection/qualitiesSurveillance, hacking, digital privacyLoss of human jobs% of RespondentsMain Reason Americans Are Concerned About AI (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.11Table of Contents Chapter 8 Preview 332Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionThe two leading reasons that Americans report being excited about AI relate to its potential to make life better and to save time (Figure 8.1.12). Of the respondents, 31% believe AI makes life and society better. A significant group also reported feeling excited about the potential of AI to save time and increase efficiency (13%), as well as to handle mundane, tedious tasks (7%).Figure 8.1.831%13%10%7%6%6%6%4%4%2%2%7%0% 5% 10% 15% 20% 25% 30%OtherOther people\u2019s fears based on sci-\ufffd, not realityPersonal anecdotesHelps those who are elderly/have a disabilityHelps humans with di\ufffdcult/dangerous tasksMore accurate than humansAI is interesting, excitingHelps with work/laborHandles mundane, tedious tasksInevitable progress, is the futureSaves time, more e\ufffdcientMakes life, society better% of RespondentsMain Reason Americans Are Excited About AI (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.12Table of Contents Chapter 8 Preview 333Artificial IntelligenceIndex Report 2023 8.1 Survey DataChapter 8: Public OpinionThe Pew Research survey also asked participants which group of people had their experiences and views taken into consideration in the design of AI systems. Respondents felt AI systems most reflected the experiences and views of men and white adults (Figure 8.1.13). There was a 15 percentage point gap in the degree to which people felt that AI systems positively considered the experiences and views of men over women. Similarly, respondents felt that the experiences and views of Asian, Black, and Hispanic adults, compared to those held by white adults, were not as positively considered.Figure 8.1.8 12% 25% 13% 23% 33% 33% 51% 36% 48% 33% 24% 23% 37% 38% 39% 43% 42% 43%0% 20% 40% 60% 80% 100%Hispanic adultsBlack adultsAsian adultsWhite adultsWomenMenNet not well Net well Not sure% of RespondentsPeople Whose Experiences and Views Are Considered in the Design of AI Systems (% of Total), 2022Source: Pew Research, 2022 | Chart: 2023 AI Index ReportFigure 8.1.1355 The numbers in Figure 8.1.13 may not sum up to 100% due to rounding.Table of Contents Chapter 8 Preview 334Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI?From May to June 2022, a group of American researchers conducted a survey of the NLP research community on a diverse set of issues, including the state of the NLP field, artificial general intelligence (AGI), and ethics, among others. According to the authors, a total of 480 individuals completed the survey, 68% of whom had authored at least two Association for Computational Linguistics (ACL) publications between 2019 and 2022.6 The survey represents one of the most complete pictures of the attitudes AI researchers have toward AI research.In general, the NLP research community strongly feels that private firms have too much influence (77%) and that industry will produce the most widely cited research (86%) (Figure 8.1.14). Curiously, 67% either agreed or weakly agreed with the statement that most of NLP is dubious science. A small proportion, 30%, think an \u201cNLP winter\u201d\u2014a period when the field faces a significant slowdown or stagnation in research and development\u2014is coming in the next decade.Narrative Highlight: 77%86%30%62%67%63%0% 10% 20% 30% 40% 50% 60% 70% 80% 90%Author anonymityis worth itMost of NLP isdubious scienceNLP winteris coming (30 years)NLP winteris coming (10 years)Industry will producethe most widely citedresearchPrivate \ufffdrms have toomuch in\ufffduence% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dState of the Field According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.148.1 Survey DataChapter 8: Public Opinion6 More detailed information about the survey methodology and sample group can be found in the following paper.Table of Contents Chapter 8 Preview 335Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI? (cont\u2019d)A small majority of NLP researchers believe that specific types of AI systems can actually understand language: 51% agreed with the statement that language models (LMs) understand language, with even more (67%) agreeing that multimodal models understand language (Figure 8.1.15).Narrative Highlight: 51%67%36%0% 10% 20% 30% 40% 50% 60%Text-only evaluation canmeasure languageunderstandingMultimodal modelsunderstand languageLMs understandlanguage% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dLanguage Understanding According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.158.1 Survey DataChapter 8: Public OpinionTable of Contents Chapter 8 Preview 336Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI? (cont\u2019d)NLP researchers also seem to believe that NLP\u2019s past net impact has been positive (89%) and that its future impact will continue to be good (87%) (Figure 8.1.16). The community is divided on the issue of using AI to predict psychological characteristics, with 48% of respondents feeling it is unethical. Sixty percent of researchers feel that the carbon footprint of AI is a major concern; however, only 41% feel that NLP should be regulated.Narrative Highlight: 89%87%59%74%25%48%60%41%0% 10% 20% 30% 40% 50% 60% 70% 80% 90%NLP should be regulatedCarbon footprint is a major concernEthical and scienti\ufffdc considerationscan con\ufffdictEthical concerns mostly reduce to data quality and model accuracyIt is unethical to predict psychological characteristicsIt is unethical to build easily misusable systemsNLP\u2019s future net impact is goodNLP\u2019s past net impact is good% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dEthics According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.168.1 Survey DataChapter 8: Public OpinionTable of Contents Chapter 8 Preview 337Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI? (cont\u2019d)Although a large majority of researchers feel that AI could soon lead to revolutionary societal change (73%), only 36% feel that AI decisions could cause nuclear-level catastrophe (Figure 8.1.17). A plurality of researchers, 57%, held that recent research progress was leading the AI community toward Artificial General Intelligence (AGI).Narrative Highlight: 58%57%73%36%0% 10% 20% 30% 40% 50% 60% 70%AI decisions couldcause nuclear-levelcatastropheAI could soon leadto revolutionarysocietal changeRecent progress ismoving us towardAGIAGI is an importantconcern% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dArticial General Intelligence (AGI) and Major Risks According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.178.1 Survey DataChapter 8: Public OpinionTable of Contents Chapter 8 Preview 338Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI? (cont\u2019d)When asked about the direction AI research is taking, the NLP community registered the strongest responses about the following: First, there\u2019s too much focus on benchmarks (88%); second, more work should be done to incorporate interdisciplinary insights (82%); and third, there\u2019s too great a focus on scale (72%) (Figure 8.1.18).Narrative Highlight: 72%88%37%41%50%42%82%0% 10% 20% 30% 40% 50% 60% 70% 80% 90%We should do moreto incorporateinterdisciplinary insightsOn the wrong track:black-boxinterpretabilityOn the wrong track:explainable modelsOn the wrong track:language generationOn the wrong track:model architecturesThere\u2019s too muchfocus on benchmarksThere\u2019s too muchfocus on scale% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dPromising Research Programs According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.188.1 Survey DataChapter 8: Public OpinionTable of Contents Chapter 8 Preview 339Artificial IntelligenceIndex Report 2023How Does the Natural Language Processing (NLP)  Research Community Feel About AI? (cont\u2019d)A further point on the NLP community\u2019s skepticism of scale: Only 17% of respondents agreed or weakly agreed with the statement that scaling solves practically any important problem, with a further 50% reaffirming the importance of linguistic structure (Figure 8.1.19).Narrative Highlight: 17%50%51%61%0% 10% 20% 30% 40% 50% 60%Linguistics/CogSci willcontribute to themost-cited modelsExpert inductivebiases are necessaryLinguistic structureis necessaryScaling solves practicallyany important problem% of Respondents That \u201cAgree\u201d or \u201cWeakly Agree\u201dScale, Inductive Bias, and Adjacent Fields According to the NLP Community, 2022Source: Michael et al., 2022 | Chart: 2023 AI Index ReportFigure 8.1.198.1 Survey DataChapter 8: Public OpinionTable of Contents Chapter 8 Preview 340Artificial IntelligenceIndex Report 2023 8.2 Social Media DataChapter 8: Public OpinionDominant ModelsPublic attitudes toward AI can also be gauged through quantitative and qualitative analyses of posts that people make on social media. The NetBase Quid team leveraged the NetBase platform to analyze social conversation around AI models and new releases for uses across sectors from January to December 2022, looking at 2.74 million social media posts.Figure 8.2.1 shows the net sentiment score of various AI models that were released throughout the year. The net sentiment score expresses the ratio of positive to negative sentiment around a given topic. In this case, a net sentiment score of +100 means that all conversation is positive; a score of -100 means that all conversation is negative. AlphaCode had the most consistently high sentiment over time, as well as the highest average sentiment for 2022, due to positive press coverage on social media and practical use cases of AI-driven programming. Consumers and media outlets embraced the practical use case of programming automation. Some sample social media posts relating to AlphaCode include:  \u201c#AlphaCode\u2014a new #AI system for developing computer code developed by @DeepMind\u2014 can achieve average human-level performance  in solving programming contests.\u201d  \u2013 Science Magazine, Twitter   \u201cDeepMind\u2019s AlphaCode outperforms many human programmers in tricky software challenges.\u201d \u2013 @lunamothChatGPT conversation has increasingly saturated social media conversation around AI model releases more broadly, with sentiment growing ever more mixed. Consumers question the implications of its launch as well as its underlying ethical principles. Another frequent preoccupation is the bias of the system toward certain political, ethical, or cultural beliefs.  \u201cChatGPT passed a Wharton MBA exam. Time to overhaul education.\u201d \u2013 @GRDecter   \u201cAlarm: ChatGPT by @OpenAI now *expressly prohibits arguments for fossil fuels*. (It used to offer them.) Not only that, it excludes nuclear energy from its counter-suggestions. @sama,  what is the reason for this policy?\u201d \u2013 @AlexEpstein Finally, while GLM-130B took up very little volume of the overall social media conversation, a small conversation of very negative sentiment grew over the system\u2019s ties to the Chinese government and how it was \u201cprohibited\u201d from using the software to \u201cundermine\u201d China\u2019s government in any way. Technology influencer and PhD student Jesse Wood posted a Twitter thread about GLM-130B\u2019s licensing language that gained significant traction.  \u201cThe model license for GLM-130B has a restriction: \u2018You will not use the Software for any act that may undermine China\u2019s national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\u2019\u201d \u2013 @jrhwood 8.2 Social Media DataTable of Contents Chapter 8 Preview 341Artificial IntelligenceIndex Report 2023 8.2 Social Media DataChapter 8: Public Opinion0 42 29 2173 -9 -11 4460 79 71 7029 22 15 3466 66 3047 84 6524 65 5635 5285 694 99655014322022/Q1 2022/Q2 2022/Q3 2022/Q4ChatGPTCICEROBLOOMGLM-130BAlphaTensorMake-A-VideoWhisperStable DiusionImagenGatoPaLMCoPilotAlphaCodeLaMDADALL-ENet Sentiment Score of AI Models by Quarter, 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportFigure 8.2.177 The AI Index searched for sentiment surrounding the term \u201cDALL-E,\u201d as it was more frequently referred to on social media, rather than DALL-E 2, the official name of the text-to-image model released by OpenAI in 2022.Table of Contents Chapter 8 Preview 342Artificial IntelligenceIndex Report 2023 8.2 Social Media DataChapter 8: Public OpinionFigure 8.2.2 highlights the proportion of AI-related social media conversation that was dominated by the release of particular models.8 ChatGPT dominated consumer conversation with a rapid rise, making up over half of consumer conversation by the end of 2022. Despite initial excitement, sentiment was mixed by the end of the year, as some individuals became more aware of ChatGPT\u2019s limitations. OpenAI CEO Sam Altman even publicly commented on it being \u201cincredibly limited\u201d in certain respects.  \u201cChatGPT is incredibly limited, but good enough at some things to create a misleading impression of greatness. It\u2019s a mistake to be relying on it for anything important right now. It\u2019s a preview of progress; we have lots of work to do on robustness and truthfulness.\u201d \u2013 @SamAltmanConversation around LaMDA exploded in Q2 2022 as an ex\u2013Google employee reported his experiences with a \u201csentient\u201d system that spoke of its own emotions and thoughts. Many political and technology influencers spoke out, however, about the \u201cdeepfake\u201d nature of the responses of systems like LaMDA that do not have a sense of \u201ctruth\u201d and could proliferate misinformation.  \u201cAI systems like LamDA and GPT-3 are sociopathic liars with utter indifference to truth, deepfakers with words, every day creating more compelling, more plausible misinformation on demand. It is imperative that we develop technology & policy to thwart them.\u201d \u2013  @GaryMarcus  \u201cThis story \u2026 is really sad, and I think an important window into the risks of designing systems to seem like humans, which are exacerbated by #AIhype.\u201d \u2013 @nitashatakuStable Diffusion conversation stands out as a prominent leader in conversation volume toward the end of 2022, but it is also a symbol of how the consumer lexicon around AI models is developing. Many consumers debated the \u201coriginality\u201d of what Stable Diffusion produces.  \u201cI\u2019ve worked on neural networks, so I understand stable diffusion pretty well. And while it can\u2019t have original thoughts, it can come up with original works.\u201d \u2013 r/TikTokCringe  \u201cThat\u2019s true of anywhere that datasets scrape without permission. The thing to actually be upset about is that their own generator is purposefully using the Stable Diffusion dataset that already contains tons of stolen work.\u201d \u2013 @Emily_Art8 The figures in this section consider all AI-related social media conversation. The percentage associated with the model in Figure 8.2.2 represents the share of all AI-related social media conversation that was dominated by that model.ChatGPT dominated consumer conversation with a rapid rise, making up over half of consumer conversation by the end of 2022.Table of Contents Chapter 8 Preview 343Artificial IntelligenceIndex Report 2023 8.2 Social Media DataChapter 8: Public Opinion0% 1% 3% 2%1% 35% 9% <1%2% <1% <1% 1%10% 3% 4% 1%<1% <1% <1%10% 18% 3%5% 4% 2%19% 19%<1% <1%33% 15%1%<1%<1%3%52%2022/Q1 2022/Q2 2022/Q3 2022/Q4ChatGPTCICEROBLOOMGLM-130BAlphaTensorMake-A-VideoWhisperStable DiusionImagenGatoPaLMCoPilotAlphaCodeLaMDADALL-ESelect Models\u2019 Share of AI Social Media Attention by Quarter, 2022Source: NetBase Quid, 2022 | Chart: 2023 AI Index ReportFigure 8.2.2Artificial IntelligenceIndex Report 2023Artificial IntelligenceIndex Report 2023AppendixArtificial IntelligenceIndex Report 2023Appendix345Chapter 1 Research and Development 346Chapter 2 Technical Performance 352Chapter 3 Technical AI Ethics 363Chapter 4 The Economy 366Chapter 5 Education 375Chapter 6 Policy and Governance 377Chapter 7 Diversity 384Chapter 8 Public Opinion 385Table of Contents 346Artificial IntelligenceIndex Report 2023AppendixCenter for Security and Emerging Technology, Georgetown UniversityPrepared by Sara Abdulla and James DunhamThe Center for Security and Emerging Technology (CSET) is a policy research organization within Georgetown University\u2019s Walsh School of Foreign Service that produces data-driven research at the intersection of security and technology, providing nonpartisan analysis to the policy community.For more information about how CSET analyzes bibliometric and patent data, see the Country Activity Tracker (CAT) documentation on the Emerging Technology Observatory\u2019s website.1 Using CAT, users can also interact with country bibliometric, patent, and investment data.2Publications from CSET Merged Corpus of Scholarly LiteratureSourceCSET\u2019s merged corpus of scholarly literature combines distinct publications from Digital Science\u2019s Dimensions, Clarivate\u2019s Web of Science, Microsoft Academic Graph, China National Knowledge Infrastructure, arXiv, and Papers With Code.3MethodologyTo create the merged corpus, CSET deduplicated across the listed sources using publication metadata, and then combined the metadata for linked publications. To identify AI publications, CSET used an English-language subset of this corpus: publications since 2010 that appear AI-relevant.4 CSET researchers developed a classifier for identifying AI-related publications by leveraging the arXiv repository, where authors and editors tag papers by subject. Additionally, CSET uses select Chinese AI keywords to identify Chinese-language AI papers.5To provide a publication\u2019s field of study, CSET matches each publication in the analytic corpus with predictions from Microsoft Academic Graph\u2019s field-of-study model, which yields hierarchical labels describing the published research field(s) of study and corresponding scores.6 CSET researchers identified the most common fields of study in our corpus of AI-relevant publications since 2010 and recorded publications in all other fields as \u201cOther AI.\u201d English-language AI-relevant publications were then tallied by their top-scoring field and publication year.CSET also provided year-by-year citations for AI-relevant work associated with each country. A publication is associated with a country if it has at Chapter 1: Research and DevelopmentChapter 1: Research and DevelopmentAppendix1 https://eto.tech/tool-docs/cat/2 https://cat.eto.tech/3 All CNKI content is furnished by East View Information Services, Minneapolis, Minnesota, USA.4 For more information, see James Dunham, Jennifer Melot, and Dewey Murdick, \u201cIdentifying the Development and Application of Artificial Intelligence in Scientific Text,\u201d arXiv [cs.DL], May 28, 2020, https://arxiv.org/abs/2002.07143.5 This method was not used in CSET\u2019s data analysis for the 2022 HAI Index report.6 These scores are based on cosine similarities between field-of-study and paper embeddings. See Zhihong Shen, Hao Ma, and Kuansan Wang, \u201cA Web-Scale System for Scientific Knowledge Exploration,\u201d arXiv [cs.CL], May 30, 2018, https://arxiv.org/abs/1805.12216.Table of Contents 347Artificial IntelligenceIndex Report 2023Appendixleast one author whose organizational affiliation(s) are located in that country. Citation counts aren\u2019t available for all publications; those without counts weren\u2019t included in the citation analysis. Over 70% of English-language AI papers published between 2010 and 2020 have citation data available.CSET counted cross-country collaborations as distinct pairs of countries across authors for each publication. Collaborations are only counted once: For example, if a publication has two authors from the United States and two authors from China, it is counted as a single United States-China collaboration.Additionally, publication counts by year and by publication type (e.g., academic journal articles, conference papers) were provided where available. These publication types were disaggregated by affiliation country as described above.CSET also provided publication affiliation sector(s) where, as in the country attribution analysis, sectors were associated with publications through authors\u2019 affiliations. Not all affiliations were characterized in terms of sectors; CSET researchers relied primarily on GRID from Digital Science for this purpose, and not all organizations can be found in or linked to GRID.7 Where the affiliation sector is available, papers were counted toward these sectors, by year. Cross-sector collaborations on academic publications were calculated using the same method as in the cross-country collaborations analysis. We use HAI\u2019s standard regions mapping for geographic analysis, and the same principles for double-counting apply for regions as they do for countries.Epoch National  Affiliation AnalysisThe AI forecasting research group Epoch maintains a dataset of landmark AI and ML models, along with accompanying information about their creators and publications, such as the list of their (co)authors, number of citations, type of AI task accomplished, and amount of compute used in training.The nationalities of the authors of these papers have important implications for geopolitical AI forecasting. As various research institutions and technology companies start producing advanced ML models, the global distribution of future AI development may shift or concentrate in certain places, which in turn affects the geopolitical landscape because AI is expected to become a crucial component of economic and military power in the near future.To track the distribution of AI research contributions on landmark publications by country, the Epoch dataset is coded according to the following methodology: 1.  A snapshot of the dataset was taken on November 14, 2022. This includes papers about landmark models, selected using the inclusion criteria of importance, relevance, and uniqueness, as described in the Compute Trends dataset documentation.8 2.  The authors are attributed to countries based on their affiliation credited on the paper. For international organizations, authors are attributed to the country where the organization is headquartered, unless a more specific location is indicated. The number of authors from each country represented are added up and recorded. 7 See https://www.grid.ac/ for more information about the GRID dataset from Digital Science.8 https://epochai.org/blog/compute-trends; see note on \u201cmilestone systems.\u201dChapter 1: Research and DevelopmentAppendixTable of Contents 348Artificial IntelligenceIndex Report 2023AppendixIf an author has multiple affiliations in different countries, they are split between those countries proportionately.9 3.  Each paper in the dataset is normalized to equal value by dividing the counts on each paper from each country by the total number of authors on that paper.10 4.  All of the landmark publications are aggregated within time periods (e.g., monthly or yearly) with the normalized national contributions added up to determine what each country\u2019s contribution to landmark AI research was during each time period. 5.  The contributions of different countries are compared over time to identify any trends.Large Language and Multimodal ModelsThe following models were identified by members of the AI Index Steering Committee as the large language and multimodal models that would be included as part of the large language and multimodal model analysis:9 For example, an author employed by both a Chinese university and a Canadian technology firm would be counted as 0.5 researchers from China and 0.5 from Canada.10 This choice is arbitrary. Other plausible alternatives include weighting papers by their number of citations, or assigning greater weight to papers with more authors.11 Hardware utilization rates: Every paper that reported the hardware utilization efficiency during training provided values between 30% and 50%. The AI Index used the reported numbers when available, or used 40% when values were not provided.Chapter 1: Research and DevelopmentAppendixAlphaCodeBLOOMChinchillaCodexCogViewDALL-EDALL-E 2ERNIE 3.0ERNIE-GEN (large)GLM-130BGopherGPT-2GPT-3 175B (davinci)GPT-J-6BGPT-NeoGPT-NeoX-20BGrover-MegaHyperCLOVAImagenInstructGPTJurassic-1-JumboJurassic-XMeenaMegatron-LM (original, 8.3B)Megatron-Turing NLG 530BMinerva (540B)OPT-175BPaLM (540B)PanGu-alphaStable Diffusion (LDM-KL-8-G)T5-3BT5-11BTuring NLGWu Dao 2.0Wu Dao \u2013 Wen YuanLarge Language and Multimodal Models Training Cost AnalysisCost estimates for the models were based directly on the hardware and training time if these were disclosed by the authors; otherwise, the AI Index calculated training time from the hardware speed, training compute, and hardware utilization efficiency.11 Training time was then multiplied by the closest cost rate for the hardware the AI Index could find for the organization that trained the model. If price quotes were available before and after the model\u2019s training, the AI Index interpolated the hardware\u2019s cost rate along an exponential decay curve.The AI Index classified training cost estimates as high, middle, or low. The AI Index called an estimate high if it was an upper bound or if the true cost was more likely to be lower than higher: For example, PaLM was trained on TPU v4 chips, and the AI Index estimated the cost to train the model on these chips from Google\u2019s public cloud compute prices, but the Table of Contents 349Artificial IntelligenceIndex Report 2023AppendixAI Conferences The AI Index reached out to the organizers of various AI conferences in 2022 and asked them to provide information on total attendance. Some conferences posted their attendance totals online; when this was the case, the AI Index used those reported totals and did not reach out to the conference organizers.GitHubThe GitHub data was provided to the AI Index through OECD.AI, an organization with whom GitHub partners that provides data on open-source AI software. The AI Index reproduces the methodological note that is included by OECD.AI on its website, for the GitHub Data.BackgroundSince its creation in 2007, GitHub has become the main provider of internet hosting for software development and version control. Many technology organizations and software developers use GitHub as a primary place for collaboration. To enable collaboration, GitHub is structured into projects, or \u201crepositories,\u201d which contain a project\u2019s files and each file\u2019s revision history. The analysis of GitHub data could shed light on relevant metrics about who is developing AI software, where, and how fast, and who is using which development tools. These metrics could serve as proxies for broader trends in the field of software development and innovation.Identifying AI ProjectsArguably, a significant portion of AI software development takes place on GitHub. OECD.AI partners with GitHub to identify public AI projects\u2014or \u201crepositories\u201d\u2014following the methodology developed by Gonzalez et al.,2020. Using the 439 topic labels identified by Gonzalez et al.\u2014as well as the topics \u201cmachine learning,\u201d \u201cdeep learning,\u201d and \u201cartificial intelligence\u201d\u2014GitHub provides OECD.AI with a list of public projects containing AI code. GitHub updates the list of public AI projects on a quarterly basis, which allows OECD.AI to capture trends in AI software development over time.Obtaining AI Projects\u2019 MetadataOECD.AI uses GitHub\u2019s list of public AI projects to query GitHub\u2019s public API and obtain more information about these projects. Project metadata may include the individual or organization that created the project; the programming language(s) (e.g., Python) and development tool(s) (e.g., Jupyter Notebooks) used in the project; as well as information about the contributions\u2014or \u201ccommits\u201d\u2014made to it, which include the commit\u2019s author and a timestamp. In practical terms, a contribution or \u201ccommit\u201d is an individual change to a file or set of files. Additionally, GitHub automatically suggests topical tags to each project based on its content. These topical tags need to be confirmed or modified by the project owner(s) to appear in the metadata.Chapter 1: Research and DevelopmentAppendixinternal cost to Google is probably lower than what they charge others to rent their hardware. The AI Index called an estimate low if it was a lower bound or if the true cost was likely higher: For example, ERNIE was trained on NVIDIA Tesla v100 chips and published in July 2021; the chips cost $0.55 per hour in January 2023, so the AI Index could get a low estimate of the cost using this rate, but the training hardware was probably more expensive two years earlier. Middle estimates are a best guess, or those that equally well might be lower or higher.Table of Contents 350Artificial IntelligenceIndex Report 2023AppendixMapping Contributions to AI Projects to a CountryContributions to public AI projects are mapped to a country based on location information at the contributor level and at the project level.a) Location information at the contributor level: \u2022  GitHub\u2019s \u201cLocation\u201d field: Contributors can provide their location in their GitHub account. Given that GitHub\u2019s location field accepts free text, the location provided by contributors is not standardized and could belong to different levels (e.g., suburban, urban, regional, or national). To allow cross-country comparisons, Mapbox is used to standardize all available locations to the country level. \u2022  Top level domain: Where the location field is empty or the location is not recognized, a contributor\u2019s location is assigned based on his or her email domain (e.g., .fr, .us, etc.).b) Location information at the project level: \u2022  Project information: Where no location information is available at the contributor level, information at the repository or project level is exploited. In particular, contributions from contributors with no location information to projects created or owned by a known organization are automatically assigned the organization\u2019s country (i.e., the country where its headquarters are located). For example, contributions from a contributor with no location information to an AI project owned by Microsoft will be assigned to the United States.If the above fails, a contributor\u2019s location field is left blank.As of October 2021, 71.2% of the contributions to public AI projects were mapped to a country using this methodology. However, a decreasing trend in the share of AI projects for which a location can be identified is observed in time, indicating a possible lag in location reporting.Measuring Contributions to AI ProjectsCollaboration on a given public AI project is measured by the number of contributions\u2014or \u201ccommits\u201d\u2014made to it.To obtain a fractional count of contributions by country, an AI project is divided equally by the total number of contributions made to it. A country\u2019s total contributions to AI projects is therefore given by the sum of its contributions\u2014in fractional counts\u2014to each AI project. In relative terms, the share of contributions to public AI projects made by a given country is the ratio of that country\u2019s contributions to each of the AI projects in which it participates over the total contributions to AI projects from all countries.In future iterations, OECD.AI plans to include additional measures of contribution to AI software development, such as issues raised, comments, and pull requests.Identifying Programming Languages and Development Tools Used in AI ProjectsGitHub uses file extensions contained in a project to automatically tag it with one or more programming languages and/or development tools. This implies that more than one programming language or development tool could be used in a given AI project.Chapter 1: Research and DevelopmentAppendixTable of Contents 351Artificial IntelligenceIndex Report 2023AppendixMeasuring the Quality of AI ProjectsTwo quality measures are used to classify public AI projects: \u2022  Project impact: The impact of an AI project is given by the number of managed copies (i.e., \u201cforks\u201d) made of that project. \u2022  Project popularity: The impact of an AI project is given by the number of followers (i.e., \u201cstars\u201d) received by that project.Filtering by project impact or popularity could help identify countries that contribute the most to high quality projects.Measuring CollaborationTwo countries are said to collaborate on a specific public AI software development project if there is at least one contributor from each country with at least one contribution (i.e., \u201ccommit\u201d) to the project. Domestic collaboration occurs when two contributors from the same country contribute to a project.Chapter 1: Research and DevelopmentAppendixTable of Contents Appendix 352Artificial IntelligenceIndex Report 2023ImageNetData on ImageNet accuracy was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (top-1 accuracy) correspond to the result reported in the most recent version of each paper. Learn more about the LSVRC ImageNet competition and the ImageNet dataset.To highlight progress on top-1 accuracy without the use of extra training data, scores were taken from the following papers:Aggregated Residual Transformations for  Deep Neural NetworksExploring the Limits of Weakly Supervised PretrainingFixing the Train-Test Resolution Discrepancy: FixEfficientNetImageNet Classification With Deep Convolutional Neural NetworksPeCo: Perceptual Codebook for BERT  Pre-training of Vision TransformersProgressive Neural Architecture SearchRethinking the Inception Architecture for  Computer VisionSelf-Training With Noisy Student Improves  ImageNet ClassificationSome Improvements on Deep Convolutional Neural Network Based Image ClassificationVery Deep Convolutional Networks for Large-Scale Image RecognitionViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and BeyondTo highlight progress on top-1 accuracy with the use of extra training data, scores were taken from the following papers:Big Transfer (BiT): General Visual  Representation LearningCoAtNet: Marrying Convolution and  Attention for All Data SizesCoCa: Contrastive Captioners Are Image-Text Foundation ModelsMeta Pseudo LabelsNational Institute of Standards and Technology (NIST) Face Recognition Vendor Test (FRVT)Data on NIST FRVT 1:1 verification accuracy by dataset was obtained from the FRVT 1:1 verification leaderboard.Chapter 2: Technical PerformanceChapter 2: Technical PerformanceAppendixTable of Contents Appendix 353Artificial IntelligenceIndex Report 2023Celeb-DFData on Celeb-DF AUC was retrieved through a detailed arXiv literature review. The reported dates correspond to the year in which a paper was first published to arXiv or a method was introduced. With Celeb-DF, recent researchers have tested previously existing deepfake detection methodologies. The year in which a method was introduced, even if it was subsequently tested, is the year in which it is included in the report. The reported results (AUC) correspond to the result reported in the most recent version of each paper. Details on the Celeb-DF benchmark can be found in the Celeb-DF paper.To highlight progress on Celeb-DF, scores were taken from the following papers:Deepfake Detection via Joint Unsupervised Reconstruction and Supervised ClassificationExposing Deepfake Videos by Detecting  Face Warping ArtifactsFace X-Ray for More General Face Forgery Detection FaceForensics++: Learning to Detect Manipulated Facial ImagesSpatial-Phase Shallow Learning: Rethinking Face Forgery Detection in Frequency DomainMPIIData on MPII percentage of correct keypoints (PCK) was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (PCK) correspond to the result reported in the most recent version of each paper. Details on the MPII benchmark can be found in the MPII paper and MPII dataset.To highlight progress on percentage of correct keypoints without the use of extra training data, scores were taken from the following papers:Bottom-Up and Top-Down Reasoning  With Hierarchical Rectified GaussiansCascade Feature Aggregation for  Human Pose EstimationDeeply Learned Compositional Models for  Human Pose EstimationEfficient Object Localization Using  Convolutional NetworksLearning Feature Pyramids for Human Pose EstimationStacked Hourglass Networks for  Human Pose EstimationToward Fast and Accurate Human Pose Estimation  via Soft-Gated Skip ConnectionsViTPose: Simple Vision Transformer Baselines for Human Pose EstimationCityscapes Challenge,  Pixel-Level Semantic  Labeling TaskData on the Cityscapes challenge, pixel-level semantic labeling task mean intersection-over-union (mIoU) was taken from the Cityscapes dataset, specifically their pixel-level semantic labeling leaderboard. More details about the Cityscapes dataset and other corresponding semantic segmentation challenges can be accessed at the Cityscapes dataset webpage.Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 354Artificial IntelligenceIndex Report 2023Kvasir-SEGData on Kvasir-SEG mean dice was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (mean dice) correspond to the result reported in the most recent version of each paper. Details on the Kvasir-SEG benchmark can be found in the Kvasir-SEG paper.To highlight progress on Kvasir-SEG, scores were taken from the following papers:GMSRF-Net: An Improved Generalizability With Global Multi-Scale Residual Fusion Network for  Polyp SegmentationPraNet: Parallel Reverse Attention Network for  Polyp SegmentationResUNet++: An Advanced Architecture for  Medical Image SegmentationSpatially Exclusive Pasting: A General Data Augmentation for the Polyp SegmentationCommon Object in Context (COCO) Data on COCO mean average precision (mAP50) was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (mAP50) correspond to the result reported in the most recent version of each paper. Details on the COCO benchmark can be found in the COCO paper.To highlight progress on COCO, scores were taken from the following papers:An Analysis of Scale Invariance in Object  Detection-SNIPCBNet: A Novel Composite Backbone Network Architecture for Object DetectionDeformable ConvNets v2: More Deformable,  Better ResultsDetectoRS: Detecting Objects With Recursive  Feature Pyramid and Switchable Atrous ConvolutionEVA: Exploring the Limits of Masked Visual Representation Learning at ScaleGrounded Language-Image Pre-trainingInside-Outside Net: Detecting Objects in Context  With Skip Pooling and Recurrent Neural NetworksCIFAR-10Data on CIFAR-10 FID scores was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (FID score) correspond to the result reported in the most recent version of each paper. Details on the CIFAR-10 benchmark can be found in the CIFAR-10 paper.To highlight progress on CIFAR-10, scores were taken from the following papers:GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash EquilibriumLarge Scale GAN Training for High Fidelity Natural Image SynthesisRefining Generative Process With Discriminator Guidance in Score-Based Diffusion ModelsScore-Based Generative Modeling in Latent SpaceScore-Based Generative Modeling Through  Stochastic Differential EquationsSelf-Supervised GAN: Analysis and Improvement  With Multi-Class Minimax GameChapter 2: Technical PerformanceAppendixTable of Contents Appendix 355Artificial IntelligenceIndex Report 2023STL-10Data on STL-10 FID scores was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (FID score) correspond to the result reported in the most recent version of each paper. Details on the STL-10 benchmark can be found in the STL-10 paper.To highlight progress on STL-10, scores were taken from the following papers:DEGAS: Differentiable Efficient Generator SearchDiffusion-GAN: Training GANs With DiffusionDiscriminator Contrastive Divergence:  Semi-Amortized Generative Modeling by  Exploring Energy of the DiscriminatorDist-GAN: An Improved GAN Using  Distance ConstraintsSoft Truncation: A Universal Training Technique of Score-Based Diffusion Model for High Precision Score EstimationText-to-Image Models  on MS-COCO 256 \u00d7 256  FID-30KData on MS-COCO 256 x 256 FID 30K for Text-to-Image Models was retrieved from the paper Saharia et al., 2022.Visual Question Answering (VQA)Data on VQA accuracy was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (accuracy) correspond to the result reported in the most recent version of each paper. Human-level performance is taken from the 2021 VQA challenge.To highlight progress on VQA accuracy without the use of extra training data, scores were taken from the following papers:Bilinear Attention NetworksMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual GroundingOscar: Object-Semantics Aligned Pre-training  for Vision-Language TasksPaLI: A Jointly-Scaled Multilingual  Language-Image ModelTips and Tricks for Visual Question Answering: Learnings From the 2017 ChallengeUNITER: UNiversal Image-TExt Representation LearningVLMo: Unified Vision-Language Pre-training With Mixture-of-Modality-ExpertsBEiT-3 Vs. Previous SOTAData on BEiT-3 and Previous SOTA was retrieved from the paper Wang et al., 2022.Visual Commonsense Reasoning (VCR)Data on VCR Q->AR score was taken from VCR leaderboard; the VCR leaderboard webpage further delineates the methodology behind the VCR challenge. Human performance on VCR is taken from Zellers et al., 2018. Details on the VCR benchmark can be found in the VCR paper.Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 356Artificial IntelligenceIndex Report 2023Kinetics-400, Kinetics-600, and Kinetics-700Data on Kinetics-400, Kinetics-600, and Kinetics-700 accuracy was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code (Kinetics-400, Kinetics-600, and Kinetics-700). The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (top-1 accuracy) correspond to the result reported in the most recent version of each paper. Details on the Kinetics-400 benchmark can be found in the Kinetics-400 paper. Details on the Kinetics-600 benchmark can be found in the Kinetics-600 paper. Details on the Kinetics-700 benchmark can be found in the Kinetics-700 paper.To highlight progress on Kinetics-400, scores were taken from the following papers:Co-training Transformer With Videos and Images Improves Action RecognitionInternVideo: General Video Foundation Models via Generative and Discriminative LearningLarge-Scale Weakly-Supervised Pre-training for  Video Action RecognitionNon-Local Neural NetworksOmni-Sourced Webly-Supervised Learning for  Video RecognitionSlowFast Networks for Video RecognitionTemporal Segment Networks: Towards Good Practices for Deep Action RecognitionTo highlight progress on Kinetics-600, scores were taken from the following papers:Learning Spatio-Temporal Representation  With Local and Global DiffusionMasked Feature Prediction for Self-Supervised  Visual Pre-trainingPERF-Net: Pose Empowered RGB-Flow NetRethinking Spatiotemporal Feature Learning:  Speed-Accuracy Trade-Offs in Video ClassificationRethinking Video ViTs: Sparse Video Tubes for  Joint Image and Video LearningSlowFast Networks for Video RecognitionTo highlight progress on Kinetics-700, scores were taken from the following papers:InternVideo: General Video Foundation Models via Generative and Discriminative LearningLearn to Cycle: Time-Consistent Feature Discovery  for Action RecognitionMasked Feature Prediction for Self-Supervised  Visual Pre-trainingText-to-Video Models  on UCF-101Data on UCF-101 Inception Score (IS) for Text-to-Video Models was retrieved from the paper Hong et al., 2022, and Singer et al., 2022.Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 357Artificial IntelligenceIndex Report 2023SuperGLUE The SuperGLUE benchmark data was pulled from the SuperGLUE leaderboard. Details about the SuperGLUE benchmark are in the SuperGLUE paper and SuperGLUE software toolkit. The tasks and evaluation metrics for SuperGLUE are:Reading Comprehension Dataset Requiring  Logical Reasoning (ReClor)Data on ReClor accuracy was retrieved through the ReClor leaderboard cross-referenced by technical progress reported on Papers With Code. Details about the ReClor benchmark can be found in the ReClor paper.Chapter 2: Technical PerformanceAppendixNAME IDENTIFIER METRICBroadcoverage Diagnostics AX-b Matthew\u2019s CarrCommitmentBank CB Avg. F1/AccuracyChoice of Plausible Alternatives COPA AccuracyMulti-Sentence Reading Comprehension MultiRC F1a/EMRecognizing Textual Entailment RTE AccuracyWords in Context WiC AccuracyThe Winograd Schema Challenge WSC AccuracyBooIQ BooIQ AccuracyReading Comprehension With Commonsense Reasoning ReCoRD F1/AccuracyWinogender Schema Diagnostic AX-g Gender Parity/Accuracy Table of Contents Appendix 358Artificial IntelligenceIndex Report 2023Narrative Highlight:  Just How Much Better Have Language Models Become?Complete answer outputted by GPT-2 to the AI Index prompt request.Model prompt >>> Explain to me the major accomplishments of Theodore Roosevelt\u2019s presidency.(The most notable is the establishment of a national telephone and mail exchange company.)But the only things that will change with a president who knows how to operate a business is his understanding of the American system of government. The public knows all about our problems, yet at least half of the people who need it won\u2019t know anything about ours. We can no longer have this. Our system of government is the onlyone in which you really need a public relations firm to work hard.The system of governmental control, while working, is not always effective in ensuring that the very things that our economy needs, do not always be at the top of everybody\u2019s list. So I wanted to know how successful the system you employ for managing the public\u2019s needs is in helping the American economy grow, how efficient the service our economy provides is being made, and how it can better serve the needs of a growing number of Americans.I wanted to know.This was before I ever hired my own lawyer to work on the case. This was before I ever hired my own law firm to handle government investigations. For my first major job, at least, I had to convince two top aides to get paid $90,000 for the role, and before I even started on the case, I had to convince one top staffer to get paid even $100,000 to have an interview with my lead prosecutor.I wanted to know in advance: What percentage of his work was actually done as a private citizen, and how efficiently should he handle it for his clients? And what advice would he give me should I end up in the public eye?At no time, my job as the lead prosecutor on this case required me to put forth any of those two or five requests. Most people on the case, though, did. One person asked me what the case\u2019s real value was. That didn\u2019t stop me from telling it to one of my top managers. And my top manager never even mentioned it.In the end, it was a two-week, $40 million round-trip tour of the government\u2019s business, with the best lawyer possible. In exchange, we got a chance to hear his last public comment in this case, his last comment to me.There was one more point \u2014 the end point of the case.When it came up for consideration in October of 1993, I had the choice to give it three years, or six. And I chose six.Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 359Artificial IntelligenceIndex Report 2023Large Language Models on the Blocksworld DomainData on Blocksworld domain for large language models was retrieved from the paper Valmeekam et al., 2022.arXiv Data on arXiv recall-oriented understudy for gisting evaluation (ROUGE-1) was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (ROUGE-1) correspond to the result reported in the most recent version of each paper. Details about the arXiv benchmark are in the arXiv dataset webpage.To highlight progress on arXiv, scores were taken from the following papers:Big Bird: Transformers for Longer SequencesA Discourse-Aware Attention Model for  Abstractive Summarization of Long DocumentsGet to the Point: Summarization With  Pointer-Generator NetworksLong Document Summarization With Top-Down  and Bottom-Up InferenceMemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov  Decision ProcessesPEGASUS: Pre-training With Extracted Gap-Sentences for Abstractive SummarizationPubMedData on PubMed recall-oriented understudy for gisting evaluation (ROUGE-1) was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (ROUGE-1) correspond to the result reported in the most recent version of each paper. Details about the PubMed benchmark are in the PubMed paper.To highlight progress in PubMed, scores were taken from the following papers:A Discourse-Aware Attention Model for Abstractive Summarization of Long DocumentsGet to the Point: Summarization With Pointer-Generator NetworksLong Document Summarization With Top-Down  and Bottom-Up InferenceLongT5: Efficient Text-to-Text Transformer for  Long SequencesPEGASUS: Pre-training With Extracted Gap-Sentences for Abstractive SummarizationSparsifying Transformer Models With Trainable Representation PoolingAbductive Natural Language Inference (aNLI) Data on Abductive Natural Language Inference (aNLI) was sourced from the Allen Institute for AI\u2019s aNLI leaderboard. Details on the aNLI benchmark can be found in the aNLI paper.Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 360Artificial IntelligenceIndex Report 2023SST-5 Fine-GrainedData on SST-5 Fine-Grained accuracy was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (accuracy) correspond to the result reported in the most recent version of each paper. Details about the SST-5 Fine-Grained benchmark can be found in the SST paper.To highlight progress on SST-5 Fine-Grained accuracy, scores were taken from the following papers:An Algorithm for Routing Capsules in All DomainsAn Algorithm for Routing Vectors in SequencesImproved Semantic Representations from Tree-Structured Long Short-Term Memory NetworksImproved Sentence Modeling Using Suffix  Bidirectional LSTMLearned in Translation: Contextualized Word VectorsLess Grammar, More FeaturesRecursive Deep Models for Semantic Compositionality Over a Sentiment TreebankSelf-Explaining Structures Improve NLP ModelsMMLUData on MMLU accuracy was retrieved through a detailed arXiv literature review cross-referenced by technical progress reported on Papers With Code. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (accuracy) correspond to the result reported in the most recent version of each paper. Details about the MMLU benchmark can be found in the MMLU paper.To highlight progress on MMLU accuracy, scores were taken from the following papers:Language Models Are Few-Shot LearnersLanguage Models Are Unsupervised  Multitask LearnersScaling Instruction-Finetuned Language ModelsScaling Language Models: Methods, Analysis & Insights from Training GopherNumber of Commercially Available MT SystemsDetails about the number of commercially available MT systems were sourced from the Intento report The State of Machine Translation, 2022. Intento is a San Francisco\u2013based startup that analyzes commercially available MT services.VoxCelebData on VoxCeleb equal error rate (EER) was retrieved from the VoxCeleb Speaker Recognition Challenge (VoxSRC).For the sake of consistency, the AI Index reported scores on the initial VoxCeleb dataset. Specifically, the AI Index made use of the following sources of information:ID R&D System Description to VoxCeleb Speaker Recognition Challenge 2022The IDLAB VoXSRC-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker VerificationThe SpeakIn System for VoxCeleb Speaker  Recognition Challenge 2021VoxCeleb: A Large-Scale Speaker Identification DatasetVoxCeleb: Large-Scale Speaker Verification in the WildVoxCeleb2: Deep Speaker RecognitionChapter 2: Technical PerformanceAppendixTable of Contents Appendix 361Artificial IntelligenceIndex Report 2023WhisperData on Whisper for large-scale speech recognition models was retrieved from the paper Radford et al., 2022.Procgen Data on Procgen mean-normalized score was retrieved through a detailed arXiv literature review. The reported dates correspond to the year in which a paper was first published to arXiv, and the reported results (mean-normalized score) correspond to the result reported in the most recent version of each paper. Details on the Procgen benchmark can be found in the Procgen paper.To highlight progress on Procgen, scores were taken from the following papers:Automatic Data Augmentation for Generalization in Reinforcement LearningLeveraging Procedural Generation to Benchmark Reinforcement LearningProcedural Generalization by Planning With  Self-Supervised World ModelsRethinking Value Function Learning for  Generalization in Reinforcement LearningTraining Time, Number of Accelerators, and PerformanceData on training time, number of accelerators, and performance for AI systems was taken from the MLPerf Training and Inference benchmark competitions. Details on the MLPerf Training benchmark can be found in the MLPerf Training Benchmark paper, while details on MLPerf Inference can be found in the MLPerf Inference Benchmark paper. Information about the current benchmark categories as well as technical information about submission and competition subdivisions can be found on the MLPerf Training and MLPerf Inference webpages.The AI Index made use of data from the following MLPerf Training competitions:MLPerf Training v2.1, 2022MLPerf Training v2.0, 2022MLPerf Training v1.1, 2021MLPerf Training v1.0, 2021MLPerf Training v0.7, 2020MLPerf Training v0.6, 2019MLPerf Training v0.5, 2018The AI Index made use of data from the following MLPerf Inference competitions:MLPerf Inference v2.1, 2022MLPerf Inference v2.0, 2022MLPerf Inference v1.1, 2021MLPerf Inference v1.0, 2021MLPerf Inference v0.7, 2020Chapter 2: Technical PerformanceAppendixTable of Contents Appendix 362Artificial IntelligenceIndex Report 2023GPUs\u2019 Performance and PriceThe AI Index collected data on GPUs\u2019 performance and price, building on and extending the dataset collected from Epoch AI\u2019s Trends in GPU Price-Performance blog post.The AI Index compiled a list of GPUs starting from the Median Group (2018), Sun et al. (2019), and Epoch (2022) datasets. To update and extend previous analysis, the AI Index included new GPU releases for the period 2021\u20132023, gathering information from sources such as TechPowerUp, WikiChip, and Wikipedia entries for the product series. We also collected information about GPUs released before 2021 from the manufacturer\u2019s catalog or Wikipedia\u2019s list of processors.To disambiguate duplicates of different versions of the same product with different specifications, the AI Index added the part number or difference in specification, as applicable.To find GPU prices, the AI Index searched various sources including the manufacturer\u2019s website, Wikipedia, and TechPowerUp. GPU prices have been adjusted for inflation using CPI-U data provided by the U.S. Bureau of Labor Statistics. Missing data for certain GPUs was completed using additional sources, such as the manufacturer\u2019s website, Wikipedia, and TechPowerUp. This includes information such as manufacturer, type, release date, performance (double, single, and half-precision operations per second), die size, power, clock speed, process size, and number of transistors.Carbon Footprint of Select Machine Learning ModelsData on carbon-emission estimates of select machine learning models was sourced from the paper Luccioni et al., 2022. Data on carbon-emission estimates of real-life examples was retrieved from Strubell et al., 2019.Energy Savings Results  From BCOOLER ExperimentData on energy savings over time for the  BCOOLER experiment was sourced from the  paper Luo et al., 2022.Chapter 2: Technical PerformanceAppendixTable of Contents 363Artificial IntelligenceIndex Report 2023AppendixMeta-Analysis of Fairness  and Bias MetricsFor the analysis conducted on fairness and bias metrics in AI, we identify and report on benchmark and diagnostic metrics which have been consistently cited in the academic community, reported on a public leaderboard, or reported for publicly available baseline models (e.g., GPT-3, BERT, ALBERT). We note that research paper citations are a lagging indicator of adoption, and metrics which have been very recently adopted may not be reflected in the data for 2022. We include the full list of papers considered in the 2022 AI Index as well as the following additional papers:Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language ModelsBBQ: A Hand-Built Bias Benchmark for  Question AnsweringDiscovering Language Model Behaviors With  Model-Written Evaluations\u201cI\u2019m Sorry to Hear That\u201d: Finding New Biases in Language Models With a Holistic Descriptor DatasetOn Measuring Social Biases in Prompt-Based  Multi-task LearningPaLM: Scaling Language Modeling With PathwaysPerturbation Augmentation for Fairer NLPScaling Instruction-Finetuned Language ModelsSODAPOP: Open-Ended Discovery of Social  Biases in Social Commonsense Reasoning ModelsTowards Robust NLG Bias Evaluation  With Syntactically-Diverse PromptsVLStereoSet: A Study of Stereotypical Bias  in Pre-trained Vision-Language ModelsNatural Language Processing Bias MetricsIn Section 3.3, we track citations of the Perspective API created by Jigsaw at Google. The Perspective API has been adopted widely by researchers and engineers in natural language processing. Its creators define toxicity as \u201ca rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion,\u201d and the tool is powered by machine learning models trained on a proprietary dataset of comments from Wikipedia and news websites.We include the full list of papers considered in the 2022 AI Index as well as the following additional papers: AlexaTM 20B: Few-Shot Learning Using a  Large-Scale Multilingual Seq2Seq ModelAligning Generative Language Models With  Human ValuesChallenges in Measuring Bias via Open-Ended Language GenerationCharacteristics of Harmful Text: Towards  Rigorous Benchmarking of Language ModelsControllable Natural Language Generation With Contrastive PrefixesDD-TIG at SemEval-2022 Task 5: Investigating the Relationships Between Multimodal and Unimodal Information in Misogynous Memes Detection and ClassificationChapter 3: Technical AI EthicsAppendixChapter 3: Technical AI EthicsTable of Contents 364Artificial IntelligenceIndex Report 2023AppendixDetoxifying Language Models With a Toxic CorpusDisCup: Discriminator Cooperative Unlikelihood Prompt-Tuning for Controllable Text GenerationEvaluating Attribution in Dialogue Systems:  The BEGIN BenchmarkExploring the Limits of Domain-Adaptive Training  for Detoxifying Large-Scale Language ModelsFlamingo: A Visual Language Model for  Few-Shot LearningGalactica: A Large Language Model for ScienceGLaM: Efficient Scaling of Language Models  With Mixture-of-ExpertsGLM-130B: An Open Bilingual Pre-trained ModelGradient-Based Constrained Sampling From  Language ModelsHateCheckHIn: Evaluating Hindi Hate Speech Detection ModelsHolistic Evaluation of Language ModelsAn Invariant Learning Characterization of  Controlled Text GenerationLaMDA: Language Models for Dialog ApplicationsLeashing the Inner Demons: Self-Detoxification  for Language ModelsMeasuring Harmful Representations in Scandinavian Language ModelsMitigating Toxic Degeneration With Empathetic Data: Exploring the Relationship Between Toxicity and EmpathyMULTILINGUAL HATECHECK: Functional Tests for Multilingual Hate Speech Detection ModelsA New Generation of Perspective API: Efficient Multilingual Character-Level TransformersOPT: Open Pre-trained Transformer Language ModelsPaLM: Scaling Language Modeling With PathwaysPerturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and DefensePredictability and Surprise in Large Generative ModelsQuark: Controllable Text Generation With  Reinforced [Un]learningRed Teaming Language Models With Language ModelsReward Modeling for Mitigating Toxicity in Transformer-based Language ModelsRobust Conversational Agents Against Imperceptible Toxicity TriggersScaling Instruction-Finetuned Language ModelsStreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering ModelsTraining Language Models to Follow Instructions With Human FeedbackTransfer Learning From Multilingual DeBERTa  for Sexism IdentificationTransformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary SpaceWhile the Perspective API is used widely within machine learning research and also for measuring online toxicity, toxicity in the specific domains used to train the models undergirding Perspective (e.g., news, Wikipedia) may not be broadly representative of all forms of toxicity (e.g., trolling). Other known caveats include biases against text written by minority voices: The Perspective API has been shown to disproportionately assign high toxicity scores to text that contains mentions of minority identities (e.g., \u201cI am a gay man\u201d). As a result, detoxification techniques built with labels sourced from the Perspective API result in models that are less capable of modeling language used by minority groups, and may avoid mentioning minority identities.New versions of the Perspective API have been deployed since its inception, and there may be subtle undocumented shifts in its behavior over time.Chapter 3: Technical AI EthicsAppendixTable of Contents 365Artificial IntelligenceIndex Report 2023AppendixRealToxicityPromptsWe sourced the RealToxicityPrompts dataset of evaluations from the HELM benchmark website, as documented in v0.1.0. AI Ethics in ChinaThe data in this section is sourced from the 2022 paper AI Ethics With Chinese Characteristics? Concerns and Preferred Solutions in Chinese Academia. We are grateful to Junhua Zhu for clarifications and correspondence.AI Ethics Trends at FAccT  and NeurIPSTo understand trends at the ACM Conference on Fairness, Accountability, and Transparency, this section tracks FAccT papers published in conference proceedings from 2018 to 2022. We categorize author affiliations into academic, industry, nonprofit, government, and independent categories, while also tracking the location of their affiliated institution. Authors with multiple affiliations are counted once in each category (academic and industry), but multiple affiliations of the same type (i.e., authors belonging to two academic institutions) are counted once in the category.For the analysis conducted on NeurIPS publications, we identify workshops themed around real-world impact and label papers with a single main category in \u201chealthcare,\u201d \u201cclimate,\u201d \u201cfinance,\u201d \u201cdeveloping world,\u201d \u201cscience,\u201d or \u201cother,\u201d where \u201cother\u201d denotes a paper related to a real-world use case but not in one of the other categories. The \u201cscience\u201d category is new in 2022, but includes retroactive analysis of papers from previous years.We tally the number of papers in each category to reach the numbers found in Figure 3.7.3. Papers are not double-counted in multiple categories. We note that this data may not be as accurate for data pre-2018 as societal impacts work at NeurIPS has historically been categorized under a broad \u201cAI for social impact\u201d umbrella, but it has recently been split into more granular research areas. Examples include workshops dedicated to machine learning for health; climate; policy and governance; disaster response; and the developing world.To track trends around specific technical topics at NeurIPS as in Figures 3.7.4 to 3.7.7, we count the number of papers accepted to the NeurIPS main track with titles containing keywords (e.g., \u201ccounterfactual\u201d or \u201ccausal\u201d for tracking papers related to causal effect), as well as papers submitted to related workshops.TruthfulQAWe sourced the TruthfulQA dataset of evaluations from the HELM benchmark website, as documented in v0.1.0.Chapter 3: Technical AI EthicsAppendixTable of Contents 366Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixLightcastPrepared by Scott Bingham, Julia Nania, Layla O\u2019Kane, and Bledi TaskaLightcast delivers job market analytics that empower employers, workers, and educators to make data-driven decisions. The company\u2019s artificial intelligence technology analyzes hundreds of millions of job postings and real-life career transitions to provide insight into labor market patterns. This real-time strategic intelligence offers crucial insights, such as what jobs are most in demand, the specific skills employers need, and the career directions that offer the highest potential for workers. For more information, visit www.lightcast.io.Job Posting DataTo support these analyses, Lightcast mined its dataset of millions of job postings collected since 2010. Lightcast collects postings from over 51,000 online job sites to develop a comprehensive, real-time portrait of labor market demand. It aggregates job postings, removes duplicates, and extracts data from job postings text. This includes information on job title, employer, industry, and region, as well as required experience, education, and skills.Job postings are useful for understanding trends in the labor market because they allow for a detailed, real-time look at the skills employers seek. To assess the representativeness of job postings data, Lightcast conducts a number of analyses to compare the distribution of job postings to the distribution of official government and other third-party sources in the United States. The primary source of government data on U.S. job postings is the Job Openings and Labor Turnover Survey (JOLTS) program, conducted by the Bureau of Labor Statistics. Based on comparisons between JOLTS and Lightcast, the labor market demand captured by Lightcast data represents over 99% of the total labor demand. Jobs not posted online are usually in small businesses (the classic example being the \u201cHelp Wanted\u201d sign in a restaurant window) and union hiring halls.Measuring Demand for AIIn order to measure the demand by employers of AI skills, Lightcast uses its skills taxonomy of over 31,000 skills. The list of AI skills from Lightcast data are shown below, with associated skill clusters. While some skills are considered to be in the AI cluster specifically, for the purposes of this report, all skills below were considered AI skills. A job posting was considered an AI job if it mentioned any of these skills in the job text.Artificial Intelligence: AIOps (Artificial Intelligence for IT Operations), Applications of Artificial Intelligence, Artificial General Intelligence, Artificial Intelligence, Artificial Intelligence Development, Artificial Intelligence Markup Language (AIML), Artificial Intelligence Systems, Azure Cognitive Services, Baidu, Cognitive Automation, Cognitive Computing, Computational Intelligence, Cortana, Expert Systems, Intelligent Control, Intelligent Systems, Interactive Kiosk, IPSoft Amelia, Knowledge-Based Configuration, Knowledge-Based Systems, Multi-Agent Systems, Open Neural Network Exchange (ONNX), OpenAI Gym, Reasoning Systems, Soft Computing, Syman, Watson Conversation, Watson Studio, WekaChapter 4: The EconomyTable of Contents 367Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixAutonomous Driving: Advanced Driver Assistance Systems, Autonomous Cruise Control Systems, Autonomous System, Autonomous Vehicles, Guidance Navigation and Control Systems, Light Detection and Ranging (LiDAR), OpenCV, Path Analysis, Path Finding, Remote Sensing, Unmanned Aerial Systems (UAS)Natural Language Processing (NLP): Amazon Textract, ANTLR, BERT (NLP Model), Chatbot, Computational Linguistics, DeepSpeech, Dialog Systems, fastText, Fuzzy Logic, Handwriting Recognition, Hugging Face (NLP Framework), HuggingFace Transformers, Intelligent Agent, Intelligent Software Assistant, Intelligent Virtual Assistant, Kaldi, Latent Dirichlet Allocation, Lexalytics, Machine Translation, Microsoft LUIS, Natural Language Generation, Natural Language Processing, Natural Language Processing Systems, Natural Language Programming, Natural Language Toolkits, Natural Language Understanding, Natural Language User Interface, Nearest Neighbour Algorithm, OpenNLP, Optical Character Recognition (OCR), Screen Reader, Semantic Analysis, Semantic Interpretation for Speech Recognition, Semantic Parsing, Semantic Search, Sentiment Analysis, Seq2Seq, Speech Recognition, Speech Recognition Software, Statistical Language Acquisition, Text Mining, Tokenization, Voice Interaction, Voice User Interface, Word Embedding, Word2Vec ModelsNeural Networks: Apache MXNet, Artificial Neural Networks, Autoencoders, Caffe, Caffe2, Chainer, Convolutional Neural Networks, Cudnn, Deep Learning, Deeplearning4j, Keras (Neural Network Library), Long Short-Term Memory (LSTM), OpenVINO, PaddlePaddle, Pybrain, Recurrent Neural Network (RNN), TensorFlowMachine Learning: AdaBoost, Apache MADlib, Apache Mahout, Apache SINGA, Apache Spark, Association Rule Learning, Automated Machine Learning, Autonomic Computing, AWS SageMaker, Azure Machine Learning, Boosting, CHi-Squared Automatic Interaction Detection (CHAID), Classification And Regression Tree (CART), Cluster Analysis, Collaborative Filtering, Confusion Matrix, Cyber-Physical Systems, Dask (Software), Data Classification, DBSCAN, Decision Models, Decision Tree Learning, Dimensionality Reduction, Dlib (C++ Library), Ensemble Methods, Evolutionary Programming, Expectation Maximization Algorithm, Feature Engineering, Feature Extraction, Feature Learning, Feature Selection, Gaussian Process, Genetic Algorithm, Google AutoML, Google Cloud ML Engine, Gradient Boosting, H2O.ai, Hidden Markov Model, Hyperparameter Optimization, Inference Engine, K-Means Clustering, Kernel Methods, Kubeflow, LIBSVM, Machine Learning, Machine Learning Algorithms, Markov Chain, Matrix Factorization, Meta Learning, Microsoft Cognitive Toolkit (CNTK), MLflow, MLOps (Machine Learning Operations), mlpack (C++ Library), Naive Bayes, Perceptron, Predictionio, PyTorch (Machine Learning Library), Random Forest Algorithm, Recommendation Engine, Recommender Systems, Reinforcement Learning, Scikit-learn (Machine Learning Library), Semi-Supervised Learning, Soft Computing, Sorting Algorithm, Supervised Learning, Support Vector Machine, Test Datasets, Torch (Machine Learning), Training Datasets, Transfer Learning, Unsupervised Learning, Vowpal Wabbit, XgboostTable of Contents 368Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixRobotics: Advanced Robotics, Cognitive Robotics, Motion Planning, Nvidia Jetson, Robot Framework, Robot Operating Systems, Robotic Automation Software, Robotic Liquid Handling Systems, Robotic Programming, Robotic Systems, Servomotor, SLAM Algorithms (Simultaneous Localization and Mapping)Visual Image Recognition: 3D Reconstruction, Activity Recognition, Computer Vision, Contextual Image Classification, Digital Image Processing, Eye Tracking, Face Detection, Facial Recognition, Image Analysis, Image Matching, Image Processing, Image Recognition, Image Segmentation, Image Sensor, Imagenet, Machine Vision, Motion Analysis, Object Recognition, OmniPage, Pose Estimation, RealSenseLinkedInPrepared by Murat Erer and Akash KauraCountry SampleIncluded countries represent a select sample of eligible countries with at least 40% labor force coverage by LinkedIn and at least 10 AI hires in any given month. China and India were included in this sample because of their increasing importance in the global economy, but LinkedIn coverage in these countries does not reach 40% of the workforce. Insights for these countries may not provide as full a picture as other countries, and should be interpreted accordingly.Skills (and AI Skills)LinkedIn members self-report their skills on their LinkedIn profiles. Currently, more than 38,000 distinct, standardized skills are identified by LinkedIn. These have been coded and classified by taxonomists at LinkedIn into 249 skill groupings, which are the skill groups represented in the dataset. The top skills that make up the AI skill grouping are machine learning, natural language processing, data structures, artificial intelligence, computer vision, image processing, deep learning, TensorFlow, Pandas (software), and OpenCV, among others.Skill groupings are derived by expert taxonomists through a similarity-index methodology that measures skill composition at the industry level. LinkedIn\u2019s industry taxonomy and their corresponding NAICS codes can be found here.Skills GenomeFor any entity (occupation or job, country, sector, etc.), the skill genome is an ordered list (a vector) of the 50 \u201cmost characteristic skills\u201d of that entity. These most characteristic skills are identified using a TF-IDF algorithm to identify the most representative skills of the target entity, while down-ranking ubiquitous skills that add little information about that specific entity (e.g., Microsoft Word).TF-IDF is a statistical measure that evaluates how representative a word (in this case a skill) is to a selected entity). This is done by multiplying two metrics: 1. The term frequency of a skill in an entity (TF).  2. The logarithmic inverse entity frequency of the skill across a set of entities (IDF). This indicates how common or rare a word is in the entire entity set. The closer IDF is to 0, the more common the word.So if the skill is very common across LinkedIn entities, and appears in many job or member descriptions, the IDF will approach 0. If, on the other hand, the skill is unique to specific entities, the IDF will approach 1. More details are available at LinkedIn\u2019s Skills Genome and LinkedIn-World Bank Methodology.Table of Contents 369Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixAI Skills PenetrationThe aim of this indicator is to measure the intensity of AI skills in an entity (a particular country, industry, gender, etc.) through the following methodology: \u2022  Compute frequencies for all self-added skills by LinkedIn members in a given entity (occupation, industry, etc.) in 2015\u20132021. \u2022  Re-weight skill frequencies using a TF-IDF model to get the top 50 most representative skills in that entity. These 50 skills compose the \u201cskill genome\u201d of that entity. \u2022  Compute the share of skills that belong to the AI skill group out of the top skills in the selected entity.Interpretation: The AI skill penetration rate signals the prevalence of AI skills across occupations, or the intensity with which LinkedIn members utilize AI skills in their jobs. For example, the top 50 skills for the occupation of engineer are calculated based on the weighted frequency with which they appear in LinkedIn members\u2019 profiles. If four of the skills that engineers possess belong to the AI skill group, this measure indicates that the penetration of AI skills is estimated to be 8% among engineers (i.e., 4/50).Jobs or OccupationsLinkedIn member titles are standardized and grouped into approximately 15,000 occupations. These are not sector- or country-specific. These occupations are further standardized into approximately 3,600 occupation representatives. Occupation representatives group occupations with a common role and specialty, regardless of seniority.AI Jobs and OccupationsAn \u201cAI\u201d job (technically, occupation representative) is an occupation representative that requires AI skills to perform the job. Skills penetration is used as a signal for whether AI skills are prevalent in an occupation representative in any sector where the occupation representative may exist. Examples of such occupations include (but are not limited to): machine learning engineer, artificial intelligence specialist, data scientist, computer vision engineer, etc.AI TalentA LinkedIn member is considered AI talent if they have explicitly added AI skills to their profile and/or they are occupied in an AI occupation representative. The counts of AI talent are used to calculate talent concentration metrics. For example, to calculate the country level AI talent concentration, we use the counts of AI talent at the country level vis-a-vis the counts of LinkedIn members in the respective countries.Relative AI Skills PenetrationTo allow for skills penetration comparisons across countries, the skills genomes are calculated and a relevant benchmark is selected (e.g., global average). A ratio is then constructed between a country\u2019s and the benchmark\u2019s AI skills penetrations, controlling for occupations.Interpretation: A country\u2019s relative AI skills penetration of 1.5 indicates that AI skills are 1.5 times as frequent as in the benchmark, for an overlapping set of occupations.Global ComparisonFor cross-country comparison, we present the relative penetration rate of AI skills, measured as the sum of the penetration of each AI skill across occupations in a given country, divided by the average global penetration of AI skills across the overlapping occupations in a sample of countries.Table of Contents 370Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixInterpretation: A relative penetration rate of 2 means that the average penetration of AI skills in that country is two times the global average across the same set of occupations.Global Comparison: By IndustryThe relative AI skills penetration by country for industry provides an in-depth sectoral decomposition of AI skill penetration across industries and sample countries.Interpretation: A country\u2019s relative AI skill penetration rate of 2 in the education sector means that the average penetration of AI skills in that country is two times the global average across the same set of occupations in that sector.Global Comparison: By GenderThe \u201cRelative AI Skills Penetration by Gender\u201d metric provides a cross-country comparison of AI skill penetrations within each gender, comparing countries\u2019 male or female AI skill penetrations to the global average of the same gender. Since the global averages are distinct for each gender, this metric should only be used to compare country rankings within each gender, and not for cross-gender comparisons within countries.Interpretation: A country\u2019s AI skills penetration for women of 1.5 means that female members in that country are 1.5 times more likely to list AI skills than the average female member in all countries pooled together across the same set of occupations that exist in the country/gender combination.Global Comparison: Across GenderThe \u201cRelative AI Skills Penetration Across Genders\u201d metric allows for cross-gender comparisons within and across countries globally, since we compare the countries\u2019 male and female AI skill penetrations to the same global average regardless of gender.Interpretation: A country\u2019s \u201cRelative AI Skills Penetration Across Genders\u201d for women of 1.5 means that female members in that country are 1.5 times more likely to list AI skills than the average member in all countries pooled together across the same set of occupations that exist in the country.Relative AI Hiring Index  LinkedIn Hiring Rate or Overall Hiring Rate is a measure of hires normalized by LinkedIn membership. It is computed as the percentage of LinkedIn members who added a new employer in the same period the job began, divided by the total number of LinkedIn members in the corresponding location.  AI Hiring Rate is computed following the overall hiring rate methodology, but only considering members classified as AI talent.  Relative AI Hiring Index is the pace of change in AI Hiring Rate normalized by the pace of change in Overall Hiring Rate, providing a picture of whether hiring of AI talent is growing at a higher, equal, or lower rate than overall hiring in a market. The relative AI Hiring Index is equal to 1.0 when AI hiring and overall hiring are growing at the same rate year on year.Interpretation: Relative AI Hiring Index shows how fast each country is experiencing growth in AI talent hiring relative to growth in overall hiring in the country. A ratio of 1.2 means the growth in AI talent hiring has outpaced the growth in overall hiring by 20%.Table of Contents 371Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixChangelog From Methodology Included in Last Year\u2019s AI Index  1. LinkedIn ramped a new version of its industry taxonomy (see details here).    a. This has resulted in changes to our top level five key industries. We have made the full-time series available for each industry (as with prior years).    i. \u201cSoftware & IT Services\u201d industry evolved into a wider \u201cTechnology, Information and Media,\u201d which encompasses media and telecommunications as well as other sub-industries.    ii. Former \u201cHardware & Networking\u201d industry does not exist in the new taxonomy, so we introduced \u201cProfessional Services\u201d industry as the fifth industry in scope which contains a high concentration of AI talent.    iii. Remaining \u201cEducation,\u201d \u201cManufacturing,\u201d and \u201cFinancial Services\u201d (formerly known as \u201cFinance\u201d) also had updates in their coverage resulting from the inclusion of more granular sub-industries.   b. This also resulted in minor changes in magnitudes for some metrics since the distinct number of industries, as well as the distinct number of AI occupations defined within each country-industry pair have changed:    i. We define AI occupations (occupation representatives that require AI skills to perform the job) and the respective definition of AI Talent at Country-Industry level. For example, data engineers working in the technology, information, and media industry in Germany may be identified as holding an AI occupation, whereas data engineers working in the construction industry in the United Arab Emirates may not be identified as AI Talent. Following the introduction of a more granular industry taxonomy with improved accuracy, our AI Talent identifications have been improved, and results have been reflected to the entirety of time series for each relevant metric.    ii. The following metrics have been impacted by this change in industry taxonomy: AI Talent Concentrations, and Relative AI Hiring Rates. No directional changes were observed, only minor changes in magnitudes.  2. We introduced a methodology change into Relative Skills Penetration metrics:   a. In the past, the data used to calculate these metrics were limited to top five industries with the highest AI skill penetration globally: \u201cSoftware & IT Services,\u201d \u201cHardware & Networking,\u201d \u201cManufacturing,\u201d \u201cEducation,\u201d and \u201cFinance\u201d industries. This year we updated our coverage to all industries.Table of Contents 372Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixNetBase QuidPrepared by Bill Valle and Nicole SeredenkoNetBase Quid delivers AI-powered consumer and market intelligence to enable business reinvention in a noisy and unpredictable world. The software applies artificial intelligence to reveal patterns in large, unstructured datasets and to generate visualizations that enable users to make smart, data-driven decisions accurately, quickly, and efficiently. NetBase Quid uses Boolean query to search for focus areas, topics, and keywords within social media, news, forums and blogs, companies, and patents data sources, as well as other custom datasets. NetBase Quid then visualizes these data points based on the semantic similarity.Search, Data Sources, and ScopeOver 8 million global public and private company profiles from multiple data sources are indexed in order to search across company descriptions, while filtering and including metadata ranging from investment information to firmographic information, such as founded year, HQ location, and more. Company information is updated on a weekly basis. The NetBase Quid algorithm reads a big amount of text data from each document to make links between different documents based on their similar language. This process is repeated at an immense scale, which produces a network with different clusters identifying distinct topics or focus areas. Trends are identified based on keywords, phrases, people, companies, and institutions that NetBase Quid identifies, and the other metadata that is put into the software.DataCompaniesOrganization data is embedded from Capital IQ and Crunchbase. These companies include all types of companies (private, public, operating, operating as a subsidiary, out of business) throughout the world. The investment data includes private investments, M&A, public offerings, minority stakes made by PE/VCs, corporate venture arms, governments, and institutions both within and outside the United States. Some data is simply unreachable\u2014for instance, when investors\u2019 names or funding amounts are undisclosed.NetBase Quid embeds Capital IQ data as a default and adds in data from Crunchbase for the data points that are not captured in Capital IQ. This not only yields comprehensive and accurate data on all global organizations, but it also captures early-stage startups and funding events data. Company information is updated on a weekly basis.Earnings CallsNetBase Quid leverages earnings call transcript data embedded from Seeking Alpha. For this report, NetBase Quid has analyzed mentions of AI-related keywords across all earnings call transcripts from Fortune 500 companies from January 2018 through December 2022. New earnings call transcript data is updated in NetBase Quid on the 1st and 15th of every month.Search ParametersBoolean query is used to search for focus areas, topics, and keywords within the archived company database, within their business descriptions and websites. We can filter out the search results by HQ regions, investment amount, operating status, organization type (private/public), and founding year. NetBase Quid then visualizes these companies by semantic similarity. If there are more than 7,000 companies from the search result, NetBase Quid selects the 7,000 most relevant companies for visualization based on the language algorithm.Table of Contents 373Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixBoolean Search: \u201cartificial intelligence\u201d or \u201cAI\u201d or \u201cmachine learning\u201d or \u201cdeep learning\u201dCompanies: \u2022  Global AI and ML companies that have received investments (private, IPO, M&A) from January 1, 2013, to December 31, 2022. \u2022  Global AI and ML companies that have received over $1.5M for the last 10 years (January 1, 2013, to December 31, 2022): 7,000 out of 7,500 companies have been selected through NetBase Quid\u2019s relevance algorithm.Target Event Definitions \u2022  Private investments: A private placement is a private sale of newly issued securities (equity or debt) by a company to a selected investor or a selected group of investors. The stakes that buyers take in private placements are often minority stakes (under 50%), although it is possible to take control of a company through a private placement as well, in which case the private placement would be a majority stake investment. \u2022  Minority investment: These refer to minority stake acquisitions in NetBase Quid, which take place when the buyer acquires less than 50% of the existing ownership stake in entities, asset products, and business divisions. \u2022  M&A: This refers to a buyer acquiring more than 50% of the existing ownership stake in entities, asset products, and business divisions.McKinsey & CompanyData used in the Corporate Activity-Industry Adoption section was sourced from the McKinsey Global Survey \u201cThe State of AI in 2022\u2014and a Half Decade in Review.\u201dThe online survey was in the field from May 3, 2022, to May 27, 2022, and from August 15, 2022, to August 17, 2022, and garnered responses from 1,492 participants representing a full range of regions, industries, company sizes, functional specialties, and tenures. Of those respondents, 744 said their organization had adopted AI in at least one function and were asked questions about their organization\u2019s AI use. To adjust for differences in response rates, the data is weighted by the contribution of each respondent\u2019s nation to global GDP.The AI Index also considered data from previous iterations of the survey. More specifically, the AI index made use of data from:The State of AI in 2021The State of AI in 2020Global AI Survey: AI Proves Its Worth,  But Few Scale Impact (2019)AI Adoption Advances, But Foundational Barriers Remain (2018)Table of Contents 374Artificial IntelligenceIndex Report 2023AppendixChapter 4: The EconomyAppendixGitHubData on the effects of GitHub\u2019s Copilot on developer productivity and happiness was sourced from the GitHub Copilot Survey conducted in 2022.The survey was emailed to 17,420 users who had opted in to receive communications and were using GitHub Copilot for their daily programming activities. Between February 10, 2022, and March 6, 2022, the authors received 2,047 responses that could be matched with usage measurements during the four-week period leading up to March 12, 2022. The survey contained multiple-choice questions on demographic information and Likert-type questions on different aspects of productivity, which were randomized in the order of appearance to the user.More details can be found in Ziegler at al., 2022.DeloitteData used in the Corporate Activity-Industry Motivation section was sourced from Deloitte\u2019s \u201cState of AI in the Enterprise\u201d surveys.More specifically, the AI Index made use of the following sources of information:Deloitte\u2019s State of AI in the Enterprise,  5th Edition Report (2022)State of AI in the Enterprise, 4th Edition (2021)Deloitte\u2019s State of AI in the Enterprise, 3rd Edition (2020)State of AI in the Enterprise, 2nd Edition (2018)The 2017 Deloitte State of Cognitive Survey (2017)To obtain a global view of how AI is transforming organizations, Deloitte surveyed 2,620 global business leaders between April 2022 and May 2022. Thirteen countries were represented: Australia (100 respondents), Brazil (115 respondents), Canada (175 respondents), China (200 respondents), France (130 respondents), Germany (150 respondents), India (200 respondents), Israel (75 respondents), Japan (100 respondents), Singapore (100 respondents), South Africa (75 respondents), the United Kingdom (200 respondents), and the United States (1,000 respondents). All participating companies have adopted AI technologies and are AI users. Respondents were required to meet one of the following criteria: responsible for AI technology spending or approval of AI investments, developing AI technology strategies, managing or overseeing AI technology implementation, serving as an AI technology subject matter specialist, or making or influencing decisions around AI technology. To complement the blind survey, Deloitte conducted qualitative telephone interviews with 15 AI specialists from various industries. More details are available on Deloitte\u2019s website.International Federation of Robotics (IFR)Data presented in the Robot Installations section was sourced from the \u201cWorld Robotics 2022\u201d report.Table of Contents Appendix 375Artificial IntelligenceIndex Report 2023 Chapter 5: EducationAppendixComputing Research Association (CRA Taulbee Survey)Note: This year\u2019s AI Index reused the methodological notes that were submitted by the CRA for previous editions of the AI Index. For more complete delineations of the methodology used by the CRA, please consult the individual CRA surveys that are linked below.Computing Research Association (CRA) members are 200-plus North American organizations active in computing research: academic departments of computer science and computer engineering; laboratories and centers in industry, government, and academia; and affiliated professional societies (AAAI, ACM, CACS/AIC, IEEE Computer Society, SIAM USENIX). CRA\u2019s mission is to enhance innovation by joining with industry, government, and academia to strengthen research and advanced education in computing. Learn more about CRA here.The CRA Taulbee Survey gathers survey data during the fall of each academic year by reaching out to over 200 PhD-granting departments. Details about the Taulbee Survey can be found here. Taulbee doesn\u2019t directly survey the students. The department identifies each new PhD\u2019s area of specialization as well as their type of employment. Data is collected from September to January of each academic year for PhDs awarded in the previous academic year. Results are published in May after data collection closes.The CRA Taulbee Survey is sent only to doctoral departments of computer science, computer engineering, and information science/systems. Historically, (a) Taulbee covers one-quarter to one-third of total BS CS recipients in the United States; (b) the percent of women earning bachelor\u2019s degrees is lower in the Taulbee schools than overall; and (c) Taulbee tracks the trends in overall CS production.The AI Index used data from the following iterations of the CRA survey:CRA, 2021CRA, 2020CRA, 2019CRA, 2018CRA, 2017CRA, 2016CRA, 2015CRA, 2014CRA, 2013CRA, 2012CRA, 2011Chapter 5: EducationTable of Contents Appendix 376Artificial IntelligenceIndex Report 2023 Chapter 5: EducationAppendixCode.org State Level DataThe following link includes a full description of the methodology used by Code.org to collect its data. The staff at Code.org also maintains a database of the state of American K\u201312 education and, in this policy primer, provides a greater amount of detail on the state of American K\u201312 education in each state.AP Computer Science DataThe AP computer science data is provided to Code.org as per an agreement the College Board maintains with Code.org. The AP Computer Science data comes from the college board\u2019s national and state summary reports.The State of International  K\u201312 EducationData on the state of international K\u201312 AI education was taken from the following UNESCO report, published in 2021. The methodology is outlined in greater detail on pages 18 to 20 in the report and, for the sake of brevity, is not completely reproduced in the 2023 AI index.Table of Contents 377Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixChapter 6: Policy and GovernanceGlobal Legislation Records on AIFor AI-related bills passed into laws, the AI Index performed searches of the keyword \u201cartificial intelligence\u201d on the websites of 127 countries\u2019 congresses or parliaments (in the respective languages) in the full text of bills. Note that only laws passed by state-level legislative bodies and signed into law (i.e., by presidents or through royal assent) from 2016 to 2022 are included. Laws that were approved but then repealed are not included in the analysis. In some cases, there were databases that were only searchable by title, so site search functions were deployed. Future AI Index reports hope to include analysis on other types of legal documents, such as regulations and standards, adopted by state- or supranational-level legislative bodies, government agencies, etc. The AI Index team surveyed the following databases:AlgeriaAndorraAntigua and BarbudaArgentinaArmeniaAustraliaAustriaAzerbaijanThe BahamasBahrainBangladeshBarbadosBelarusBelgiumBelizeBermudaBhutanBoliviaBrazilBruneiBulgariaBurkina FasoCameroonCanadaCayman IslandsChileChinaColombiaCroatiaCubaCuracaoCyprusCzech RepublicDenmarkEstoniaFaroe IslandsFijiFinlandFranceThe GambiaGeorgiaGermanyGibraltarGreeceGreenlandGrenadaGuamGuatemalaGuyanaHong KongHungaryIcelandIndiaIran, Islamic RepublicIraqIrelandIsle of ManIsraelItalyJamaicaJapanKazakhstanKenyaKiribatiKorea, RepublicKosovoKyrgyz RepublicLatviaLebanonLiechtensteinLithuaniaLuxembourgMacao SAR, ChinaMalawiMalaysiaMaltaMauritiusMexicoMonacoMontenegroMoroccoMozambiqueNauruThe NetherlandsNew ZealandNicaraguaNigerNorthern Marina IslandsNorwayPanamaPapua New GuineaPhilippinesPolandPortugalRomaniaRussiaSamoaSaudi ArabiaSerbiaSeychellesSierra LeoneSingaporeSlovak RepublicSloveniaSouth AfricaSpainSri LankaSt. Kitts and NevisSurinameSwedenSwitzerlandTajikistanTanzaniaTogoTongoTurkeyTuvaluUgandaUkraineUnited Arab EmiratesUnited KingdomUnited StatesUruguayVietnamYemenZambiaZimbabweTable of Contents 378Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixUnited States State-Level AI LegislationFor AI-related bills passed into law, the AI Index performed searches of the keyword \u201cartificial intelligence\u201d on the legislative websites of all 50 U.S. states in the full text of bills. Bills are only counted as passed into law if the final version of the bill includes the keyword, not just the introduced version. Note that only laws passed from 2015 to 2022 are included. The count for proposed laws includes both laws that were proposed and eventually passed as well as laws that were proposed that have not yet been passed, or are now inactive. In some cases, databases were only searchable by title, so site search functions were deployed. The AI Index team surveyed the following databases:AlabamaAlaskaArizonaArkansasCaliforniaColoradoConnecticutDelawareFloridaGeorgiaHawaiiIdahoIllinoisIndianaIowaKansasKentuckyLouisianaMaineMarylandMassachusettsMichiganMinnesotaMississippiMissouriMontanaNebraskaNevadaNew HampshireNew JerseyNew MexicoNew YorkNorth CarolinaNorth DakotaOhioOklahomaOregonPennsylvaniaRhode IslandSouth CarolinaSouth DakotaTennesseeTexasUtahVermontVirginiaWashingtonWest VirginiaWisconsinWyomingTable of Contents 379Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixGlobal AI MentionsFor mentions of AI in AI-related legislative proceedings around the world, the AI Index performed searches of the keyword \u201cartificial intelligence\u201d on the websites of 81 countries\u2019 congresses or parliaments (in the respective languages), usually under sections named \u201cminutes,\u201d \u201chansard,\u201d etc. In some cases, databases were only searchable by title, so site search functions were deployed. The AI Index team surveyed the following databases:AndorraAngolaArmeniaAustraliaAzerbaijanBarbadosBelgiumBermudaBhutanBrazilCabo VerdeCanadaCayman IslandsChina11Czech RepublicDenmarkDominican RepublicEcuadorEl SalvadorEstoniaFijiFinlandFranceThe GambiaGermanyGibraltarGreeceHong KongIcelandIndiaIrelandIsle of ManIsraelItalyJapanKenyaKosovoLatviaLesothoLiechtensteinLuxembourgMacao SAR, ChinaMadagascarMalaysiaMaldivesMaltaMauritiusMexicoMoldovaNetherlandsNew ZealandNorthern Mariana IslandsNorwayPakistanPanamaPapua New GuineaPhilippinesPolandPortugalRomaniaRussiaSamoaSan MarinoSeychellesSierra LeoneSingaporeSloveniaSouth AfricaSouth KoreaSpainSri LankaSwedenSwitzerlandTanzaniaTrinidad and TobagoUkraineUnited KingdomUnited StatesUruguayZambiaZimbabwe11 The National People\u2019s Congress is held once per year and does not provide full legislative proceedings. Hence, the counts included in the analysis only searched mentions of \u201cartificial intelligence\u201d in the only public document released from the Congress meetings, the Report on the Work of the Government, delivered by the premier.Table of Contents 380Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixUnited States  Committee MentionsIn order to research trends on the United States\u2019 committee mentions of AI, the following search was conducted:Website: Congress.govKeyword: artificial intelligenceFilters: Committee ReportsUnited States AI Policy PapersOrganizationsTo develop a more nuanced understanding of the thought leadership that motivates AI policy, we tracked policy papers published by 55 organizations in the United States or with a strong presence in the United States (expanded from last year\u2019s list of 36 organizations) across four broad categories: \u2022  Civil Society, Associations, and Consortiums: Algorithmic Justice League, Alliance for Artificial Intelligence in Healthcare, Amnesty International, EFF, Future of Privacy Forum, Human Rights Watch, IJIS Institute, Institute for Electrical and Electronics Engineers, Partnership on AI  \u2022  Consultancy: Accenture, Bain & Company, Boston Consulting Group, Deloitte, McKinsey & Company  \u2022  Government Agencies: Congressional Research Service, Defense Technical Information Center, Government Accountability Office, Library of Congress, Pentagon Library \u2022  Private Sector Companies: Google AI, Microsoft AI, Nvidia, OpenAI  \u2022  Think Tanks and Policy Institutes: American Enterprise Institute, Aspen Institute, Atlantic Council, Brookings Institute, Carnegie Endowment for International Peace, Cato Institute, Center for a New American Security, Center for Strategic and International Studies, Council on Foreign Relations, Heritage Foundation, Hudson Institute, MacroPolo, National Security Institute, New America Foundation, RAND Corporation, Rockefeller Foundation, Stimson Center, Urban Institute, Wilson Center  \u2022  University Institutes and Research Programs: AI and Humanity, Cornell University; AI Now Institute, New York University; AI Pulse, UCLA Law; Belfer Center for Science and International Affairs, Harvard University; Berkman Klein Center, Harvard University; Center for Information Technology Policy, Princeton University; Center for Long-Term Cybersecurity, UC Berkeley; Center for Security and Emerging Technology, Georgetown University; CITRIS Policy Lab, UC Berkeley; Hoover Institution, Stanford University; Institute for Human-Centered Artificial Intelligence, Stanford University; Internet Policy Research Initiative, Massachusetts Institute of Technology; MIT Lincoln Laboratory; Princeton School of Public and International AffairsTable of Contents 381Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixMethodologyEach broad topic area is based on a collection of underlying keywords that describe the content of the specific paper. We included 17 topics that represented the majority of discourse related to AI between 2018\u20132021. These topic areas and the associated keywords are listed below: \u2022  Health and Biological Sciences: medicine, healthcare systems, drug discovery, care, biomedical research, insurance, health behaviors, COVID-19, global health \u2022  Physical Sciences: chemistry, physics, astronomy, earth science \u2022  Energy and Environment: energy costs, climate change, energy markets, pollution, conservation, oil and gas, alternative energy \u2022  International Affairs and International Security: international relations, international trade, developing countries, humanitarian assistance, warfare, regional security, national security, autonomous weapons \u2022  Justice and Law Enforcement: civil justice, criminal justice, social justice, police, public safety, courts \u2022  Communications and Media: social media, disinformation, media markets, deepfakes \u2022  Government and Public Administration: federal government, state government, local government, public sector efficiency, public sector effectiveness, government services, government benefits, government programs, public works, public transportation \u2022  Democracy: elections, rights, freedoms, liberties, personal freedoms \u2022  Industry and Regulation: economy, antitrust, M&A, competition, finance, management, supply chain, telecom, economic regulation, technical standards, autonomous vehicle industry and regulation \u2022  Innovation and Technology: advancements and improvements in AI technology, R&D, intellectual property, patents, entrepreneurship, innovation ecosystems, startups, computer science, engineering \u2022  Education and Skills: early childhood, K\u201312, higher education, STEM, schools, classrooms, reskilling \u2022  Workforce and Labor: labor supply and demand, talent, immigration, migration, personnel economics, future of work \u2022  Social and Behavioral Sciences: sociology, linguistics, anthropology, ethnic studies, demography, geography, psychology, cognitive science \u2022  Humanities: arts, music, literature, language, performance, theater, classics, history, philosophy, religion, cultural studies \u2022  Equity and Inclusion: biases, discrimination, gender, race, socioeconomic inequality, disabilities, vulnerable populations \u2022  Privacy, Safety, and Security: anonymity, GDPR, consumer protection, physical safety, human control, cybersecurity, encryption, hacking \u2022  Ethics: transparency, accountability, human values, human rights, sustainability, explainability, interpretability, decision-making normsTable of Contents 382Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixNational AI StrategiesThe AI Index did a web search to identify national strategies on AI. Below is a list of countries that were identified as having a national AI strategy, including a link to said strategy. For certain counties, noted with an asterisk(*), the actual strategy was not found, and a news article confirming the launch of the strategy was linked instead.Countries with AI Strategies in PlaceFederal Budget for Nondefense AI R&DData on the federal U.S. budget for nondefense AI R&D was taken from previous editions of the AI Index (namely the 2021 and 2022 versions) and from the following National Science and Technology Council reports:Supplement to the President\u2019s FY 2023 BudgetSupplement to the President\u2019s FY2022 BudgetU.S. Department of Defense Budget RequestsData on the DoD nonclassified AI-related budget requests was taken from previous editions of the AI Index (namely the 2021 and 2022 versions) and from the following reports:Defense Budget Overview United States Department of Defense  Fiscal Year 2023 Budget RequestDefense Budget Overview United States Department of Defense  Fiscal Year 2022 Budget RequestCountries with AI Strategies in DevelopmentAlgeria*ArgentinaAustraliaAustriaBangladeshBotswana*BrazilBulgariaCanadaChileChinaColombiaCroatiaCyprusCzech RepublicDenmarkEgypt, Arab RepublicEstoniaFinlandFranceGermanyGreeceHungaryIndiaIndonesiaIrelandItalyJapanKenyaKorea, RepublicLatviaLithuaniaLuxembourgMaltaMauritiusMexicoThe NetherlandsNorwayPeruPhilippinesPolandPortugalQatarRomaniaRussiaSaudi ArabiaSerbiaSierra LeoneSingaporeSloveniaSpainSwedenSwitzerlandThailandTunisia*TurkeyUkraineUnited Arab EmiratesUnited KingdomUnited StatesUruguayVietnamArmeniaAzerbaijanBahrainBelgiumBeninCubaIcelandIsraelJordanMoroccoNew ZealandNigeriaOmanUzbekistanTable of Contents 383Artificial IntelligenceIndex Report 2023AppendixChapter 6: Policy and GovernanceAppendixGoviniGovini is the leading commercial data company in the defense technology space. Built by Govini, Ark.ai is used at scale across the national security sector of the U.S. federal government. This platform enables government analysts, program managers, and decision-makers to gain unprecedented visibility into the companies, capabilities, and capital in national security to solve challenges pertaining to acquisition, foreign influence and adversarial capital, nuclear modernization, procurement, science and technology, and supply chain.Govini curated USG AI spend data from their annual Scorecard Taxonomy by applying supervised machine learning (ML) and natural language processing (NLP) to parse, analyze, and categorize large volumes of federal contracts data, including prime contracts, grants, and other transaction authority (OTA) awards. Govini\u2019s most recent scorecard focused on critical technologies, of which AI/ML technologies was a segment and consistent of six subsegments: data-at-scale, decision science, computer vision, machine learning, autonomy, and natural language processing. By initially generating search terms and then subsequently excluding specific terms that yield erroneous results, Govini delivers a comprehensive yet discriminant taxonomy of subsegments that are mutually exclusive. Repeated keyword searches and filters allow a consensus, data-driven taxonomy to come into focus. Govini SMEs conduct a final review of taxonomic structure to complement this iterative, data-driven process.The use of AI and supervised ML models enables the analysis of large volumes of irregular data contained in federal contracts\u2014data that is often inaccessible through regular government reporting processes or human-intensive analytical approaches.Moreover, beyond simply making usable an expansive body of data sources, Govini\u2019s SaaS Platform and National Security Knowledge Graph establishes high fidelity standards in categorized and fused data to produce a comprehensive and accurate depiction of federal spending, and the supporting vendor ecosystem, over time.U.S. AI-Related Legal CasesTo identify AI-related legal cases, the AI Index research team did a keyword search on the LexisNexis database, under their U.S. legal cases filter. The keywords that were searched include \u201cartificial intelligence,\u201d \u201cmachine learning,\u201d and \u201cautomated decision-making.\u201d Cases that contained one of these keywords were coded according to a variety of variables of interest.Table of Contents 384Artificial IntelligenceIndex Report 2023AppendixChapter 7: DiversityAppendixComputing Research Association (CRA Taulbee Survey)To learn more about the diversity data from the CRA, please read the methodological note on the CRA\u2019s data included in the Chapter 5 subsection of the Appendix.Code.orgTo learn more about the diversity data from Code.org, please read the methodological note on Code.org\u2019s data included in the Chapter 5 subsection of the Appendix.Chapter 7: DiversityTable of Contents Appendix 385Artificial IntelligenceIndex Report 2023NetBase Quid  Social Media DataNetBase Quid collects social media data from over 500 million sources in real time and analyzes this data through AI-powered Natural Language Processing. This process parses out language and breaks out posts by filters such as drivers of positive and negative sentiment, emotions, and behaviors, allowing for deeper insights to be reached. To understand public perception of advancements in artificial intelligence, NetBase Quid analyzed social media conversation around AI and AI model releases from January 2022 to December 2022. First, the NetBase Quid team analyzed conversation around AI to understand key drivers of general sentiment around AI advancements, such as ethical, cultural, and economic concerns and perceptions among consumers. Then, the NetBase Quid team leveraged the platform for a more targeted analysis of the same conversation, understanding volume and sentiment around the major AI model updates and releases in 2022. This NetBase Quid analysis ultimately showcases the relationship between public perception and the advancement of AI, leveraging targeted analytics tools to understand both specific reactions to model releases as well as a wider consumer conversation and what drives it.Chapter 8: Public OpinionAppendixIPSOSFor brevity, the 2023 AI Index does not republish the methodology used by the IPSOS survey that features in the report. More details about the IPSOS survey\u2019s methodology can be found in the actual survey.Lloyd\u2019s Register Foundation and GallupFor brevity, the 2023 AI Index does not republish the methodology used by the Lloyd\u2019s Register Foundation and Gallup survey that features in the report. More details about the Lloyd\u2019s Register Foundation and Gallup survey methodology can be found in the actual survey.Pew ResearchFor brevity, the 2023 AI Index does not republish the methodology used by the Pew Research survey that features in the report. More details on the Pew Research survey methodology can be found in the actual survey.Chapter 8: Public OpinionArtificial IntelligenceIndex Report 2023",
        "title": "Artificial Intelligence Index Report 2023 - HAI",
        "citationCount": 0
    },
    {
        "content": "Marquette Universitye-Publications@MarquettePhilosophy Faculty Research and Publications Philosophy, Department of11-28-2011Artificial Intelligence and AngelologyHoward P. KainzMarquette University, howard.kainz@marquette.eduAccepted version. Crisis Magazine (November 28, 2011). Permalink. \u00a9 2011 Crisis Magazine. Usedwith permission. Marquette University e-Publications@Marquette  Philosophy Faculty Research and Publications/College of Arts and Sciences  This paper is NOT THE PUBLISHED VERSION; but the author\u2019s final, peer-reviewed manuscript. The published version may be accessed by following the link in the citation below.  Crisis Magazine. Link. This article is \u00a9 Crisis Magazine and permission has been granted for this version to appear in e-Publications@Marquette. Crisis Magazine does not grant permission for this article to be further copied/distributed or hosted elsewhere without the express permission from Crisis Magazine.  Contents One of the better philosophy newsgroups on the Internet .......................................................................................2 Microprocessors and Angelic Self-possession ............................................................................................................2 ROM and Innate Ideas: ...............................................................................................................................................3 RAM and the Negative Active Potency of Angels: ......................................................................................................3 Hard Drives and Intellectual Memory: .......................................................................................................................3 The Operating System and the Reception of Information: ........................................................................................4 Software and Habitually Acquired Species: ................................................................................................................4 \u201cDownward Compatibility\u201d and Proportional Universality: .......................................................................................4 Multitasking: ...............................................................................................................................................................5 Handshaking\u201d and the Ability to Transfer Information: .............................................................................................5 Networks and the Transmission of Ideas, via Hierarchies: ........................................................................................5 Speed-caches and the Effect of Immateriality: ..........................................................................................................5   Artificial Intelligence and Angelology Howard Kainz Philosophy Department, Marquette University, Milwaukee, WI  One of the better philosophy newsgroups on the Internet is entitled \u201ccomp.ai.philosophy.\u201d  This group features constant variations on questions such as:  How close can artificial intelligence (particularly computers) approximate to human consciousness? Is free will reducible to neurological mechanisms? and so forth.   From my unscientific sampling, I would estimate that the clientele of this newsgroup is about evenly split between those who tend towards a reductive materialism, and those who maintain that consciousness or some element in human consciousness is not reducible to neural structures or functions.  So the classical \u201cHobbes vs. Berkeley\u201d debate continues on into the twenty-first century. One of the problems facing those who theorize about the independence and irreducibility of consciousness is the fact that it is difficult to conceptualize the essence of consciousness, as distinct from the sensations, feelings, etc. that are often associated with consciousness.  Here we are definitely getting into abstract metaphysics.  Medieval philosophers such as Aquinas, Duns Scotus and Suarez faced up to this challenge with a little help from Christian revelation, by speculating about the characteristics and functions of angels or \u201cseparate substances,\u201d who would presumably exemplify consciousness in its \u201cpure\u201d state, without any distracting admixtures.  In this article, I would like to take a look in particular at Aquinas\u2019 theory of separate substances.  With this theory, we bypass the old question of the reducibility or irreducibility of consciousness to its material conditions, and we also find, in my opinion, some interesting analogies to contemporary computer technology.  In some cases the analogies almost seem to throw light on Aquinas\u2019 speculations. Microprocessors and Angelic Self-possession: The microprocessors of today\u2019s computers are integrated circuits which contain the CPU on a single chip or a few chips.  The latest developments, with variable clock speeds now often exceeding 2.0 ghz, include Intel\u2019s i7  and the AMD Phenom.  The CPU chip is the heart of the computer; only memory and input-output devices have to be added.  Fans, sometimes elaborate, are added on top of the fastest chips to cool them down, but in the chip itself there are no moving parts, no complex gaps between the movement being imparted and that which imparts the movement.  So also, an angel, in Aquinas\u2019 formulation, is a simple, subsistent form, without any distinction between a material and an immaterial part, between matter and form.  Since there is no such division, and thus no material impediment to self-possession, the angel possesses itself directly in an immaterial way, and possesses itself completely.  But here is where the \u201cmiddle term\u201d enters: to possess anything in an immaterial way is to possess it cognitively, i.e., to know it.  Thus the angel knows itself, its own essence, completely; and since will is proportioned to intellect, the angel\u2019s volition and \u201cmovement\u201d is characterized by completely spontaneous self-determination on the basis of its knowledge.  And just as increasing the clock-speed in certain types of processors narrows the gap between the issuance of a command and its implementation, so also the degree of immateriality of an angel decreases the gap between intention and accomplishment, principles and conclusions, causing the self-determination of the angel to approximate almost to instantaneity.  ROM and Innate Ideas: The chip storing the Read-Only Memory in a computer contains the immutable routines controlling input and output for the computer.  It determines the basic functions of a computer, and cannot be modified in its essential functions by programs running in the computer.  In a sense, it is the innate, predetermined program for the computer as a whole.  So also, the angel is endowed from the time of its creation with an innate type of knowledge \u2014 the knowledge that results from possessing its own essence in an immaterial way, that is, cognitively, prior to acquiring any supervening intelligible species.  It is capable of acquiring additional ideas, as we shall see below, but even prior to receiving additional species it knows itself completely and all other things in a partial manner, insofar as the perfections of these other things are contained in a unique way in the characteristics of its own essence. RAM and the Negative Active Potency of Angels: The Random-Access Memory of a computer opens up the possibility of receiving an influx of data and images from programs and input devices; thus the maximum capacities of a computer can to a great extent be utilized only if maximal gigabytes of RAM, proportional to hardware and operating-system capabilities, have been installed in that computer.  Angels, according to Aquinas, possess a negative or privative potency which is inversely proportional to their perfection.  The higher angels have maximum act and minimum potentiality, while the inferior angels have proportionally \u201cless act, more potency.\u201d  The negative potency of all the angels is a side-effect of their finitude.  The inferior angels, because of the greater relative deprivation of actuality in their form, need to be continually equalized with their superiors through the proportionally greater number of infused species they receive from their comrades and God Himself.  Thus the lower angels have a greater capacity for receiving infusions of knowledge than the higher angels, and this capacity complements the greater power of the higher orders for giving such infusions. Hard Drives and Intellectual Memory: A distinction is made in contemporary computers between \u201cmain memory\u201d or storage capacity, and the volatile memory in RAM.  When \u201cmemory\u201d is spoken of without further qualification with reference to computers, RAM is usually meant.  When the \u201cmemory\u201d of intellectual beings is referred to without further qualification, something like \u201cstorage capacity\u201d is usually meant.  Among intellectual beings, humans have both sense memory and intellectual memory.  Using a little literary/philosophical license, we might say that sense memory for human beings is analogous to external storage devices such as CDs, DVDs or flash drives that can provide ad hoc input to a computer, while intellectual memory is analogous to the more permanent storage provided by hard disks or the \u201ccloud\u201d of Internet storage of data and apps.  Whatever the merits of this analogy, angels, according to Aquinas, being purely spiritual creatures, have only intellectual memory, not sense memory.  For them, there is no such thing as externally-received input in the strict sense.  A comparison with Leibniz\u2019s monads, which mirror all things in an intrinsic fashion, is probably in order here.  In other words, images of all things, including sense objects, are stored in an angel\u2019s intellectual memory; and these images can be accessed and applied whenever needed. The Operating System and the Reception of Information: Granted that RAM gives a computer the capacity to receive information, how does the data actually come to be accessed?  The answer to this question in the case of computers is obviously the operating system; a computer must have an operating system such as Windows 7, Mac OSX, or Linux, to begin receiving data and commands.  Human beings, because of their corporeality and discursive type of rationality, must process information sequentially with what Aristotle called the \u201cpossible intellect.\u201d  But according to Aquinas, angels being completely incorporeal do not have a \u201cpossible intellect\u201d in the strict sense, although we could say that the angels of the lower orders have a \u201cquasi-possible\u201d intellect insofar as they are in potency to receiving intensifications of illuminations from the higher orders.  But they do not need a possible intellect to begin processinginformation.  Like a computer shipped with an operating system fully installed, angels are fully equipped at their creation with innate and habitual intelligible species. Software and Habitually Acquired Species: In a computer, information, techniques and procedures not contained in ROM and not included with the operating system, are supplied by supplementary software.  According to Aquinas, angels continually and unendingly receive habitual impressed species directly from God as a supplement to the connatural knowledge which they have through their own essence.  Even at creation, before they were tested for fidelity, these species provided them with their natural knowledge of God; the species likewise endow them with a knowledge of all beings other than God.  There are some limitations to their knowledge of other intellective beings, however. Something like a \u201cright to privacy\u201d is operative amid the angelic hierarchies, since angels cannot have access to the thoughts of other angels, unless the other angels explicitly will that they have such access.  These restrictions apply also to angelic interrelationships with human beings: they cannot have direct access to human thoughts, although they can gain some knowledge of the thoughts of human beings indirectly, from observation of their dispositions, movements and appetites. \u201cDownward Compatibility\u201d and Proportional Universality: Later versions of software are usually able to do all the things that earlier versions of the same software could do.  So also, the higher angels contain more universal and fewer species than the lower; these species contain in a simpler way everything that is contained in the species of subordinate orders, thus increasing the scope of comprehension and the parameters of influence of the higher orders.  Thus the superior angels might be compared to the manager who knows how to do everything that his/her subordinates can do, but specializes just in a supervisory function. Multitasking: Contemporary operating systems when installed in a sufficiently advanced microprocessor offer the capability of \u201cmultithreading\u201d (32-bit or 64-bit on most home computers) whereby multiple tasks can be carried out simultaneously \u2014  for example, simultaneous printing, copying to disk, and search-and-replace operations.  If I am presently working on a particular task, this is designated the \u201cforeground\u201d task, while the other operations are carried out in the \u201cbackground.\u201d  In like manner, angels are characterized as having the ability to know and do multiple things at once.  In one simple act, they know all the variety of particular things, although they can bring this or that object into the \u201cforeground\u201d by turning their attention to it.  They are also continually combining the knowledge of things as they exist in God (cognitio matutina) with the knowledge of things according to their own proper existence (cognitio vespertina).  And an angel also has the ability to engage in \u201coblique contemplation\u201d (simultaneous combination of action and contemplation). Handshaking\u201d and the Ability to Transfer Information: Individual computers or local network nodes can come into contact via modems with their counterparts through a process called \u201chandshaking.\u201d  Ethernet cards have a similar function. Signals are sent out, for example, from a browser, and coordinated, and as a result commands can be processed, binary and text files can be transferred from a server. According to the Thomistic theory of separate substances, angels cannot receive new intellectual species from other angels, but they can receive an intensification of illumination of the species they already possess by coming into intuitive contact with other angels; an angel of a higher order merely wills to communicate something and it is instantaneously transmitted. Networks and the Transmission of Ideas, via Hierarchies: In networks, signals are sent from major servers and their \u201cmirrors,\u201d sometimes through a chain of many intermediate servers (as in Internet transmissions), until they reach an end \u201cuser.\u201d  In the angelic hierarchies, the gifts of grace among angels must always be proportional to the gifts of nature (unlike the case with humans, where, for example, someone with little native intelligence could be much more generously endowed with grace than a highly gifted individual).  And so, angelic communications always take place from the naturally superior to the naturally inferior.  But, as was mentioned above, the communication does not involve the transmission of new species, but only an intensification of illumination.  What this means is that a superior angel can cause in an inferior a heightened perception of species the latter already possesses, or offer different insights or perspectives on the knowledge the latter possesses; or possibly put his own higher species into terms more comprehensible by the other angel, like a teacher explaining advanced concepts in simpler terms to a student. Speed-caches and the Effect of Immateriality: Speed caches in computers consist of high-speed memory within the main processor. They help to increase computing speed by providing a route for signals to avoid getting slowed down by operations in the hard disk, which is mechanical and relatively slow, or by calls for data from the RAM memory, which is faster, but slower than the cache in the CPU.  Bits of programs or data can be stored in the speed cache to facilitate almost instantaneous access. An angel, on the other hand, completely avoids problems of slow transmission; it simply has no matter to slow it down.  Thus, for all practical purposes, and from our corporeal vantage point, all angelic operations and comprehensions are instantaneous. It should be mentioned, however, that there was considerable disagreement among medieval and patristic authors concerning the \u201cimmateriality\u201d of angels.  Augustine and Origen had speculated about the possibility of angelic bodies being composed of matter, albeit a more \u201csubtle\u201d matter.  Duns Scotus gave lip-service to the received doctrine of angelic incorporeality, but challenged his readers\u2019 imagination by theorizing that angels must have some kind of \u201cincorporeal\u201d matter (maybe something like the \u201cdark matter\u201d that physicists talk about?); and, consistently with this theory, Duns Scotus thought that angels must reason discursively in some fashion.  If there were any kind of matter in angels \u2014 subtle or \u201cincorporeal\u201d \u2014 the leap from completely immaterial beings to partly material beings would be less formidable, and the analogy between angelic spontaneous instananeity and the linear, sequential operations of computers would be even closer.  Howard Kainz is professor emeritus at Marquette University. He is the author of several books, including Natural Law: an Introduction and Reexamination (2004), The Philosophy of Human Nature (2008), and The Existence of God and the Faith-Instinct (2010).  ",
        "title": "Artificial Intelligence and Angelology",
        "citationCount": 0
    }
]